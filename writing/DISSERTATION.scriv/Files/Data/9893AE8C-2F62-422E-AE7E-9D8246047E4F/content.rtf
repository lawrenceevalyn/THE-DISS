{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 Georgia;\f1\ftech\fcharset77 Symbol;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\pard\tx360\tx720\tx1080\tx1440\tx1800\tx2160\tx2880\tx3600\tx4320\fi360\sl312\slmult1\pardirnatural\partightenfactor0

\f0\fs22 \cf0 ** Use the word vector exercises from DH2018 workshop\
\
Word embeddings allow for analogy checking. For example, man is to king as woman is to queen, expressed as man:king :: woman:queen, has its reflection on the vector representions of the words man, king, woman, queen in such a way that  king
\f1 \uc0\u8594 
\f0 \uc0\u8722 man
\f1 \uc0\u8594 
\f0 +woman
\f1 \uc0\u8594 
\f0 \uc0\u8776 queen
\f1 \uc0\u8594 
\f0 king
\f1 \uc0\u8594 
\f0 \uc0\u8722 man
\f1 \uc0\u8594 
\f0 +woman
\f1 \uc0\u8594 
\f0 \uc0\u8776 queen
\f1 \uc0\u8594 
\f0  . However, this can also highlight some biases in the specific corpora the model has been trained on. Using as a base the pair she-he, find the most similar term for female from the term in the next list: doctor, captain, gallant, sheriff, engineer, scientist, author, surgeon, honorable, philosopher, warrior, architect, magician, liar, and coward. \
When possible, compute the similarity between the expected term for female and the one for male. Use the Google News (2015), English Fiction (1900, 1950) and Genre-Balanced American English (1900, 1950) embeddings. For example, using the English Fiction 1850 embeddings, he:gallant :: she:x solves to x=gentle, and the similarity between gallant and gentle is ~0.418. Can you spot the problem? \
Hint: Use Gensim's most_similar_cosmul()/most_similar() and similarity() functions\
\
Really cherry-picking the "interesting" results; scroll right past the total nonsense and latch onto anything that can be vaguely thinks\
\
By comparing how different the he:X :: she:X results are, we can quantify the gendered bias within the corpus, to skeptically evaluate our corpora\
\
Very hard to remove gendered bias without removing the other relationships that interest us\
\
Not just corpora that are biased; because the algorithm depends on the corpus, it will become biased too}