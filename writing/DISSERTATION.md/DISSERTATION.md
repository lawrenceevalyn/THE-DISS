Title: DISSERTATION  
Author: Lawrence Evalyn

# introduction (9k) #  

According to the English Short Title Catalogue (ESTC), the most popular English authors of the 1790s were Thomas Paine, Hannah More, John Wesley, and William Shakespeare. Of course this claim immediately falls apart on further scrutiny. In fact, by the metric of ‘unique entries in the ESTC database,’ the most popular author of the decade is by far Great Britain, followed by Great Britain, Great Britain, Great Britain, and King George III.[^cf1] Paine, More, Wesley and Shakespeare are only able to rise to our notice if we intervene in the dataset to filter out all authors whose names contain the phrase “Great Britain”; otherwise, Shakespeare is outnumbered by the House of Lords and by the Church of England. And a single paragraph cannot contain all of the reasons that the quantity of unique entries in a database would not correlate with any useful definition of popularity -- although later parts of this dissertation will undertake to enumerate them at greater length. These claims demonstrate that a poorly formed question will produce a useless and stupid answer even (or perhaps especially) if computation is used to answer it. This dissertation is dedicated to the formulation of better questions. I am interested in the limits of the generalizations that we make, both in “distant reading” research and in non-digital scholarship[ , which still frequently relies on claims that a given work was “popular” because it went through a certain number of editions, or the author was paid a certain amount, and so on. These generalizations break down in part because “popular,” as a concept, is overdetermined: does it mean financially successful, or widely beloved, or important? Examinations of “popularity” also break down, at close scrutiny, because of the contentious relationship between concepts of “popular” and “literary”: important literature should have some claim to cultural relevance, but it shouldn’t be *too* popular or it becomes suspect. Nonetheless, \[TRANSITION\]]. I take as my starting point the contention that, in order to identify what is “popular” or “important,” we must also understand what is normal. At its core, my question is: given that it is not possible to read everything (or even most things), how do we, and how *should* we, determine what to read, preserve, study, and teach? This “question” is, of course, many questions: what we do is by no means what we *should* do; what we read is not necessarily what we study or teach. It is also an old, nearly an old-fashioned question. The current moment of self-reflection in the field of Digital Humanities, however, provides a timely reason to revisit it. Even literary scholars who do not carry out “Digital Humanities” research are impacted by the corpus-building choices of major digital resources, since all literary research is now mediated at some level by search algorithms and databases, even if this mediation is as small as looking up the holding libraries for physical copies of texts. It is therefore relevant to the field as a whole if, as I contend, corpus-building has become the new canon-building: an invisible and naturalized process of selecting texts for idiosyncratic and historically-specific reasons, and then treating those individual texts as ideal representatives of an imagined “whole” of literature.   

Despite the crucial importance of corpus-building to the interpretation of “distant reading” research, it is often extremely difficult to know what is in a corpus. Even large institutional resources used by many scholars provide little context for their choices of what to include or exclude. These hidden choices are particularly problematic when historical selection factors might have led to the creation of databases which re-create social inequalities. I focus specifically on writing printed in England between 1789 and 1799, to explore how works from this eleven-year “decade” have been selected as important, literary, or popular. For this period, the English Short Title Catalogue provides basic bibliographic data for nearly 52,000 titles, but the Eighteenth Century Collections Online Text Creation Partnership corpus of XML-encoded full texts includes fewer than 500 titles. This difference raises the question: why were the other 51,500 titles *not* considered worth the investment of scholarly effort? And with particular urgency: do the most invested-in resources underrepresent women? My experiments examine six major databases to answer these questions: The English Short Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the Eighteenth Century Collections Online Text Creation Partnership (ECCO-TCP), Google Books, Project Gutenberg,[^cf2] and HathiTrust. For each database, I download their holdings identified as printed in England 1789-1799.[^cf3] I identify how many titles the database attributes to each year. I calculate how many works are attributed to male, female, or unknown authors.[^cf4] These very simple pieces of information, when they differ widely between databases,[^cf5] provides the basis for an initial analysis of the assumptions and limitations of each database. I then examine the contents of each database more closely, to compare the inclusion of broad categories of writing like poetry, drama, prose fiction, and ephemera.[^cf6] Identifying these categories of writing within each corpus reveals a predictable preference for “literary” forms such as novels and poetry in the smaller databases. This preference for particular kinds of writing might explain changes in gender representation of smaller databases. If the novel is the domain of women, for example, a corpus can underrepresent women by under-representing novels. Or it could include a representative number of novels, but disproportionately include novels by men. My investigation allows me to identify the patterns of selection.[ TK: “What would be your preliminary hypothesis? I guess one assumption might be that market-driven choices are made in light of existing demand i.e. the established teaching / research canon, in which case there would be an inherent conservatism in selection of texts.”] To ground my analysis in specifics, I take Charlotte Smith as a case study author. Smith has a long history of contentious reception, rooted in debates about seriousness, popularity, and women’s writing. I revisit them to see how her career and reception might be interpreted through a new lens. I do so, in part, to challenge the contrast drawn between ‘popular’ and ‘serious’ writing, especially in the historical evaluation of women’s writing as literary.



## 1.1.  from canon to corpus ##



Play with the idea of the digital revolution? Challenging the simplicity of people’s assumptions about the digital revolution

Profusion of print in this decade

Revolution -\> print culture in revolution -\> revolutionary women writers -\> and then I am interested in Charlotte Smith in particular

Page 4 is very jumpy -- Smith has a long history of contentious reception -- pull out these details. To get to reviewing culture more smoothly. Be more specific about why she might have fallen out of favor in the 19thC.

Have a first section “from canon to corpus”? From “the problem of valuing literature” -- begin with the idea of the canon, how it’s been constructed historically, which gets into reviewing culture

Why Charlotte Smith?  

The problem of evaluating literature is not a new or a simple one. In the eighteenth century, the debate took the form of urgently needing to distinguish ‘trash’ from ‘treasure’. Michael Gamer, in *Romanticism and the Gothic: Genre, Reception, and Canon Formation*, highlights the role of the eighteenth-century reviewer as a crucial mediator between the writers and readers of books[ Reviewers sought to dictate the social assessment of individual literary works in order to enforce morality for society at large.\[ The emerging idea of the public sphere (cf Habermas) in the eighteenth century brought with it an urgent task of literary assessment. --- this is true but maybe I don’t have to say it! and then I don’t have to waste time talking about Habermas\] \[They basically say so themselves in their reviews---I’ll quote a juicy representative one.\] ]. Importantly, although the assessments take the form of reviews of individual works, Gamer also argues that the critics’ objections are in fact “a regulatory discourse -- carried out under the fiction of paternalistic advice to a given gothic writer, but functioning as an implicit threat to other readers and writers” that affiliation with the gothic comes with “cultural costs” (42). The gothic stands in as a proxy for any kind of “popular” reading that takes place “in the absence of formal education and training” (57), so a denunciation of a gothic work becomes a reaffirmation of class-based literary hierarchies. In other words, these reviews create and affirm the cultural capital of a category of ‘serious’ literature[ The really clever bit of Gamer’s argument is that he then unpacks how Romantics, especially Wordsworth, use just enough Gothic material to sell their books while also repeating these conventional attacks on the Gothic --- they get to have their cake and eat it too, pursuing both financial and cultural capital. But I don’t think that’s relevant here.]. Gamer is only concerned with the gothic and romanticism, but the overall regulatory function of literary reviewers as moral arbiters--- and the stock conventionality of their objections, which do not affect the actual production or consumption of the works attacked--- applies to most forms of writing in the period. For example, George Taylor sees the same dynamic in the theatre. In *The French Revolution and the London Stage,* he argues that, “\[c\]ritics might make sharp comparisons” between the many kinds of entertainments that were staged, “but little of the programme was dismissed \[by audiences\] as ‘trash', or ‘immoral', or irrelevant ‘fancy’” (3). Taylor sees the repetitive discourse of eighteenth-century literary critics as proof of a larger social divide: “Disagreement as to what is trash and what is treasure suggests cultural crisis, when values are put under question by social stress or political conflict” (3). Gamer and Taylor both suggest that moral judgment of literature by its critics was driven by social friction, rather than by the aesthetic distinctions which they claimed as their motivation.  

In other words, Gamer and Taylor both affirm the key conclusion of John Guillory’s *Cultural Capital: The Problem of Literary Canon Formation*, that “in fact ‘aesthetic value’ is nothing more or other than cultural capital” (332). Guillory’s sociological history of literary canons is a well established part of literary studies, which will take on new dimensions as I apply it to the current moment of digital databases. In the eighteenth century, he argues, the cultural capital of vernacular English literature is defined by its use within the school system to enable and restrict social mobility. English vernacular literature first begins to accumulate cultural capital in middle-class schools where it is “a substitute for the study of Greek and Latin, but with the same object of producing a linguistic sign of social distinction” (97) that would allow readers to improve and signify their social standing. The public re-assessment of literature as described by Gamer and Taylor is, for Guillory, “the first crisis in the status of the vernacular canon, the problem of assimilating new vernacular genres such as the novel” (xi), which seem in danger of affording too much social mobility by offering too little literary distinction for social elites.[^cf7] The ‘solution’ is institutionalization, in which “the school becomes the exclusive agent for the dissemination of High Canonical works,” and therefore, he argues, “the prestige of literary works as cultural capital is assessed according to the limit of their dissemination, their relative exclusivity” (133). Under this system, ‘serious’ literature may not be identifiable linguistically, but it can still be identifiable by the difficulty of accessing it. This history of canonization has important implications for the field of literary study. As Guillory himself insists, if the aesthetic value of a text is determined by the social operations of class, it undermines the notion of literature itself as a category of writing distinguishable in aesthetic terms from non-literary writing[ Q for Gillespie: should I still research “the new formalist work to reclaim the literary qua the literary - and e.g. Steve Conner's - ‘nah’” ? What is useful/important to address in this work, given what I say here?]. Guillory’s book is motivated by the canon debates of the 1990s, which were driven by an urgent re-valuation of literature by women and people of colour.[^cf8] His response insists that it is untenable to conceive of the problem in terms of increasing the ‘representation’ of individual works or authors within existing systems. Instead, for Guillory problem lies in the institutionalization of literature itself. “If literary criticism is ever to conceptualize a new disciplinary domain,” he says, embedding his prescription in that “if,” “it will have to undertake first a much more thorough reflection on the historical category of literature; otherwise I suggest that new critical movements will continue to register their agendas symptomatically, by ritually overthrowing a continually resurgent literariness and literary canon” (265). In other words, assigning the cultural capital of “literature” to different works cannot change the underlying system.[ Do I need to say more about this? I feel like I’ve already spent a long time on Guillory.



Bourdieu: “‘\[T\]o deny evaluative dichotomies is to pass a morality off for a politics. The dominated in the artistic and the intellectual fields have always practiced that form of radical chic which consists in rehabilitating socially inferior cultures of the minor genres of legitimate culture. ... To denounce hierarchy does not get us / anywhere. **What must be changed are the conditions that make this hierarchy exist, both in reality and in minds. We must---I have never stopped repeating it---work** __to *universalize in reality the conditions of access*__ **to what the present offers us that is most universal.**’" (Qtd in Guillory 339-340)]  

Perhaps indicating that Guillory was correct, twenty years later, we are still debating the need for “literary criticism ... to conceptualize a new disciplinary domain” (Guillory 265), now in the context of computation. The reconceptualization of literary study itself is at the core of Franco Moretti’s coinage of ‘distant reading’: the problem for which “\[r\]eading ‘more’ seems hardly to be the solution” (“Conjectures” 55) is the problem of conceiving of *world* literature, rather than the “canonical fraction, which is not even one per cent of published literature” (55). His new methods are meant to enable literary studies to examine a new object. The field of distant reading has been moving away from Moretti himself. However, it is still shaped by the attempt to redefine the disciplinary domain of literary studies. In many cases, the new domain is no longer the “canon” but the “corpus,” a collection of texts which are studied *en masse* for macroanalytical insights. Katherine Bode, for example, in “The Equivalence of ‘Close’ and ‘Distant’ Reading,” argues that Moretti and Matthew Jockers replicate the approaches of New Criticism with their corpora, and calls for “a new scholarly object of analysis” (79) that directly examines historical and textual context of corpora as representations of “literary systems” (97). Lauren Klein, too, treats the textual corpus as the new object of literary analysis requiring curation, contextualization, and interpretation. Her critique argues that “it’s not a *coincidence* that distant reading does not deal well with gender, or with sexuality, or with race,” but also that these failings are not inevitable: “it’s not that distant reading *can’t* do this work,” she insists, “it’s that it’s yet to sufficiently do so” (n. pag.). Bode, too, despite her strong critique of distant reading as it has been practiced by Moretti and Jockers, does not blame distant reading itself. Distant readers like Moretti and Jockers, she argues, “while claiming direct and objective access to ‘everything,’ ... represent and explore only a very limited proportion of the literary system, and do so in an abstract and ahistorical way” (78). Klein, like Bode, calls for “more corpora---more accessible corpora---that perform the work of recovery or resistance” to allow research “beyond quote ‘representative’ samples, which tend to reproduce the same inequities of representation that affect our cultural record as a whole” (n. pag.). This framing re-creates, at the site of the corpus, the identical narratives of exclusion and representation which were previously located in critiques of the canon.  

The relocation of the debate from the canon to the corpus is not without grounds. As this dissertation will explore in depth, challenges to the technological accessibility of texts have created new hierarchies, and a new “great unread.” Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. For example, the English Short Title Catalogue records 51,965 titles printed in England between 1789 and 1799. The corpus most commonly used for DH work on eighteenth-century literature, ECCO-TCP, includes only 466 titles for that same time period. What are the other 51,499 titles, why are they accessible in the ways they are, and what does it mean for digital eighteenth-century studies that they are not included? Although the examination of databases prompts similar hypotheses of exclusion as in longstanding conversations about canons, digital databases do not simply replicate new canons. \[By the end of my dissertation, I will be able to state here what IS happening --- something structured by related logics of access and prestige, and related simplifications of historical complexity, and related *institutional* replication of privileged texts.. But very importantly different, too, since we don’t *read* databases.\] In a series of computational and non-computational research processes, I examine six databases of eighteenth-century texts to learn about four eighteenth-century authors, and I examine four eighteenth-century authors to learn about eighteenth-century databases. This dissertation, therefore, takes place within three scholarly conversations: the digital humanities, as an increasingly self-reflective set of practices; eighteenth-century studies, and the challenges presented by the 1790s; and the frameworks of reparative reading within queer theory which seem to offer valuable resources for both. The remainder of this chapter will describe in more detail the relevant scholarship shaping my frameworks, and then introduce my chapters by introducing my four case study authors.



## 1.2.  frameworks ##  

My work takes a critical algorithm studies approach to digital databases of eighteenth-century literature, examining the structural assumptions of the most-used resources (including some that scholars don’t like to admit to using). I close read the database structures, file formats, and historical documentation for the English Short Title Catalogue, Eighteenth Century Collections Online, the Text Creation Partnership, HathiTrust, Project Gutenberg, and Google Books, to examine how each resource’s algorithmic definition of a “book” (and the information that might matter about a book) is shaped by the material, historical conditions of each organization’s development. My initial research question was, by Eve Kosofsky Sedgwick’s definition, a classically paranoid approach: I sought to expose the under-representation of women’s writing underlying apparently “neutral” digital infrastructures. This question carried the combined urgency and futility of paranoid critique: urgent, because an unfair database would expose an unfair society; and futile, since the research could only be motivated by the conviction that its answer was already known. My paper will touch briefly on some specifics of this research and my findings, as the basis for a broader discussion of critical algorithm studies, and the project of imagining reparative algorithm studies.

One of the current problems of critical algorithm studies is how difficult it is to move from critique to action: it seems that no matter how carefully we dissect the flaws of oppressive computational systems, we cannot opt out of them. Excellent work by scholars like Wendy Hui Kyong Chun and Safiya Noble, for example, meticulously historicizes computational systems, and there is real value to the denaturalization of the systems they thus reveal. But this work relies on the paranoid logic of exposure, and I am interested in other attitudes. In my examination of digital infrastructures for eighteenth-century studies, I take a brief detour through Marxist thinking (via Bourdieu and John Guillory) to diagnose a deep tension between capitalist and anticapitalist value systems as the likely cause of the flaws in these systems today. I then aim to move beyond the obvious paranoid critiques prompted by this observation. I confess that, at this stage, this is the point at which my thinking remains speculative--- but I feel certain that the right direction lies in queer strategies of creative reappropriation, subversion, and resistance.  

The theoretical frameworks of this dissertation are drawn from the fields of feminist DH and queer DH, and from non-DH schools of thought which seem to offer valuable tools. My core motivating framework, as I conceptualize my work, is that of reparative reading. Eve Sedgwick’s “Paranoid Reading and Reparative Reading” persuasively describes in the dominance of paranoia in literary criticism, and attempts to sketch an alternative in what she terms reparative reading. A paranoid rhetoric of exposure and critique strikes me as the most obvious narrative to structure this dissertation’s investigation of the uneven institutional valuation of different writing. However, these obvious critiques also require rejecting many generations of sincere work by my fellow academics, without necessarily offering new discoveries of value to replace them. One experiment of this project, not yet complete, is to articulate an assessment of the limitations of contemporary digital resources which nonetheless allows those resources to be recuperated. My touchstones are two descriptions from Sedgwick’s original chapter:  

The desire of a reparative impulse... is additive and accretive. Its fear, a realistic one, is that the culture surrounding it is inadequate or inimical to its nurture; it wants to assemble and confer plenitude to an object that will then have resources to offer to an inchoate self. (149)

What we can best learn from such practices are, perhaps, the many ways selves and communities succeed in extracting sustenance from the objects of a culture - even of a culture whose avowed desire has often been not to sustain them. (150-151)

What Sedgwick describes, here, is a “desire,” not a methodology. I therefore understand “reparative reading” to refer, not to a precise set of practices, but to a position one might occupy in relation to a text. What I posit is also a desire: that my methods here can provide useful practices for others. The reparative position is a generous one, both in terms of giving of oneself to a text, and in terms of seeking a text’s strengths over its weaknesses. What I learn from Sedgwick, therefore, that *attention* is the first step toward *caring*, and that non-judgment can be more informative than rejection.  

I have mentioned moving away from critique as well as from paranoia: in rethinking the role of critique, I draw upon the work of Rita Felski, and the theories of “surface reading” described by Sharon Marcus, Stephen Best, and Heather Love. Felski, in her article “After Suspicion” and then further in her monograph *The Limits of Critique*, seeks to attend seriously to literary attachments, including our own attachments as critics. Felski’s approach to these attachments is essentially sociological, drawing heavily on Bruno Latour’s actor-network-theory, and thus involves almost no close reading. “Surface reading” positions itself as an alternative to “symptomatic reading”; rather than seeking to expose hidden truths concealed within texts, it attempts accurate descriptions that “make visible what is invisible only because it's too much on the surface of things” (Best 13). The analogues to reparative and paranoid reading are obvious, but not perfect: all paranoid reading is symptomatic, but not all symptomatic reading is paranoid[ unpack]. Reparative reading, as described by Sedgwick, is often still interested in ‘deep’ meanings of texts, in which striking textual features can be interpreted to locate additional meanings. Felski’s readings are often symptomatic in this way. In contrast, “surface reading,” as Heather Love describes, pursues “a turn away from the singularity and richness of individual texts” (374), seeking descriptions that are “complex and variegated, but not rich, warm, or deep” (378). Love’s disavowal of “richness” here is part of her attempt to move away from “the ethical charisma of the literary translator or messenger” (374) who characterizes the paranoid, critical figure that both Sedgwick and Felski also seek to escape.  

Love’s later article, “Close Reading and Thin Description,” provides a more precise articulation of the kind of close reading that she calls for, in which an “exhaustive, fine-grained attention to phenomena” (404) enables “taking up the position of the device; by turning oneself into a camera, one could---at least ideally---pay equal attention to every aspect of a scene that is available to the senses and record it faithfully” (407). Although Love is uninterested in “distant reading” as synonymous with Moretti (Love 411), this invocation of the mechanical implies, I argue, an obvious potential for computation. The actual *practice* of computational research requires a great deal of laborious, intimate encoding. The researcher must occupy a “mechanical” position of receiving inputs and responding to them consistently over time, whether entering details in a spreadsheet with a consistent taxonomy or running the same program over multiple datasets.[^cf9] Love says:

Good descriptions are in a sense rich, but not because they truck with imponderables like human experience or human nature. They are close, but they are not deep; rather than adding anything ‘extra’ to the description, they account for the real variety that is already there. (377)

A computational model is unlikely to “truck with imponderables,” but it *absolutely* *must* “account for the real variety that is already there” or else the code will simply fail to run. If you are forced to manually encode your assumptions into a system, you are forced to confront what they are. Even deleting or ignoring information is still a way of “accounting for” it in the coding process: some part of the program will have to say, in effect, ‘if I get an input that doesn’t match what I expect, discard it.’ Choosing to ignore contradictory or difficult information carries the assumption that this information does not ‘count,’ or does not matter to the question at hand. The choice faced by scholars is how to address our encoded assumptions. The encounter with variety does not in itself produce nuanced results: it is possible to selectively ignore any uncomfortable details. But it is also possible to do computation reflectively, asking not “how can I make this work the way I want?” but “where do my assumptions encounter resistance?” and turning one’s attention to the nature of the resistance. Integrating this reflection into the research process can allow a scholar to avoid both the pitfalls of “conquering” their material and of claiming an algorithmic grasp of “objective” truth.   

“The relationship between the individual cultural object and the curated dataset is not a transparent one; the latter is rather a heavily mediated and discipline-specific representation of the former. **Through the collection and curation of our own dataset we are acutely aware of the choices that went into its creation.** The use of already curated datasets has other undeniable advantages: it may temper the influence of the researcher on his or her findings; furthermore, from a practical standpoint, it allows work to advance past the time-consuming labor of curation. While we would not suggest that researchers need to reinvent the wheel, we do advocate for a more explicit reflection on the relationship between the dataset and the objects it describes. Such reflection allows for a deeper resonance between digitally enabled research agendas and existing intellectual and disciplinary traditions.” (Vareschi and Burkert 612)  

To bring these principles into the field of Digital Humanities by way of an example, I want to offer an alternative geneaology for the practice of distant reading itself. Rachel Buurma and Laura Heffernen provide a valuable history[ Ted Underwood’s “Genealogy of Distant Reading” presents a history of distant reading which is not for the most part centrally concerned with computers, and is therefore fundamentally distinct from concepts of “digital humanities” \\cite\{Underwood:2017uc\}. In Underwood’s history, distant reading is “a tradition continuous with earlier forms of macroscopic literary history, distinguished only by an increasingly experimental method, organized by samples and hypotheses that get defined before conclusions are drawn” (Underwood:2017uca p.29\}. Underwood “tease\[s\] out the elided social-scientific genealogy behind distant reading” \\cite\{Underwood:2017uca p.39\} to argue that the term “\[d\]istant reading was not coined to describe a radically new method. The first occurrence of the phrase, in \[Franco Moretti’s\] ‘Conjectures on World Literature,’ seems in fact to describe the familiar scholarly activity of aggregating and summarizing previous research” \\cite\{Underwood:2017uca p.9\}\[ what is your position on this? You go on to Buurma who does connect distant reading to computation - and you suggest how you will draw on similar ideas about collaboeation etc - but this underwood bit dangles\].

]of Josephine Miles as the first ‘distant reader’. Miles’ history, briefly, is as follows:

In the 1930s, as a graduate student at Berkeley, she completed her first distant reading project: an analysis of the adjectives favored by Romantic poets. In the 1940s, with the aid of a Guggenheim, she expanded this work into a large-scale study of the phrasal forms of the poetry of the 1640s, 1740s, and 1840s. In all of this distant reading work, Miles created her tabulations by hand, with pen and graph paper. She also directed possibly the first literary concordance to use machine methods. In the early 1950s, Miles became project director of an abandoned index-card-based Concordance to the Poetical Works of John Dryden. Partnering with the Electrical Engineering department at Berkeley, and contracting with their computer lab and its IBM tabulation machine, Miles used machine methods to complete the concordance. It was published in 1957, six years after she and several woman graduate students and woman punch-card operators began the work. It was thus begun around the time that Busa circulated early proof-of-concept drafts of his concordance to the complete works of St. Thomas Aquinas, and published 17 years before the first volumes of the 56-volume Index Thomasticus began to appear. (Buurma and Heffernan)

Buurma and Heffernan bring Miles’ history to our attention not simply because Miles predates Roberto Busa, whose *Index Thomisticus* is often credited as the first large scale computational literary study.[^cf10] Rather, they emphasize, Miles’ origin story for computational literary study “can stand as an example of how we might write a history of literary scholarship that does not center originality and individual accomplishment” (n. pag.). Unlike Busa, Miles not only gave authorship to the (female) graduate students who carried out much of the labour of creating the concordances, she also thanked and credited the (female) punch card operators who encoded the resulting data.[^cf11] Moreover, when talking of Penny Gee, one of the female staff members of the computer lab, Miles praises her as “‘very smart and good’ and---most importantly---a true collaborator, as opposed to those ‘IBM people from San Jose’ ... ‘I’ve never been able to connect with them,’ Miles explains, ‘though I did with Penny Gee. She really taught me’” (n. pag.). Of the positive qualities highlighted here, only one, “smart,” is traditionally valorized among literary critics: to be “good,” a “collaborator,” who can “connect” and “teach” --- these qualities are often seen as irrelevant to the singular authority of the figure of the critic, but they are core to a reparative practice. Miles’ work, too, struggled to find appreciation “among literary critics who viewed her datasets as merely preparatory to the true work of evaluation” (n. pag.).  

What’s crucial, to use computational reading reparatively, is to use it *reflectively*. The desirable kinds of computation which I describe above will not happen inevitably. Here I draw upon the rich body of work emerging in critical algorithm studies, which examines (and attempts to reform) the human elements of computational algorithms. Any methodology is, to a certain extent, an “algorithm,” in the loose definition of ‘a series of pre-defined steps to be carried out’. But computational algorithms differ from “algorithms” implemented by humans. Computational algorithms have two key vulnerabilities: first, their operations are less easily scrutinized; second, their results are more easily trusted. The second vulnerability --- the cultural aura of empirical trustworthiness which accrues to anything ‘computational’ --- is another flavour of the same vulnerability that Drucker describes with ‘data’ generally. Because the human agents who designed and trained any given algorithm appear to be absent from its operation, the algorithm appears able to discover truth directly. This is how Daily Wire reporter Ryan Saavedra was able to tweet with disdain that “Rep. Alexandria Ocasio-Cortez (D-NY) claims that algorithms, which are driven by math, are racist” (@RealSaavedra): anything “driven by math,” he assumes, must be incapable of human fallibilities like racism. But as Safiya Noble shows extensively in *Algorithms of Oppression*, algorithms by default reproduce, and can easily exaggerate, the assumptions and biases of the culture in which they are made (CITE). In other words, in a racist world, algorithms *are* racist --- and sexist, and duplicative of all other systemic inequities. To analyze an algorithm, one must articulate the implicit argument underlying the assumptions that allow it to operate --- what Ian Bogost would call its procedural rhetoric. As Katherine Bode’s recent work on data-rich literary history has shown, digital infrastructures themselves contain an implicit procedural rhetoric, even an argument, which must be addressed.  

Critical algorithm studies is therefore a crucial background for my work --- but “critical” is literally in the name of of the field, and I still seek to be post-critical and reparative. As I encounter the limitations of the various information and tools through which I attempt to understand the 1790s, my goal is to do something other than facilely observe that they are limited. Instead, I want to identify the best ways to continue building on their foundations. In a digital humanities context, a focus on building connections can be mundanely practical: typing indexes from print works into spreadsheets, correcting errors within datasets, writing programs to process metadata: all of these maintain the functional usability of existing resources in new contexts. When this kind of extended, detail-oriented labour is combined with serious reflection on the histories and possible futures of these resources, I contend, they bring us to new knowledge. In this, maintaining and using digital resources is also a way to repair them --- and to produce reparative readings of their contents.



## 1.3.  methods ##  

This dissertation undertakes computational distant reading. At every possible point, however, the underlying methodology will be made visible, and its assumptions scrutinized. The bibliographic histories of my multiple corpora are explicit objects of inquiry. Much of the code underlying this project I have written myself. Some has been written at my request. In every case where the code is available to me, the program itself appears in Appendix A (“Codebase”), accompanied by a plain language explanation of how it operates. Where I have used closed-source software, Appendix A contains an explanation of my best guess at its underlying process. My exact use of these tools --- sufficient for another to replicate my work --- is provided in Appendix B (“Methodology”). These details are explicated in full in the appendices in order not to over-burden the body of the dissertation, but they are by no means *confined* to the appendices. Computation is not a “black box” to be consulted for simple answers, but is inextricable from my reasoning and argument.  

My attention to the *sources* of digital knowledge creation comes, in part, from Johanna Drucker, and her distinction between “data” and “capta.” Drucker, in “Humanities Approaches to Graphical Display,” specifically addresses the digital humanities practice of creating, and then close reading, data visualizations. She argues that the tools for visual representation which may be effective in the sciences cannot be simply and uncritically transposed to humanistic subject matter. When an experiment is presented as a ‘data visualization,’ she says, “the rendering of statistical information into graphical form gives it a simplicity and legibility that hides every aspect of the original interpretative framework” (8). In fields where the readers of such charts are also frequent creators of charts, and where norms exist to explicitly describe one’s interpretive frameworks in a methodology section, the simplicity and legibility of an individual chart may be a benefit which does not impede complex scrutiny of the information it presents.[^cf12] In a field like literature, however, the “graphical force” of something like a network graph or even a simple pie chart “conceals what the statistician knows very well --- that no ‘data’ preexist their parameterization” (8). Drucker problematizes the term “data,” the etymology of which presents it as a “given” which is stable and independent of observation. She proposes that humanities visualizations embrace, instead, the framework of “capta,” that which is “‘taken’ actively” (3), “fundamentally codependent, constituted relationally, between observer and observed phenomena” (50). Drucker’s assessment shapes my own prioritization of qualitative and reflective computational research. The term “capta” itself has not seen uptake in subsequent digital humanities scholarship, even in cases where scholars explicitly take Drucker’s warnings to heart. Accordingly, for clarity, this dissertation will continue to use the more usual term “data” to refer to the information gathered for analysis here. However, as I integrate and compare a wide variety of data from many disparate sources, a preliminary task of my analysis is always to determine, as precisely as possible, how the information was captured and quantified.   

Additionally, all of the figures presented in this dissertation are of my own design. My design praxis is informed by the work of Edward Tufte and Alberto Cairo, both of whom provide practical design advice in service of demystifying the visual rhetoric by which graphs present their arguments.[^cf13] Neither Tufte nor Cairo is a scholar of media studies; rather, they are professional practitioners of ‘data visualization’ who reflect critically on the assumptions of their work. Tufte’s work primarily strives to correct badly designed data visualizations, and the dangerous decisions that bad design can lead people to. His most famous example is an analysis of the engineers’ report at NASA which led to the ill-fated launch of the Challenger space shuttle in 1986: as his extensive visual analysis argues, the engineers (untrained in graphic design) unintentionally obfuscated crucial information about the day’s launch conditions. The poorly designed graphics these engineers produced made the launch appear low risk to their superiors; despite the engineers’ strong warnings, their verbal argument was disregarded in favor of their accidental graphical argument. As Tufte demonstrates, a few simple alterations of their graphic design would have made it obvious that the day’s unprecedentedly cold weather was extremely dangerous, and potentially averted disaster \\cite\{Tufte:2001vw\}.[^cf14] Tufte’s six principles of design[^cf15] primarily seek to guide undertrained designers away from misleading themselves. Cairo, following on Tufte’s work from the perspective of an active journalist, more often turns his attention to successful designs which mislead their audiences intentionally. His forthcoming book, *How Charts Lie*, addresses the readers of infographics with insights into visual literacy \\cite\{Cairo:ikIksuMr\}. His preceding book, *The Truthful Art*, addresses the creators of good faith infographics with insights into visual manipulation \\cite\{Cairo:2016uv\}. Cairo draws a distinction between “data visualization” and “infographics”: “an infographic tells the stories that its designer wants to explain, but a data visualization lets people build their own insights based on the evidence provided,” summarized more succinctly as “infographics to explain, data visualizations to explore” \\cite\{Cairo:2014tl\}. Using this terminology, my argument will proceed with infographics in the body of the dissertation as curated figures to support my argument, with fuller data visualizations available in Appendix C (“Data”) to allow further exploration. Following in both Tufte and Cairo’s footsteps, I conceive of the figures throughout this dissertation as rhetorical devices. In service of arguing honestly, therefore, my designs --- in the body of the dissertation and in Appendix C --- are accompanied by footnoted explanations of my design rationale.   

This dissertation understands archives, bibliographies, anthologies, and corpora to all be, variously, *models* of an imagined object of study. In the language of social science, these models might be described as ‘samples,’ which are intended to permit discoveries about an underlying ‘population’ by being ‘representative’ of that population’s features. Only the language and not the methods of social science need to be imported here, since it has long been ordinary practice in literary studies to select and examine representative texts for insights about larger movements[^cf16]. A work like Ann Tracy’s bibliography *The Gothic Novel 1790-1830*, for example, clearly names the population of works which are of interest to her: all Gothic novels published between 1790 and 1830. She tentatively defines her principles of selection as \_\_\_\_\_. But in providing detailed information on 208 texts --- mostly Gothic, mostly novels, mostly between 1790 and 1830 --- Tracy obviously does not claim to have presented all that might belong within this population. Instead, her book operates as a model of the underlying population, which can be queried for further insight into ‘the Gothic novel, 1790-1830’ only so long as one keeps the limits of the model in mind. Indeed, by presenting plot summaries and bibliographic data, rather than reproducing the novels in full, Tracy provides a model of a model. One challenge to studying these models is that they present a “moving target”: even a bibliography or anthology is subject to change through successive editions (not to mention their now-common digital supplements), and a digital database has the potential to change daily. I follow Kath Bode’s approach in *A World of Fiction*, in artificially “freezing” each resource for study, and presenting my analysis as a description of a snapshot in time.[^cf17] Importantly, a model is a tool for thinking, and not necessarily a truth claim in itself: creating a model is a way of saying, ‘it might be helpful to think of X as Y,’ not an assertion that X is equivalent to Y. Willard McCarty[ need a citation for McCarty here] articulates this important feature of models by stressing that a model’s value is determined not by its exact correspondence with the object it models --- if it were possible to fully examine the underlying object, then no model would be necessary --- but by the *fruitfulness* of its simplifications. Even a deeply incorrect model can be fruitful if its divergence from observed phenomena rules out an incorrect theory. As I examine the many existing models of ‘English literature, 1789-1799,’ and create several more of my own, I articulate the underlying assumptions of each model, and assess the fruitfulness of the results.  

One of the slight embarrassments that attends on digital humanities research within the institution of the university, can be the extent to which truly insightful and influential related work occurs outside of that institution, and thus fails to conform to institutional expectations. This awkwardness is intensified by disciplinary differences, wherein even within the institution of the university, fields like computer science and literature value very different forms of publication. The ‘peak’ publication in literature, in its assumed rigor and importance, is the monograph, followed by the journal article, then perhaps a chapter in an edited collection. A conference presentation is hardly even a source to be cited in future work: these talks in literature are ephemeral, partly ad-hoc; one expects to cite, instead, the journal article which will follow a top-notch talk. This rough allocation of cultural capital to various forms of publication tracks with the level of peer review and the effort of composition involved with each. The more thoroughly scrutinized and edited a work is, the more potential value it is regarded as having: this is perfectly sensible.

The difference in computer science, then, is the difference in where the discipline carries out its own reviews. A conference paper in computer science will be blind peer-reviewed by external reviewers, with readers’ reports and revisions, based on a full manuscript (rather than just an abstract) before the paper is accepted to the conference. In other words, very nearly the process which occurs for a journal article in literature. Conferences in computer science have acceptance rates around 30%, though particularly prestigious conferences have acceptance rates around 15%. At the conference itself, rather than the many simultaneous panels of a literature conference, one paper is given at a time, which is assumed to the worth the attention of all attendees \[DOUBLE-CHECK THIS\]. When the full manuscripts of these presentations are then distributed in the conference proceedings, therefore, it is a top-tier publication. Computer science thinks of itself as a fast-moving field such that any research which went through the old-fashioned process of becoming a book is sure to be outdated by the time it’s printed: books are assumed to be textbooks for undergraduates, the sort of thing one produces as a career is winding down, to demonstrate broad expertise in well-established foundations \[DOUBLE-CHECK THIS\]. \[ALSO EXPLAIN ARXIV.ORG\]

Some digital humanities work, then, circulates in conference proceedings which can appear suspect in the literary field, and we have relatively few monographs (though that is starting to change). \[Also important: blog posts.\] \[Bode calls it the “gray literature”\]



## 1.4.  scope ##  

All of the computational work in this dissertation aims to identify, in as minute detail as possible, all works printed in England between January 1 1789 and December 31 1799. This eleven-year “decade” was a turbulent one across the Channel, encompassing the whole of the French Revolution, from the Estates General in 1789 to Napoleon’s coup in 1799.[^cf18] In England, these events caused strong and variously nationalist reactions in a country which had so recently lost its colonies in America and feared that a French invasion could come at any moment. This is the decade of *Rights of Man*, it is the decade of *Lyrical Ballads*; it is the decade of Hannah More, it is the decade of Ann Radcliffe; it was the age of wisdom, it was the age of foolishness; it was the epoch of belief, it was the epoch of incredulity. Charles Dickens’ now famous superlatives capture the tension often seen by scholars between ‘Enlightenment’ modes of writing and ‘Romantic’ or ‘Gothic’ modes, which are no longer neatly periodized as mutually exclusive.

Scholarship on 18thC works often takes the form of evaluating or assigning the cultural capital of individual works, or, perhaps, analyzing the strategies by which they accrue or fail to accrue that capital. The winners of the cultural capital game are the Romantics in poetry and Walter Scott in prose. For example, Simon Bainbridge examines the decade and its poetry through the lens of war to identify “the attempts made by several writers to fill the role of national bard prior to Scott” (3). Both poetry and the poet, in his conception, are pursuing a particular kind of cultural capital that allows them to rise above their own popularity. Richard Cronin’s *The Politics of Romantic Poetry* and Robert Miles’ \[WHICHEVER ONE IT IS\], too, seem to treat Scott’s \[intensely serious popular romances\] as the teleological end of the late eighteenth century birth development within the novel. These works follow a pattern established from the beginning with \[Kiely and Tompkins\], of treating the novel as synonymous with the realist novel, and treating Romantic and especially Gothic novels as aberrations in the history of the novel, a problem which needs to be explained away. E.J. Clery’s *The Rise of Supernatural Fiction* has examined at length the historical conditions by which supernatural plot elements began to make limited claims to literary seriousness throughout the eighteenth century. The “rise” she describes is not an increase in volume and prominence of supernatural stories, since her starting point in 1762 (the Cock Lane ghost) is a major national phenomenon with many imitators. Rather, supernatural fiction ‘rises’ when it acquires cultural legitimacy. Michael Gamer has more recently expanded on how this ‘rise’ fuelled Romanticism’s own rise. Gamer, like Bainbridge and Cronin, primarily examines Wordsworth and the ‘winners’ of the struggle for cultural capital: I, like Clery, am more interested in the ‘losers.’ Accordingly, I attend to much that is *not* literature, in order to better understand why it is not.  

Some limiting factor was necessary to make this project feasible from a technical standpoint. I needed to define a small enough scope that I could attempt something like comprehensiveness within that scope. I also knew that for many of the databases I wanted to use, I would not be able to access their full records, but samples of up to roughly 50,000 records had been given to other scholars (based on other papers I saw people publish on the ESTC). I decided that roughly a decade would give me enough texts to be worth approaching in this way, but not more texts than I could handle.

I narrowed my focus to England as a way to sidestep problems of metadata. The Ireland of the eighteenth century had an unstable and contested relationship with the United Kingdom: how is this addressed by metadata assigning countries to texts? Is something tagged as “UK” if the city is in the UK now, or if it was in the UK at the time? I assume now. I decided not to open this can of worms! “Future Work.” Narrowing to England also helps to reduce the number of texts being considered. I’m disappointed that this means excluding all those Scottish pirated editions. Additional Future Work could compare Scotland to England, or grapple with Ireland.

Once I decided to pick roughly a decade, I picked the 1790s for a couple reasons. I began in the eighteenth century as a Gothicist, and remain curious about how the Gothic might relate to its print context: choosing a decade that was important to the Gothic opened up the possibility that my ultimate findings would be Gothic-related. The 1790s were also just a generally exciting decade, as there was a massive expansion of print, and a massive cultural anxiety about the role of print in peoples’ lives. The nature of the project meant that I had to pick my decade before I really knew what to find in it: the heightened revolutionary stakes of this particular decade seemed to give me the best chance that the decade would have something interesting in it. It’s also an important decade for the eighteenth century texts which would later become canonical, namely, the origins of Romanticism.



### 1.4.1.  Charlotte Smith ###  

To navigate the 1790s, I turn to an author whose career and works usefully focalize my core questions of genre, publics, and the status of literature: Charlotte Smith. Smith was highly productive in multiple genres throughout the 1790s, and had a complex and contested literary legacy after the 1790s. As literary scholars re-assess ideas about literary seriousness, popularity, and women’s writing, our assessment of Smith has shifted as well. By examining their bibliographies with computational methods, I again ask how she might continue to look different if we look at her a different way. I particularly examine the extent to which digital resources have kept up with the re-evaluation of Smith as a central figure in British Romanticism.

\[All of this introduction to Smith remains rough to give a gist of the kind of material I want to cover here.\]  

In addition to being a prolific and interesting writing who was prominent across multiple genres (of which there are many other authors), Charlotte Smith offers an interesting case study in a ‘successful’ recovery project. This allows me to ask: to what extent do research infrastructures ‘lag behind’ scholarly consensus? No eighteenth centuryist is now likely to say that Smith is irrelevant or unimportant to the period. In the infrastructure of literary canons as described by Guillory, she has certainly succeeded: she is given prominent space in all anthologies of Romantic literature; she regularly appears on introductory syllabi, including surveys of all British literature; there are scholarly editions, and seminars, and dissertations, conference panels, and every other sign that she is an important and valued writer. But what about digital infrastructures? Are they “up to date”? For many I would say, not really. The ESTC ecosystem is still strongly shaped by editorial decisions made at the time of microfilming, or at the time of indexing. On Wikipedia, Smith herself had a respectable wiki article, but none of her major works were covered until I began to create those articles myself over the course of this dissertation. \[FOOTNOTE: One reason this dissertation does not engage with Wikipedia as an object of study, despite many interesting implications, is because I am too involved as a wiki editor to maintain an arms-length distance.\]  

Smith is also self-consciously navigating a series of questions about genre that interest me, namely, the fact that not everybody reads everything, or for the same reasons --- she is not the same author from genre to genre.

“Very few Smith scholars work actively on both the novels and the poetry, and consequently we have been learning about two separate Smiths, each closely linked to the genre she writes in, neither closely linked to the other. Because the novel during the Romantic period is undergoing an extraordinary amount of change and innovation, as it moves closer to its modern form, editors of the novels (myself included) tend to focus on Smith’s techniques and innovations, her use of tropes and themes, her facility with genres and description. Conversely, because Romantic poetry in the Smithian tradition is so closely tied up with explorations of selfhood and subjectivity, memory and a personalized past, editors of the poetry tend to present it as reflective of a personalized state of mind, of ‘woman’s’ experience, treating its manifold themes and narratives as, finally, reducible to and manifested from Smith’s life. **Is it all to do with inherent qualities of genre, or is it more to do with the expectations we as readers bring to different genres?** Genre, it seems, carries a greater force in constructing our preconceptions of identity than has been recognized, and Smith is a case in point, a case we can crack by studying closely Smith’s style and techniques *across* genres.” (Labbe 5)

Is poet-Smith different or separate from novelist-Smith of didactic-Smith? If so, does that tell us something about poetry or novels, aesthetically? About the marketing or consumption of poetry or novels?

One of my core interests is grappling with heterogenous groups of texts, which are usually examined each in isolation, and trying to bring them together. This might be one of those things that people don’t do because it’s a bad idea, rather than because it’s hard; maybe there aren’t any meaningfully questions that can be posed about All Writing. But I still wonder: are these things as separate from each other as we assume? Are we *really* dealing with different Smiths, or do we just *expect to find* different Smiths?  

Charlotte Smith is selected as a writer who was productive in multiple genres, only some of which may end up represented in corpora. Charlotte Smith’s literary career began with the publication of her volume of poetry *Elegiac Sonnets*, in 1784.[ maybe these whole bibliography sections should just be tables?] This work is the one upon which much of Smith’s fame and prestige rested in the eighteenth century. A second edition of *Elegiac Sonnets* rapidly followed the first in the same year, with only slight amendments. The third and fourth editions of *Elegiac Sonnets* appeared in 1786, adding new poems. 1786 also saw the publication of Smith’s *The Romance of Real Life*, a translation of *Les Causes Célèbres,* her first foray into prose, which would occupy the major part of the next phase of her career. In 1788 she published her first original novel, *Emmeline, or the Orphan of the Castle*. 1789 begins this dissertation’s decade of interest, a period of intense productivity for Smith: she had at least one new publication almost every year from 1789-1799. In 1789, she published her second original novel, *Ethelinde, or the Recluse of the Lake*, and a fifth edition of *Elegiac Sonnets*. In 1791 she published *Celestina,* her third novel; in 1792, her fourth novel, *Desmond*[ add info about the “phases” of her novelistic career], and a sixth edition of *Elegiac Sonnets*. Although *Elegiac Sonnets* continued to be reprinted, reaching its tenth edition in 1812, after this edition no further poems were added. Instead, her new poetry appeared in their own independent publications, and no longer took the form of sonnets. In 1793 she published *The Emigrants*, a poem in two volumes, as well as *The Old Manor House*, her fifth novel. In 1794, her sixth and seventh novels, *The Wanderings of Warwick* and *The Banished Man*. In 1795 she published her eighth novel, *Montalbert*, and began writing in a new genre with *Rural Walks*. With *Rural Walks*, Smith’s dominant genre again changed: having gone from a poet to a novelist, she now primarily published in a form which does not have a contemporary name: morally instructive natural history for “young persons.” 1796 saw the sequel to *Rural Walks*, *Rambles Farther*, as well as the novel *Marchmont*, and the poem *A Narrative of the loss...* of several ships. 1797 saw the eighth edition[ when was the seventh???] of *Elegiac Sonnets*, unchanged since the sixth. 1798 saw the novel *The Young Philosopher*, and more natural history for children in *Minor Morals*. In 1799, Smith tried her hand at theatre with *What Is She?*, a comedy --- not a form she will revisit. After this dissertation’s decade of interest, Smith continued to write at a slightly less frenetic pace. In 1800 she published the first three volumes of *Letters of a Solitary Wanderer*, an epistolary anthology of narratives. In 1802 she published two additional volumes of *Letters of a Solitary Wanderer.* In 1804, she published *Conversations, Introducing Poetry*, for children. In 1806, Smith published *History of England*, another work for young persons, and Smith herself died, age 55. The next year saw the posthumous publication of the poem *Beachy Head* and the work for young persons, *The Natural History of Birds.*  

Smith’s personal life sometimes overshadows this career. As her works often make clear to her readers, after a briefly comfortable youth as the daughter of a well-off country gentleman who lived beyond his means, she was married at age sixteen to Benjamin Smith, “son of a prosperous London merchant and owner of Barbados sugar cane plantations. The marriage was contracted hastily to remove her from her paternal home, now dominated by her new wealthy stepmother. Looking back in bitterness nearly forty years later, Charlotte Smith described the event as her father's decision to sell her like a ‘legal prostitute, in my early youth, or what the law calls infancy’ (Smith to Sarah Rose, 15 June 1804)” (Roberts). Benjamin Smith was cruel and violently abusive. He was also so financially irresponsible that his wealthy father, Richard Smith, wanted to prevent Benjamin from inheriting. Charlotte Smith assisted Richard with business correspondence and impressed him as responsible and competent. In recognition of her husband’s unreliability, “she persuaded \[Richard\] to relieve his son of all his ties to the business and establish him as a gentleman farmer in Hampshire” in 1774 (Zimmerman). Richard Smith died in 1776. “In an attempt to provide for his daughter-in-law, Richard bequeathed the bulk of his property to her children. But he had drawn up his will without professional advice; legal wranglings over the inheritance worth nearly £36,000 soon arose and were not settled until almost forty years later. By 1783 Benjamin had already unlawfully squandered more than a third of this trust and, as a consequence, found himself first in deep debt and then in King's Bench Prison.” (Roberts). After the success of the *Elegiac Sonnets* allowed Smith to pay for her husband’s release from prison, Benjamin Smith fled to France to escape further creditors. Charlotte Smith moved between England and France over the next year and a half to negotiate his debts, and in 1785, the family was able to return to England. In 1787, after 22 years of marriage, Charlotte Smith legally separated from her husband, “an unusual step for a woman of her time” (Fry 7), and moved to a town near Chichester with her nine surviving children (of the twelve she had given birth to). However, despite this separation, Benjamin Smith retained a legal right to Charlotte Smith’s profits from her writing. Smith moved frequently after her separation, due to financial instability and declining health. “On 23 February 1806 Benjamin died in a debtors' prison and some money reverted to Charlotte Smith. By then she was far too ill to execute her favourite scheme, to settle on the shores of Lake Leman. On 28 October 1806 she died, only eight months after her husband, and seven years before Richard Smith's estate was finally settled.” (Blank)  

Smith’s posthumous critical reception has undergone multiple shifts in appreciation and obscurity. Duckling’s study of her presence in anthologies indicates that shortly after her death in 1806, Smith was widely eulogized and anthologized, remembered and emulated as an important British poet. As the nineteenth century went on, poetesses began to be anthologized separately from poets, in collections with ambitions that were commercial rather than intellectual; Smith, too, “lost intellectual ground” even as she continued to be sold (Duckling 2016). By the end of the nineteenth century, even these volumes marginalized Smith’s poetry, with prefatory material which dismissed them as trite and depressing, unenjoyable reading. In the early twentieth century, Smith began to be considered as a novelist, rather than a poet; this new field did not lead at first to a much better reputation for her. Florence Hilbish produced the first extensive study of Smith, considering her as both poet and novelist, in 1941, to unappreciative reviews: Ernest Bernbaum’s faint praise said that “‘much time and care have been devoted to it; whether deservedly, is perhaps questionable,” since “the subtle or intricate is absent from Charlotte Smith's writings” (138). Hilbish presents Smith’s emotional poetry as sincere rather than conventional, and her prose as more motivated by politics than commerce.

Duckling credits the feminist movement of the 1960s and 1970s with the beginning of Smith’s recovery (217): the renewed interest in women’s writing rediscovered her novels, and especially the radical political content which Hilbish had observed. At the same time, Bishop Hunt published a record of Smith’s influence on Wordsworth, as demonstrated by an almost overwhelming amount of physical evidence: Wordsworth owned copies of her works, which he annotated; he copied out some of her sonnets in his own hand; he paid her a personal visit; he edited some of her poetry for publication; he wrote explicitly of her influence in notes to his works. Hunt calls Smith “an important early influence on Wordsworth which has not been explored in any detail up to now” (85); his abstract somewhat snarkily asserts that “Wordsworth did not suddenly start writing sonnets in 1802 simply because he happened to read Milton’s.” However, Hunt has little praise for Smith herself: of one poem, he says, “Whatever the artistic value of such verses,” what matters is the underlying theme which Wordsworth would later express more masterfully (89). Smith continued to be treated separately as an interesting woman novelist, and a minor pre-Romantic poet, through the 1980s. Smith rose to greater prominence in both of these fields in the 1990s: with work by Stephen Curran, Roger Lonsdale, Jennifer Breen, Andrew Ashfield, and Jacqueline Labbe, “Smith became established not only as a prominent figure in the revised female canon, but also as a central figure in Romanticism” (Duckling 217).

Throughout this history, two aspects of Smith which have prompted frequent re-assessments are her personal life, and her work across genres. The first matter, the importance of a female author’s life as a woman to her importance as a figure worth remembering, is implicit in several phases of the rise and fall described above. Fry is not alone in concluding that “\[f\]ew writers have presented themselves in their works so fully as did Charlotte Smith” (3): Smith’s poetry lyricizes her personal experiences, her novels feature autobiographical stand-in characters, and “the often intensely personal pleading prefaces” (Behrendt 189) to her works explicitly ask for them to be read light of her ongoing struggles. Perhaps as a result, much scholarship on Smith takes the stance of *The Literary Encyclopedia* in defining her as a woman who wrote because of, and chiefly about, her personal distress. Antje Blank’s article there highlights Smith’s financial motive to write: “Smith turned to writing when a failing marriage and a costly lawsuit left her without resources to raise her large family” (Blank). “And so,” Blank says, Smith “churned out” her novels (and the many editions of *Elegiac Sonnets*, and her other poetry, and her educational writing) to support herself and her nine children (Blank). Even when Smith’s Elegiac Sonnets “won her the reputation as an author of serious verse,” this is important primarily because it “lent greater respectability to her ensuing productions in a less prestigious but more lucrative genre -- the novel” (Blank). At the same time, as Labbe argues in her article “Selling One's Sorrows: Charlotte Smith, Mary Robinson, and the Marketing of Poetry,” Smith cultivated a public persona as a paragon of victimhood and motherhood, suffering deeply but turning her suffering into marketable prose out of a duty to her children. In periods where this image of womanhood is valuable, Smith is more easily valued, as in the eighteenth and nineteenth century anthologies which saw Smith as a moral exemplar (Duckling 203-4). Or, in periods when women’s resistance to patriarchal oppression is of scholarly interest, the direct, personal nature of Smith’s writing is valuable in itself, as in early feminist scholarship.

A complicating factor to these evaluations of Smith is that, as Labbe’s edited volume *Charlotte Smith in British Romanticism* thoroughly demonstrates, Smith’s writing is neither as uniform nor as simplistically personal as autobiographical readings sometimes see it. Labbe contends that Smith-the-novelist and Smith-the-poet have been largely studied as separate entities, “and consequently we have been learning about two separate Smiths, each closely linked to the genre she writes in, neither closely linked to the other” (5). Labbe is not quite the first to attempt to unify Smith: Carol L. Fry’s 1996 monograph *Charlotte Smith* also addresses her poetry before moving on to the several phases of her novel-writing, including the children’s writing which made up much of Smith’s later career but does not appear in Labbe. Indeed, from the beginning, Hilbish’s 1941 monograph explicitly identifies Smith as “Poet and Novelist” in its title. However, Labbe is accurate regarding the somewhat different assessments of Smith current in the somewhat separate study of novels and of poetry in general: Labbe argues that as a novelist, Smith is now often praised for her innovative narrative techniques (implying a mode of writing that is intellectual and ‘distant’), whereas as a poet, she is praised for her innovative expressions of interiority (implying a mode of writing that is emotional and ‘close’). Labbe draws greater attention to important differences between Smith’s writing personae in different genres, and her edited collection “pulls together many Smiths” (2) to address these disjunctions. The volume not only addresses her novels and poetry, but also includes her plays, letters, and posthumous reception. Each of these Smiths, the volume contends, has something innovative and unexpected to reveal, important to the formation of British Romanticism. In Judith Phillips Stanton’s “Recovering Charlotte Smith's Letters,” for example, Smith’s letters, less studied, reveal a third kind of writer, different from both the novelist and the poet, who conceives of herself as a professional businesswoman of her craft. More Smiths are available in genres not included in this volume, such as Smith the naturalist and children’s author (touched on only lightly in Labbe’s volume), or Smith the political philosopher who drives Amy Garnai’s *Revolutionary Imaginings in the 1790s,* a highly political Smith who consciously participates in the “political public sphere” conceived by Habermas, despite Habermas’ insistence that women were excluded from this sphere (1)*.*[ \[Something about each Smith having her own peers...? Political Smith now becomes peers with Mary Robinson and Elizabeth Inchbald, whereas Poetic Smith is peers with Wordsworth & Coleridge, and Novelistic Smith with Radcliffe etc\]] From these distinctions, Labbe concludes that “Smith, significantly, composes herself anew according to genre” (2) --- and then asks, “Is it all to do with inherent qualities of genre, or is it more to do with the expectations we as readers bring to different genres?”[ Labbe’s immediate answer: “Genre, it seems, carries a greater force in constructing our preconceptions of identity than has been recognized, and Smith is a case in point, a case we can crack by studying closely Smith’s style and techniques across genres.” (Labbe 5)] (5). This question about genre is one of the initial questions to inspire this dissertation: to see it asked as a core question about Smith demonstrates Smith’s suitability as a figure whose career can shed light on important questions about the mediascape of the 1790s.



### 1.4.2.  databases ###  

A core object of study for this dissertation is the makeup and history of contemporary digital databases. Eighteenth-century materials of various kinds have been collected in many digital archives, of very different scopes. I will draw materials from the English Short-Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the ECCO Text Creation Partnership corpus (ECCO-TCP), Google Books, Project Gutenberg and HathiTrust. My examination of these six databases will, of necessity, examine a ‘time capsule’ of their holdings at a particular moment; the sources of my data, and my procedures for working with them, are described in more detail in Appendix B (“Methodology”). The databases vary from each other in terms of two main qualities: their size, and their reputation. The reputation of any given digital resource is shaped largely, I argue, by its ability to signal ‘rigour’ in its collection practices. Several databases of different sizes have established reputations of seriousness, and, correspondingly, cultural capital within scholarly communities. The databases that I will examine at length form two groupings of three each, to explore two sets of related concepts. The first set consists of ESTC, ECCO, and ECCO-TCP, all of which follow the same rigorous collection practices at different scales. The second set consists of Google Books, HathiTrust, and Project Gutenberg, which follow very different collection practices while sharing a dubious scholarly reputation.  

The first three databases I examine will be no surprise to eighteenth-century scholars: ESTC, ECCO, and ECCO-TCP. Gale’s Eighteenth Century Collections Online (ECCO), contains over 180,000 titles 1701-1800, of which 42,000 were printed in England between 1789 and 1799. ECCO is itself (mostly) a subset of the broader English Short Title Catalogue (ESTC), which contains more 460,000 texts 1473-1800, of which 51,965 were printed in England between 1789 and 1799 (indicating that nearly 10,000 titles in the decade appear in the ESTC but not ECCO). The ESTC does not provide access to texts themselves: instead, it is an authoritative bibliographic catalogue, available as a searchable database. It is ECCO which provides texts: ECCO’s 180,000 titles works are available as photographed facsimiles of the full text of each title. The facsimiles can be searched within ECCO’s online interface; these searches examine a plaintext version of the facsimile pages that was generated by Optical Character Recognition (OCR), but this OCR text is not made directly available. As a result, the facsimiles may be read individually by scholars, but cannot form the basis for computational corpus analysis. A subset of ECCO’s texts have been hand-prepared, as part of the Text Creation Partnership (TCP), to be easier to use in computational research. The resulting corpus of ECCO-TCP texts contains 2,231 titles, of which 466 were printed in England between 1789 and 1799. These titles are available as carefully edited texts encoded according to the Text Encoding Initiative (TEI) standard, which not only provides an accurate version of the text’s words, but encodes substantial details regarding its context on the page. Most large scale distant reading of eighteenth-century literature relies on the ECCO-TCP corpus as its ‘model’ or ‘sample’ to represent the period[ footnote some examples]. Accordingly, one of the tasks of this dissertation is to examine the makeup of this corpus, and how it differs both from other corpora and from print culture in the period itself. These three digital collections --- ECCO, ESTC, and ECCO-TCP --- are the primary digital resources for the period, which form the basis of most digital research. However, they represent only one approach toward the collection and presentation of digital texts, to which there are two broad kinds of alternatives. These large but meticulous collections occupy a middle space between, on the one hand, highly selective thematic collections, such as The Shelley-Godwin Archive, of which there are many, and the giants of indiscriminate textual accumulation, such as Google Books, of which there are few.  

Smaller collections allow for more scholarly curation, but have corresponding limitations. Whereas the ‘main players’ of the the mega-archives can be easily enumerated, these specialized collections are numerous. Some will focus on particular kinds of texts, such as the Early Novels Database (2,041 novels 1700-1799) or Broadside Ballads Online (more than 30,000 broadside ballads). Others exhaustively index particular publications, such as *The Hampshire Chronicle* (1,950 references to fiction in issues from 1772-1829), the Index to the *Lady’s Magazine* (14,729 articles from 1770 to 1818), or the Novels Reviewed Database (1,836 reviews from *The Critical Review* and *The Monthly Review*, 1790-1820). Feminist scholarship in particular has seen the creation of resources like the Orlando Project, the Chawton House library Novels Online, Northeastern University’s Women Writers Online and UC Davis’s British Women Romantic Poets. The virtue of these collections is that they achieve even greater accuracy and comprehensiveness within their defined scope. The Shelley-Godwin Archive, for example, can reasonably aspire to digitize *every* known manuscript of Percy Bysshe Shelley, Mary Wollstonecraft Shelley, William Godwin, and Mary Wollstonecraft, and to provide these manuscripts in hand encoded plaintext transcripts. However, as is inevitable, these specialized archives have the vices of their virtues: their specialized focus allows them to adapt precisely to their materials, and their idiosyncratic data structures can rarely be combined with other resources. The William Blake Archive, for example, benefits enormously from designing its archive around the unique images of each page of each copy of each of Blake’s works. But because this approach is so well suited to Blake, it cannot be applied beyond Blake. Even if the archive’s resources were available for download, they could not be directly compared to materials from another source which does not record its information at such a minute level of detail. As a result, although a great deal of excellent digital scholarship is contained in specialized micro archives, I do not examine them further in this dissertation.  

Instead, I look at a set of larger archives of more contested “scholarly” status: Google Books, Project Gutenberg, and HathiTrust. Google Books may be the most infamous database of books. In a scholarly context, one hesitates even to designate this as an “archive,” particularly in the same breath as resources like ECCO: books of all kinds are scanned indiscriminately with only the bare minimum of roughly accurate metadata collected about them. These rapidly scanned books are prone to unpredictable errors, including inaccurate dates, misspellings, duplicate copies, and inaccurate subject classifications[^cf19] --- infamously, many books have “1899” assigned as their publication date because this date was used as a placeholder for “no date”.[^cf20] Nonetheless, Google Books is frequently used to study the prevalence of various “n-grams” (words or short phrases) over time, thanks to Google’s built in tool. The tool is able to search books which are, for copyright restrictions, not available directly to readers, making it highly tempting for questions about contemporary language use.  

Also in the category of smaller and specialized archives is Project Gutenberg. Project Gutenberg makes no claims to scholarly reliability but nonetheless underlies a not-significant amount of scholarly work[^cf21] --- its cultural capital as a resource lags far behind its use and utility. Project Gutenberg is easily conceived of as a haphazard, ‘unscholarly’ source for materials, but unlike Google Books, Project Gutenberg actually does have selection criteria. Project Gutenberg will only collect public domain works which contemporary audiences might be interested in reading for pleasure. It narrows the field substantially to exclude works which have either ceased to be broadly interesting (as in the case of most forgotten fiction), or which were never particularly interesting (as in the case of almanacs and tax codes). Project Gutenberg includes 57,796 texts: far more than specialized scholarly archives like the Early Novels Database or the Shelley-Godwin Archive, but nonetheless an order of magnitude fewer than its more voracious potential competitors. And, like smaller specialized scholarly archives, Project Gutenberg has tailored its holdings to make it easy for readers to read, and quite difficult for its collection to be applied to any other use. By tailoring the structure of the archive itself to its specific materials, these collections are able to thoughtfully achieve their aims --- but they also make it correspondingly difficult for users[ does this not depend on the user? unpack] to achieve their own, different aims.  

What makes Google Books of interest in the context of this dissertation is its relationship to HathiTrust, an increasingly popular resource for scholars. HathiTrust’s collection contains digitized content from “a variety of sources, including Google, the Internet Archive, Microsoft, and in-house member institution initiatives.” The “in-house member institutions” include one hundred and fifty-five universities, colleges, and consortia of universities. The aggregate scholarly authority of these institutions carries the weight of elevating HathiTrust above the Google Books scans which form the backbone of much of its contents: “The members ensure the reliability and efficiency of the digital library,” the website assures us, “by relying on community standards and best practices.” The texts themselves are stored in the database as facsimile page images and full-text OCR transcripts. In order to comply with copyright law, however, HathiTrust only provides large scale downloads and OCR transcripts for texts which are in the public domain. Most scholars use HathiTrust to run experiments on OCR transcripts of copyrighted texts, which they can only access through computational workarounds that intentionally make it impossible for the scholar to see the full transcript itself.[^cf22] These tools provide a unique solution to real barriers for digital scholars of contemporary literature: although copyright law would make it prohibitively expensive or even impossible to build corpora of post-1920s literature, HathiTrust’s mediated access to these texts enables corpus analysis. Through its collection, HathiTrust provides a hodgepodge of texts, of often unverifiable provenance and accuracy, selected largely by happenstance and convenience in a quest to contain all printed books. Through its tools, however, and through its institutional affiliations, HathiTrust has acquired a cultural capital among scholars which Google Books still lacks.  

HathiTrust’s success in acquiring scholarly capital stands in interesting contrast with Project Gutenberg’s continued lack of cachet. Project Gutenberg is used in research with similar frequency to Google Books’ n-gram tool,[^cf23] but scholars often mention Project Gutenberg with a note of apology for not having found a better source. Its cultural capital as a resource lags far behind its actual use and utility, likely, I argue, because its organizing principles are the ‘unserious’ ones of popularity and pleasure. Project Gutenberg is easily conceived of as a haphazard source for materials, but unlike Google Books, Project Gutenberg actually does have selection criteria. Project Gutenberg will only collect public domain works which contemporary audiences might be interested in reading for pleasure. This criteria might not render Project Gutenberg more useful for scholarly work but, it nonetheless narrows its selection substantially. Project Gutenberg includes 57,796 texts: far more than specialized scholarly archives like the Early Novels Database or the Shelley-Godwin Archive, but an order of magnitude fewer than its more voracious potential competitors. In taking Project Gutenberg seriously as a collection of texts, I seek to explore the extent to which its reputation as “unreliable” may or may not be deserved.  

As this brief survey of eighteenth-century digital archives shows, there is no ‘perfect’ corpus for large scale study of eighteenth-century texts.  Moreover, I argue, the imperfect samples which each archive provides are shaped not only by historical factors of eighteenth-century print culture, but also by contemporary digital culture. Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. As this dissertation will argue, these questions of digital history have important resonance with literary questions about literary canon formation.



## 1.5.  dissertation map ##  

Chapter two describes in more detail the databases to be studied, and examines Charlotte Smith and the ways that her writing is made accessible today. The specific experimentation undertaken in chapter two tests the basic assumptions and methods of my project. I begin with a the histories of the ESTC, ECCO, ECCO-TCP, Project Gutenberg, Google Books, and HathiTrust: highlighting the chronological relationships between these resources can explain each database’s scope and technical implementation. Each new resource must contend with the possibility of either competing or collaborating with those which have come before. Examining materials like the meeting minutes and internal communications of these resources’ early histories will show how those which currently enjoy the lowest reputation among scholars --- Project Gutenberg and Google Books --- defined their initial scope around an explicit rejection of scholarly norms. After establishing the history of each resource’s development, I describe its current digital infrastructure, through the lens of critical algorithm studies. This begins with basic questions: what file formats does it use? What kinds of metadata, what ontologies? How does it make its materials available for use? Through close reading and comparison of these details, I articulate each database’s implicit construction of what a text is and what it is for. Combining each resource’s history with its technical infrastructure, I return to 

Having established these databases as objects of study, I identify what subset of Smith’s works each corpus contains, as a concrete example to compare their holdings overall. Smith’s *Elegiac Sonnets*, for example, are not included in the ECCO-TCP corpus (which is the one most often used for text mining research) --- only *Celestina* and *The Emigrants* are included. Why these two texts? And what text mining research based on ECCO-TCP might have found slightly different answers if Smith’s sonnets had been included? As a related test of comparison between databases, for each database which provides access to the actual text of Smith’s works, I compare the textual similarity of *Celestina* and *The Emigrants.* What editorial choices are being made? How *much* worse is the OCR text than the transcribed text? Another key concept I will explore through Smith is the role of reprints. HathiTrust, for example, includes multiple editions of *Elegiac Sonnets*. How reliable and effective are its distinctions between editions? How do the databases I examine handle multiple editions of a single work? I am particularly interested in how reprints can be incorporated into our understanding of what literature is “of” a particular decade: what does it mean to think of *Elegiac Sonnets*, initially printed in the 1780s, as “1790s literature”? Finally, having surveyed my six databases with the help of Smith, I discuss the multiple “Smiths” which emerge, and what it means to attempt to unify her disparate works.  

In chapter three, I re-examine my core databases, but no longer with Smith as a focalizing lens. Instead, I undertake computational assessment and comparison of the databases’ contents. My research examines the authorship and subject matter (broadly construed) of all titles printed in England between 1789 and 1799 which are included in each database. I calculate the proportion of the titles in each resource that are attributed to men, to women, or are left unsigned. My naive hypothesis is that, as each resource demanded a greater investment of scholarly effort in each text, women and unsigned writers will grow increasingly underrepresented, so that the ECCO-TCP corpus will have substantially different demographics than the ESTC. Using the titles of these works and a topic modelling tool which I have built, I also roughly identify the subject matter of each title, categorizing works into broad genres such as drama, poetry, Romance, History, or sermons. Although the topic modelling tool is able to cluster what it sees as “similar” titles, individual interpretation is required to make these clusters meaningful. A substantial portion of chapter three is dedicated to discussing how scholars apply genre categories retrospectively to clusters of texts, how publishers sought to advertise their texts to particular audiences, and how the categories I develop ought to be understood in the context of existing eighteenth century scholarship on print genres.[^cf24] Using my resulting genre classifications, I am then able to compare these four resources to each other, and to existing scholarly work on the print production of the 1790s. For example, I compare each resource’s holdings to the statistics on the English novel included in Garside, Raven and Schöwerling’s *Bibliographical Survey of Prose Fiction*. In examining genres, I anticipate discovering a preference, in the more specialized resources, for more “literary” forms of writing.

I will also correlate gender and genre. This preference for particular kinds of writing might explain changes in gender representation of smaller corpora. If the novel is the domain of women, for example, a corpus can underrepresent women by underrepresenting novels. Or it could include a representative number of novels, but disproportionately include novels by men. My investigation allows me to identify the patterns of exclusion. Asking bibliographical questions of multiple corpora, in order to learn about the corpora themselves, emphasizes an under-examined stage of text mining research, and provides a basis for other scholars to use these corpora more precisely.  

In my fourth chapter, I playfully attempt what might be considered a devil’s advocate method of textual selection: pure random sampling. Using a random number generator, I select arbitrary texts to close read, and weave together a narrative of 1790s print from their contents. Much of my work will involve defining and justifying the parameters for my random selection --- ESTC, or a full-text database? How many texts? From which years? --- but once I have taken my sample, I will not re-sample. For each text, I explore the path which brought it into the databases in question, and what scholarship (if any) might be used to interpret it. How far afield do I have to look, to find scholarly conversations addressing each text? What, if anything, can be produced by placing them in conversation? This methodology is inspired by work in the field of speculative computing, a practice of creating strange and possibly non-functional programs in order to generate productive forms of surprise.  

A final brief conclusion to this dissertation offers an assessment of the role of digital textual collections in contemporary literary study.



# ch 1 - history of databases (13k) #  

This chapter will describe the primary object of study of this dissertation, namely, contemporary digital databases with substantial holdings of 1790s literature. These databases require substantial description, preliminary to analysis, since they have conventionally been treated as tools for accessing objects of study, rather than objects of study in themselves. Figure 1 shows a venn diagram of the approximate relative scale, and overlap in holdings, of the eight databases in three ecosystems which I will explore. In blue is the purely academic ecosystem: the English Short Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), and the ECCO Text Creation Partnership (ECCO-TCP). In pink is the commercial Google-backed ecosystem: Google Books, Google Ngrams, and HathiTrust. And in green is a crowdsourced ecosystem: Project Gutenberg. The division of databases into these ecosystems represents my own analysis of the institutional processes and selection principles which have shaped them. Databases within a shared ecosystem may or may not be interoperable, but they made their initial textual selections with a similar logic, and make make their holdings available for a similar imagined audience. An immediate difference between them, for example, is that the commercial ecosystem treats textual holdings like a trade secret; my discussion of Google Books and Google Ngrams will be primarily a discussion of barriers and lacunae. Google Books and Google Ngrams are of crucial importance, however, for understanding HathiTrust, whose presence in my diagram of the commercial ecosystem runs counter to its current highly academic reputation. This chapter will begin with a discussion of the theoretical stakes involved in taking a system of databases as one’s object of study. To illustrate these states, I will examine the current scope and data structure of each database, and the holdings of works by Charlotte Smith that each contains. Then I will present a chronological history of these nine databases. The chapter concludes by returning to this map of database ecosystems, to examine the role of commercial forces in defining each of the three clusters. As I will show, these resources are different in their core assumptions --- so different it can even be difficult to bring them into simple comparison with each other --- as a result of three different strategies of response to the economic forces of the emerging internet.  

![][fig-databases-venn]

Figure 2: A hand-drawn venn diagram of the relative scale and overlapping holdings of databases containing 1790s literature. The orientation and colour-coding of the ovals groups them into three ecosystems: the academic databases in blue pointing left, the commercial databases in pink pointing right, and the crowdsourced databases in green oriented horizontally.



## 2.1.  why write a history of a database? ##  

It is a common critique of Digital Humanities research that computational methods, or computational evidence, go hand in hand with claims of scientific, empirical, objective truth. Johanna Drucker’s influential “Humanities Approaches to Graphical Display,” for example, suggests that even by drawing a bar chart or line graph like those which appear in scientific journal articles, a humanities scholar goes astray. Drucker introduces a valuable contrast between “data” (an essentially imaginary phenomenon, of unmediated true information which a realist, empiricist researcher can accept as “given”) and her term “capta” (the true form of all data, partial and ambiguous information, “captured” and constructed for a particular researcher’s purpose). Drucker’s distinction between data and capta is widely cited, especially by DH researchers who then go on to refer to their capta as data throughout. Drucker’s core argument has, essentially, two parts:

...the basic categories of supposedly quantitative information, the fundamental parameters of chart production, are already interpreted expressions. But they do not present themselves as categories of interpretation, riven with ambiguity and uncertainty, because of the representational force of the visualization as a “picture” of “data”. (12)

The first claim, that “data” is always already interpreted, is one that continues to animate and trouble DH scholarship. The second claim, that the primary site at which capta masquerade as data is through visualization, has received less uptake. In chapter two, as I produce visualizations of the role of women’s writing in 1790s literature, I will return to Drucker and the role of data visualization in humanist research. It is certainly possible for visualizations to disguise the constructed nature of the underlying information, but, as Katherine Bode has recently shown in detail, the illusion of unmediated truth in computational research runs deeper than visual display.  

Bode’s article “The Equivalence of ‘Close’ and ‘Distant’ Reading” sets out to explain how distant reading, as a particularly headline-grabbing subset of DH, became so firmly associated, especially in the minds of its critics, with “the view of distant reading as enabling direct and objective access to a comprehensive literary-historical record.” (78) Bode contends that the exaggerated vision of distant readers who finally discover literary truths without getting bogged down by the pesky details of actual books --- oracles who deliver scientific objectivity to rescue literature from its messiness --- is not a vision wholly fabricated by media outlets and naysayers, but one fostered by the most prominent distant readers’ unacknowledged New Critical roots. Bode’s extended analysis of Franco Moretti and Matthew Jockers’ work contends that they “take the core premise of the New Criticism -- that the text is the source of all meaning -- to an extreme conclusion” (91) by relying on the accumulated contents of works of literature to explain the history of literary production. In the “Slaughterhouse of Literature” chapter of *Distant Reading*, for example, Moretti asks why Arthur Conan Doyle has become the canonized figurehead of early detective fiction. The mechanism for canonization that he proposes is that readers chose to continue reading Doyle over the years, in preference to other mystery writers, thus keeping Doyle “alive”; they made their selection based on formal features of Doyle’s stories; and specifically, Moretti suggests, they selected for the formal feature of “decodable clues.” Bode takes issue with this line of reasoning from its very first step: “Moretti takes as transparently true the idea that authors who have a canonical status in the present were selected from the time of first publication,” an assumption immediately countered by, for example, the relatively small readerships for five of the “big six” Romantic poets ([^cf25]90). Then, Moretti’s assumption relies on taking a historical, social process (cultural capital accruing to “canonical” works in different periods), and translating it into a purely formal, textual phenomenon to be observed directly within the work itself.[ eventually also write about Jockers?



Jockers, too, computationally models literary history as “a dispersed linguistic field” (92)] Bode’s final critique is about New Criticism directly, but applies equally well to this version of distant reading: 

...the assumption that literary works are texts, and that texts are single, stable, and self-evident entities, dismisses the documentary record’s multiplicity, and with it the critical contributions of those disciplines (particularly bibliography and scholarly editing) that are dedicated to investigating that multiplicity. (92)

What Bode reveals, then, is that the monolithic concept of “data” can (as Drucker says) “\[collapse\] the critical distance between the phenomenal world and its interpretation” (1) with seductive ease regardless of whether visualization has been involved.  

Bode’s monograph, *A World of Fiction: Digital Collections and the Future of Literary History*, develops her proposed alternative. Answering her own call for the construction of scholarly editions of literary systems, Bode meticulously explains the corpus she builds to represent early Australian periodical fiction. \[TODO-WRITE: summarize Bode’s monograph\] As one encounters her arguments about Australian literary history, therefore, each claim is very specifically situated as a preliminary claim, hypothesizing about Australian literature through the mediating lens of her textual samples.  

It is increasingly *de rigueur* for distant reading work to carefully discuss its underlying “corpus building.” The terminology of “corpus building” goes perhaps halfway to the ideal humanistic relationship to knowledge which Drucker and Bode envision. By referring to the process as *building*, and discussing it as it own methodological step, scholars acknowledge their active intervention in creating the information they will then analyze. In Ted Underwood’s monograph *Distant Horizons*, for example, he models many different kinds of literature to explore how texts might be distinguished --- fiction vs non-fiction; popular vs literary; “genre fiction” in its many forms --- and each model is first described in terms of how the definition of each group has been translated into a method of textual selection. \[TODO-WRITE: explain how he defines “reviewed” fiction\] All of this explicit discussion of corpus building serves to temper the strength of his arguments, cutting off the impossible illusion of “direct and objective access” (Bode “Equivalence” 78).  

However, common ways of discussing corpus building also serve to reify the corpus, and isolate it from other factors. 

Other terms could be “textual selection” or “sampling”

It is increasingly possible for scholars to publish *just* a corpus, separately from any analysis of it. \[TODO-RESEARCH: just quickly list 3-5 corpora here.\]  

A “corpus” is a body that cries out for *Frankenstein* metaphor: a collection of disparate parts, joined together and then animated to fend for themselves. This metaphor is sufficiently apt and illuminating that it is worth attending to Victor Frankenstein’s exact design principles as he built his own corpus:

As the minuteness of the parts formed a great hindrance to my speed, I resolved, contrary to my first intention, to make the being of a gigantic stature, that is to say, about eight feet in height, and proportionably large. After having formed this determination and having spent some months in successfully collecting and arranging my materials, I began. (CITE FRANKENSTEIN)

Important details: Victor makes a questionable design choice due to challenges with his source materials. (Victor’s ambitions, in the future scholarship he hopes to seed, are also perhaps uncomfortably familiar)  

What is “out of scope” for almost all discussions of corpus building, even when framed as textual selection or sampling or modelling, is how the scholars go from the list of texts they wish to examine, to the digital surrogates for those texts. To be more explicit: underneath many a carefully-curated well-theorized corpus is, usually, a database. Again Bode is unusual, because she describes the creation, maintenance, lacunae, and even the irregular funding of the *Trove* database underlying her own corpora. More common is an approach like Underwood’s, where methods sections dead-end in HathiTrust or, perhaps, the British Library. 



“the suggestion implicit in databases such as EEBO and ECCO that this deluge of texts they make available is complete, constituting a form of universal coverage, leads the average users, especially the average students, to be lulled into a false sense that they now have access to “everything.” This in turn reinforces a belief that if you don’t find something---an author, a text, and a document---in these massive digital repositories, it did not exist” (Ezell 9)



For essentially practical reasons, moreover, researchers often work within just one database ecosystem.



This, then, is why the rest of this chapter will tell a long and perhaps even excruciatingly detailed history of databases: because the implicit arguments of textual selection must be made explicit, and a database, too, carries an implicit argument in its textual selection.  

“However, the path to digitization was not ideal: these documents were imaged in the late 1970s, transformed into microfilm during the 1980s, and the microfilms digitized in the 1990s. Because of the state of reproductive technologies during the late 20th century, as well as the circuitous path to digitization (through microfilm), the image quality is very poor and bitonal, with no greyscale / images available. Furthermore, the original documents themselves, printed with premodern technologies, pose problems even for human readers of their pages, but much more so for optical character recognition (OCR) engines. For example, printed characters were not perfectly situated on a baseline, blackletter fonts were used, ink bled through the paper, and the typeface was broken and overworn.” (Christy et al. 1-2)

Reference “deep fried memes” --- xkcd comic: <https://xkcd.com/1683/>



The punchline of this comic strip is a classic “form vs content” joke: the literal words relay a techno-optimism that is undercut by the visual decay of the image itself, which is meant to remind the comic’s audience of current state of older comics or memes that circulate on the internet. The first two panels are merely low-resolution, reflecting purely technological trends of digital data loss. Even without data loss, an old image is likely to look damaged in a modern context, as standards for “high resolution” have climbed with every year; at one time, it was pointless to save any digital image at higher resolution than 72dpi, since 72dpi was the maximum a screen could display. Today, Apple’s Retina displays commonly show 227dpi, and mobile phones go up to 500dpi. A perfectly unaltered 72dpi image from twenty years ago can easily look blurry and out of place on a modern website. As another way that the ordinary passage of time degrades digital images: although it is possible for bits to be copied exactly (as the comic’s main character says), in reality most image files are compressed, uncompressed, and recompressed as they are shared between computers, especially if they are uploaded to and downloaded from websites which want to minimize their hosting costs. It would be almost impossible for time and technology to continue changing, developing new standards and formats, without older images becoming degraded through multiple conversions --- and therefore suffering in comparison to newer images.

The second two panels of the comic, however, draw attention to distinctly social human processes which often accelerate the decay of digital images. In the third panel, the stick figures have slightly shrunk, and beneath them we can see some icons of a computer menu. Implied in this panel is the idea that we are no longer looking at a copy of the image file itself, but a screenshot someone has taken of the image open on their computer. The final panel adds another layer of screenshotting through the mobile phone menu at the top, and two different watermarks implying that the image has been reposted at several image-sharing websites. The relevant lesson here is: \[???\]

The inevitability of digital decay has been acknowledged in informal cultural circles online, such as this website, and the wider phenomenon of “deep fried memes,” which present images (conventionally, established meme formats for other memes) with hyperbolic distortions. Pixellation, colour shifts, and layers of emoji (which, like the watermarks in the last panel of the *xkcd* comic, imply human intervention and not just technological deterioration) all contribute to a visual effect that makes the images “look like they've been compressed, re-uploaded, and compressed again,” implying an often incongruous long history for the content of the image, and creating an overall effect of being “comically over-processed” (Matsakis).



## 2.2.  contemporary snapshot ##  

Before delving into the more than fifty years of interlocking institutional histories which have created the current landscape of digital databases in eighteenth century studies, it is worth pausing to observe carefully what that current landscape actually looks like. One way to gain our bearings with an overwhelming quantity of information is to particularize it into anecdote: accordingly, our first look at these resources will be with the guiding figure of Charlotte Smith. Which works by Smith are in which database, why, and how? Another way to understand the pieces is to generalize them into systems: to take this approach, our second look at the same resources will examine graphs of their relative scale and overlapping contents. How does the system as a whole draw boundaries between its parts? Which parts are ‘peers’ or ‘colleagues,’ which are ‘rivals,’ and which do not even acknowledge their presence within the same system?  

In the next section I will close read the implicit models underlying each database, to examine how each enforces a particular concept of “literature” and “a text.”  

“The English Short-Title Catalog (ESTC) is a vast database designed to include a bibliographic record, with holdings, of every surviving copy of letterpress produced in Great Britain or any of its dependencies, in any language, worldwide, from 1473-1800” (CBSR)[ not sure how best to cite this - <http://estc.ucr.edu/index.html#>]. Today, “The English Short-Title Catalogue is the most comprehensive record of what has appeared in print in Britain and the English-speaking world for all branches of human experience from the last quarter of the fifteenth century to the start of the nineteenth. More specialized studies exist for fields and eras within that span, but no other resource matches ESTC’s dependability over such a broad range” (Vander Meulen 265).  

EEBO: “unlike scholarly facsimile editions, the selection process for microfilming was often arbitrary. Copies were selected primarily by reference to the copies listed in STC and Wing, with particular preference for certain major collections; they were not selected because they were considered representative of a particular edition. By bringing together the bibliographical record for an edition and (usually but not always) only a single witness of that edition,22 EEBO is obviously aiming to provide a useful scholarly mechanism in terms of searching but by doing so are implying -- albeit not deliberately -- that the record and the copy *are* *one and the same thing*. It would be better, perhaps, if EEBO represented itself as a library of copies, rather than a catalogue of ‘titles’.” (Gadd 687)



EEBO: “in digitising the microfilms in their original forms, EEBO decided (presumably for commercial reasons rather than purely scholarly ones) against sanitising the images. Openings are retained rather than broken into single pages; images are not cropped; rulers, place-holders, and descriptive notes are left in place; blank leaves are not removed. While Kichuk’s concern about ‘remediation’ is a real one, it is difficult to use EEBO for any length of time without being reminded that these are reproductions of actual objects.” (Gadd 688)



### 2.2.1.  Smith in databases ###  

For the purposes of this chapter, I examine Smith’s works which fall outside this dissertation’s decade of interest. As Table 1 shows, Smith’s publishing career began in 1784 and continued until her death in 1806; when I refer to Smith’s “full” output, I consider all 47 editions of her works published in her lifetime or in the year immediately following her death. Her 1790s output (that is, the editions published 1789-99) consists of 30 of those editions.  I have slightly expanded my chronological focus in part because some of the most interesting exclusions occur earlier and later in Smith’s publishing career, such as the first edition of her immensely influential *Elegiac Sonnets* (1784), which is listed in the ESTC but not available in facsimile anywhere, or the publications in the last years of her life, which are excluded from the chronological focus of most resources but can still appear in HathiTrust. Of particular interest is the fact that *Beachy Head*, which is now one of Smith’s most frequently anthologized and taught poems, does not appear in a single digital database. None of these inclusions or exclusions represent an agenda against (or for) Smith, or indeed an interpretive choice at all, but they nonetheless shape the disciplinary infrastructure.  

![][CSmith-in-ESTC-ECCO-TCP-Hathi-table]

Table 1: All editions of Charlotte Smith’s works published in England during her lifetime or in the year immediately following her death, and their inclusion in the ESTC, ECCO, ECCO-TCP, and HathiTrust databases.  

Figure 3 shows how Smith’s presence in four major databases has the effect of winnowing down her full output arbitrarily. Even the largest collection, the 42 editions included in the ESTC, is not comprehensive: since the ESTC does not include any works published after 1800, it excludes volumes 4 and 5 of *Letters of a Solitary Wanderer* (1802), three works for children (*Conversations, Introducing Poetry*, 1804; *History of England*, 1806; and *Natural History of Birds*, 1807), and the posthumous publication that now forms a major part of Smith’s reputation as a poet, *Beachy Head* (1807). ECCO lacks these five editions for the same reason, and is also missing five others: the first and ninth editions of *Elegiac Sonnets* (1784 and 1800), the second edition of *The Banished Man* (1795), the first edition of *Minor Morals* (1798), and the second edition of *Rambles Farther* (1800[ Why these five?]).

HathiTrust contains 18 of Smith’s 47 editions, though these are not a simple subset of the ESTC and ECCO. Unlike the ESTC and ECCO, HathiTrust contains volumes 4 and 5 of *Letters of a Solitary Wanderer* (1802)[^cf26]. This is the only post-1800 work which appears in HathiTrust, however--- the others are also missing, including the important volume *Beachy Head* (1807). There is one work included in HathiTrust but not in ECCO, the second edition of *The Banished Man* (1795). Whereas ECCO does not include works unless there is a complete copy available, HathiTrust provides scans of volumes 2, 3, and 4, and simply implies through their numbering that there is a missing first volume --- perhaps in the optimism that a volume 1 will appear from another library’s holdings, to complete the set later.[^cf27] The remaining HathiTrust included titles appear in both the ESTC and ECCO, and a further 21 titles appear as facsimiles in ECCO but not in HathiTrust. At first blush it is somewhat surprising that HathiTrust has failed to include works which are, demonstrably, in known locations at institutional libraries, and in physically sound condition to be scanned--- but the scans making up HathiTrust bear no relation to the scans in ECCO. *The Young Philosopher* (1798), for example, appears in ECCO sourced from a British Library copy, but the HathiTrust images are “Google-digitized” from the New York Public Library. Google’s rapacious book-scanning, evidently, was not as thorough as ECCO’s sustained scholarly project.

The smallest subset of all of these texts is the ECCO-TCP holding of just two titles: the second edition of *Celestina* (1791), and the first edition of *The Emigrants* (1793). Both titles appear in all larger databases, including HathiTrust (though, as I will discuss, they arrive in HathiTrust from a different source). *The Emigrants* is included in ECCO-TCP as one file, based on the ECCO facsimile of an original from the Huntington Library. *Celestina* is included as four files, one for each of four volumes, based on the ECCO facsimile of an original from the British Library. Both works were first reproduced in the microfilm version produced 1982-2002 in by Research Publications,[^cf28] then digitized in 2003 (released on ECCO in June 2004), and finally published as TEI XML files in January 2007. The current files have been kept up to date with changes in TEI standards, and were created by converting TCP files to TEI P5 using tcp2tei.xsl. The bibliographic metadata for these works is the same between ESTC, ECCO, and ECCO-TCP records. In HathiTrust, however, the source text for *The Emigrants* is a University of California Library copy, rather than the British Library, scanned by Google Books, and presented with substantially less detailed bibliographic information. The ESTC, ECCO, and ECCO-TCP records for The Emigrants all provide the same physical description “ix,\[3\],68\[i.e. 60\]p. ; 4⁰” with the same note“\[n\]umbers 9-16 omitted in pagination; text is continuous.” HathiTrust, in contrast, gives the physical description “ix, 68 p. ; 26 cm,” which is both more and less information: a quarto volume could be a range of sizes, so HathiTrust provides new detail by giving a measurement in centimetres, but the data on page numbers is now misleading. Consulting the HathiTrust facsimile shows that it, too, omits the page numbers 9-16, going directly from page 8 to page 17 without a break in the poem. HathiTrust also omits information on the three unnumbered pages between the preface and the poem. Evidently, a human did consult the book, to identify a nine-page preface in roman numerals, and the page number on the last page, but they did not carry out a full collation.  

![][CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3]

Figure 3: An alluvial chart, showing the winnowing down of Smith’s works from database to database. Of the 47 editions printed in England between 1784 and 1807, 42 are included in the ESTC, and 5 do not appear in the ESTC because they were printed after 1800 and thus fall outside its purview. ECCO contains 37 of Smith’s 47 editions, all of which also appear in the ESTC. ECCO is missing the 5 editions not listed in the ESTC (since it, too, does not contain works past 1800), as well as another 5 works. HathiTrust contains 18 of Smith’s 47 editions, but unlike ECCO, these are not a simple subset of the ESTC. HathiTrust contains one of the 5 editions excluded from the ESTC, and one of the 5 editions included in ESTC but excluded from ECCO. The remaining 16 HathiTrust editions appear in both the ESTC and ECCO. ECCO-TCP includes only 2 of Smith’s 47 editions, both of which appear in every previous database. Graph generated using RAW Graphs (Mauri et al.).  

Only one of Charlotte Smith’s works is available in Project Gutenberg: *Emmeline, the Orphan of the Castle* (first published 1788).  

Searching the ESTC for records which both have “Toronto” in the library name and “Charlotte Turner” in the author name turns up two records: volume one of *Rural Walks* (1795) and *Minor Morals* (1798), both held at the Toronto public library. The Toronto Public Library catalogue has two distinct author identities for “Smith, Charlotte Turner, 1749-1806, author.” and for “Smith, Charlotte, 1749-1806,” and the special collections holdings only appear under the latter name (making them initially difficult to find). Under the “Smith, Charlotte” name, however, six titles printed during Smith’s appear: the two listed in ESTC, plus a complete two-volume copy of *Rural Walks* (1795), the first and second editions of *Rambles Farther* (1796 and 1800), and *Conversations Introducing Poetry* (1804). Of these, *Rural Walks* and both editions of *Rambles Farther* are listed in the ESTC but without records of the Toronto copies. All six titles are part of the Osborne Collection of Early Children's Books. \[This is interesting because it shows how scholarly disciplinary interpretations perpetuate themselves *infrastructurally*: as a Toronto-based scholar, the path is easier for me to study Smith-the-children’s-writer than other Smiths.\]



## 2.3.  history of databases ##  

Although each individual database has attracted some discussion, they have not been discussed together as an interlocking system. Instead, each database (or cluster of related databases) attracts discussion within its own conventional sphere. Here, I will discuss them together, organized chronologically by decade. A chronological organization makes it clear how many different organizations are in fact responding to shared historical conditions, or even to each others’ development, as they make strategic decisions over time. A chronological organization also contrasts with essentially teleological descriptions of individual resources, which tend to work backward from a current state to present a clean narrative of how that current state was discovered to be ideal. By refusing to gloss over dead ends, periods of stagnation, and other oddities, we can better understand the current state of contemporary databases as the outcome of historically contingent processes which might have turned out differently, rather than accepting them uncuriously as inevitabilities.  

1918	Pollard first proposes a “short-title handlist”

1926	Pollard and Redgrave Short-Title Catalogue for 1476--1640

1938	Eugene B. Power founds University Microfilms

1945	Wing starts collecting his STC, 1641--1700

1951	Donald Wing’s catalogue for 1641--1700, first edition

1971	First text in what would be Project Gutenberg. Over the next twenty years, Michael Hart personally keyed the first hundred books.

1972	Beginning of second ed of Wing STC, 1641--1700

1976	Proposal for Eighteenth Century Short Title Catalogue, British Library and the American Society for Eighteenth Century Studies

1976	Second edition, vol 1, of Wing’s STC

1976	Beginning of second ed of Pollard & Redgrave STC, 1475-1640

1977	ESTC pilot begun at British Library, directed by Robin Alston

1979	ESTC: Libraries from USA, Germany, and Australia began contributing to ESTC

1980	ESTC database available via British Library BLAISE \[British Library Automated Information SErvice\]

1981	Research Publications, Inc begins microfilming books

1981	ESTC database available via US Research Libraries Group RLIN \[Research Libraries Information Network\] system

1983	ESTC catalogue of BL holdings and indexes published in microform

1983	*Eighteenth Century Collection* microfilm produced by Research Publications, Inc

1985	ESTC online databases in RLIN and BLAISE upgraded to allow dynamic updates to a single shared file

1986	Second edition, vol 2, of Wing’s STC

1987	ESTC expanded scope to add all print prior to 1700, changing its name to the English Short Title Catalogue. Information from Wing and STC is added to ESTC.

1987	Michael Hart recruits first Project Gutenberg volunteers

1989	Project Gutenberg completes its tenth book, the King James Bible

1991	End of second edition of Pollard & Redgrave STC, 1475-1640

1991?	Exhaustive index to Wing’s STC --- after which Bibliographical Society no longer supported Wing

1992	ESTC expanded scope to add serials

1994	ESTC made pre-1700 records available

1994	Project Gutenberg completes its 100^th^ book, the Complete Works of William Shakespeare

1994	Project Gutenberg’s first website is developed by volunteer Pietro Di Miceli

“By the late 1990s, several thousand reels had been published in two series: ‘Early English Books, 1475--1640’ and ‘Early English Books, 1641--1700’.” (Gadd)

1997	Project Gutenberg publishes its 1000^th^ book, La Divina Commedia di Dante, in Italian

1998	ESTC second edition released on CD-ROM

1998	Conclusion of second ed of Wing STC

1998	Beginnings of EEBO: University Microfilms (now ProQuest) began to make available digitised copies of its microfilms across the Internet to subscribing institutions

199	9	ESTC assumed official responsibility for receiving new Wing STC data

1999	TCP began

2000	Project Gutenberg: Charles Franks launches Distributed Proofreaders

2003	ESTC third edition released on CD-ROM

2003	Project Gutenberg 600 “best” ebooks released on CD-ROM, followed by 10,000 item DVD

2003	Beginning of ECCO: Thomson Gale (now Gale Cengage Learning) made digital copies of Eighteenth Century Collection microfilms available to subscribers online

2004	Google Print is announced

2005	TCP begins encoding ECCO texts

2006	ESTC made available to search free online; ESTC begins transcribing full title and imprints

“As of 26 April 2007 the number of microfilm reels of The Eighteenth Century that had been released was 16,625; the total number of titles on these reels is 189,569 (information provided by Katri Russick, Thomson Gale, Australia and New Zealand, in a private email). This number increased to at least 17,828 microfilm reels - the number received and catalogued by Monash University - by 1 November 2010.” (Spedding 450)

2007	Project Gutenberg DVD released with 17,000 items

2008	Project Gutenberg publishes its 25,000^th^ book, English Book Collectors, by William Younger Fletcher

2008	HathiTrust founded, by 12-university Committee on Institutional Cooperation and 11-library University of California Libraries

2009	EEBO-TCP Phase I complete: produced 25,000 books; beginning of Phase II

2010	Project Gutenberg DVD released with 30,000 items

2011	40,000 books in Project Gutenberg

2015	EEBO-TCP Phase I books released to the general public

2017	Project Gutenberg discontinues free mailing of CDs and DVDs, though the files remain available for people to burn their own copies at home

2021	EEBO-TCP Phase II books released to the general public



Table 2: A chronological history of major events in the development of six databases: the English Short-Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the Text Creation Partnership (TCP), Project Gutenberg, Google Books, and HathiTrust. Also included are events in the development of related resources, such as Early English Books Online (EEBO).





### 2.3.1.  prehistories ###



Some important threads of this history of mass digital archives begins long before the invention of the computer, with the bibliographic collections which served as the extant solutions to the challenge of large-scale text-tracking. *The Pollard and Redgrave Short-Title Catalogue for 1476--1640* first appeared in 1926. Donald Wing’s catalogue for 1641--1700 appeared in 1951. After the completion of Wing’s STC, “\[e\]xploratory studies, poorly funded and inadequate though they were” (Korshin 209) throughout the 1950s and 60s pursued the feasibility of systematically accounting for the much larger body of printed work produced in the eighteenth century. \[Also add the Library of Congress card catalogue.\]



### 2.3.2.  1970s ###  

Concurrently with the development of the ESTC, Wing’s seventeenth-century STC was undergoing redevelopment into a second edition, overseen by Katharine Pantzer. The second edition of Wing’s STC published its first volume in 1976. This second edition “represented a vast development of the original” (Vander Meulen 268), incorporating thousands of new entries, expanding the titles, and adding explanatory notes and headnotes.  

The English Short Title Catalogue began as the Eighteenth Century Short Title Catalogue in the 1970s, operating in a similar line as the original Pollard and Redgrave Short-Title Catalogue for 1476--1640 and Donald Wing’s catalogue for 1641--1700. These catalogues established the ambitious simplicity of the ESTC: to accurately describe every edition of every printed work in English or from the United Kingdom. The Eighteenth Century Short Title Catalogue began properly in 1976, at a conference jointly sponsored by the British Library and the American Society for Eighteenth Century Studies (Crump 106). Here, “bibliographers and librarians attempted both to arrive at a consensus of the size of the task and the methodology that would have to be adopted to achieve a union catalogue. However, until the works were catalogued, it would not be possible to answer basic questions (such as the potential number of extant items) which would predetermine working methods. The very fact that they found it difficult to agree for want of sound and accepted figures indicated the need for ESTC.” (Crump 105). A pilot project began at the British Library in 1977, under the direction of Robin Alston (Crump 105). Unlike earlier Short-Title Catalogues, which appeared as lengthly print publications, the Eighteenth-Century Short Title Catalogue was conceived as digital from the beginning --- a decision which, as Karian notes, “exhibited considerable foresight” (283) in the 1970s. As a result, “ESTC records existed in digital form long before many humanists saw computer technology as central to their work” (Karian 283).  Robin Alston and Mervyn Jannetta developed their own cataloguing rules, distinct from the Library of Congress MARC and UK MARC standards (Korshin 211). Once these standards were established, the British Library began to re-catalogue its own holdings, and in 1979 libraries in the United States, Germany, and Australia undertook to supplement them. In these international collaborations, “Where ESTC records already existed, these were adopted as the \[new\] record and only those works not held in the ESTC base file were catalogued again” (Crump 105).  

Project Gutenberg began in 1971 with one individual, Michael Hart, who did not begin with a specific project vision in mind. From the beginning, then, Project Gutenberg was not goal-oriented in the same way as the other resources under discussion. By this I mean that Project Gutenberg orients itself toward goals of a fundamentally different kind than the goals which structure other textual archives, not that it has no goal. Project Gutenberg is, in general, subject to being dismissed as unserious or lacking rigorous standards, but I argue that these dismissals come from a failure to recognize and respect the real goals, seriousness, and standards which drive the project. In the case of the project’s founding, that goal was not, as in the case of the other databases under discussion, to provide a particular kind of access to a particular kind of texts. Instead, the goal of Project Gutenberg was born from a moment of happenstance and nepotism by which Hart, a student at the time, was donated $100,000,000 of computer time on the Xerox Sigma V mainframe at the Materials Research Lab at the University of Illinois. This mainframe was one of the first fifteen nodes on the early ARPANet, the precursor to the modern internet. As Hart described it, he “decided there was nothing he could do, in the way of ‘normal computing,’ that would repay the huge value of the computer time he had been given ... so he had to create $100,000,000 worth of value in some other manner” (“History and Philosophy”). Rather presciently for 1971, Hart concluded that the greatest value computing would offer was the storage, searching, and retrieval of other materials. He therefore typed up and distributed the Declaration of Independence.[^cf29] This became the first text of what would eventually become Project Gutenberg. It might even be considered the first ebook (according to Lebert 2008). Project Gutenberg was certainly “the first information provider on the internet and is the oldest digital library” (Lebert).

“During the first twenty years, Michael Hart himself keyed in the first hundred books, with the occasional help of others from time to time.” (Lebert)

“when we started, the files had to be very small as a normal 300 page book took one meg of space which no one in 1971 could be expected to have (in general). So doing the U.S. Declaration of Independence (only 5K) seemed the best place to start. This was followed by the Bill of Rights --- then the whole US Constitution, as space was getting large (at least by the standards of 1973). Then came the Bible, as individual books of the Bible were not that large, then Shakespeare (a play at a time), and then into general work in the areas of light and heavy literature and references.” (Hart “History and Philosophy”) “That edition of Shakespeare was never released, due to copyright changes. If Shakespeare's works belong to the public domain, the comments and notes may be copyrighted, depending on the publication date. But other editions belonging to the public domain were posted a few years later.” (Lebert)



### 2.3.3.  1980s ###  

In 1980, the ESTC began to go online.  “One implication of the publication history of short-title catalogues is that they have been deemed functional and valuable even before they were complete. (That estimation is crucial, for their full completion is for all practical purposes impossible.) Judging that even a preliminary form of the records was useful to scholars, the planners of ESTC determined to conduct its development ‘in full public view’ and to make the incomplete file available ‘warts and all’[ I can probably say a LOT more about all of this --- this is my answer to the “moving target” problem; move it to the introdction] (in the words of Henry Snyder and Michael Crump, responding to criticism by Peter Blayney)” (Vander Meulen 270). Accordingly, the in-progress database “was soon available online, from 1980 via the British Library BLAISE \[British LibraryAutomated Information SErvice\] system and from 1981 in the US Research Libraries Group RLIN \[Research Libraries Information Network\] system” (Norman). Each of these databases was worked on locally by researchers, and then updated and reconciled with each other weekly.

To supplement these databases, accessible almost exclusively to librarians with specialized training in operating them and primarily used by the scholars compiling the file, the ESTC intended to publish editions at particular milestones of completeness, intended for the use of non-librarian scholars. Their “first step, a fiche catalogue of \[the British Library’s\] holdings, together with indexes, generated by the computer” (Crump 105) was published in a microform “snapshot” in 1983, but other milestones did not occur according to schedule. The “joint Anglo-American interim publication of the ESTC file ” (Korshin 212) which was expected to follow on microform in 1984 (Korshin 212) did not appear. Alston attributed the delays partly to the immensity of the task, and partly to the impact of short-term cost-cutting decisions, like the reduction of early-stage proofreading or of in-person examination of books, which dramatically increased the labour of verifying the resulting database record. Although he consistently warned “how easily strategic decisions based exclusively on cost usually lead to greater, not less, eventual costs” (Alston), the ESTC each year seemed to be facing a new budget struggle, and important maintenance labour was several times deferred. This created something like a paradox for the ESTC: funding bodies wanted to commit less money to a project which was behind schedule, but the project would remain behind schedule unless it was funded to complete the work required. 

Nonetheless, work continued, and in 1985, the online databases in RLIN and BLAISE were upgraded to allow dynamic updates to a single shared file (Crump 106), which for the first time allowed continuous access to a shared record, rather than the constant exchange and messy merging of individual partially-overlapping records. “Until the file was dynamically available online on RLIN in 1985 batch processing was a weekly nightmare” (Alston). At this time, it was hoped that the new RLIN file would “result in a more complete and coherent ‘first edition’ of ESTC” to be published in 1989 (Crump 106), though this deadline, too, was not met. In the mean time “the ESTC file \[was\] available to scholars on both BLAISE-LINE and on RLIN.” (Crump 106). To facilitate its use, the ESTC distributed “\[a\] simplified manual for searching the file on-line” (Crump 106). Crump took the opportunity of the update to rhapsodize on the database’s potential usefulness for other scholars: “No longer is the scholar limited in access to the data by the fixity of the printed page” (106). This valuable resource was not without cost. Although the manual on how to formulate search queries was free, use of the ESTC itself was notably not. Institutions or individuals paid to subscribe to the ESTC itself, paid per query for searches to be run, paid per minute for being connected to the database, and often paid for access to the computers they must use in their own libraries. Tabor says “the ongoing expense of consulting ESTC was the cyber-equivalent of the hefty up-front payment needed to acquire its printed predecessors, STC and Wing” (367).

“In 1987, with the agreement of the Bibliographical Society and the Modern Language Association of America, the International Committee approved the extension of the database to cover the period from the beginning of printing in the British Isles (ca. 1472) to 1700. The file changed its name to the 'English Short Title Catalogue', thereby keeping its well-known acronym.”(Norman)  

The book facsimiles which would become Eighteenth Century Collections Online (ECCO) began as in 1983, when the company Research Publications, Inc began to produce its *Eighteenth Century Collection* microfilm. Research Publications was a newly-founded for-profit company, which was founded in 1981. They and their rival, University Microfilms, produced many of the facsimile images in contemporary databases. Today, the former Research Publications, Inc is part of Gale Cengage, and University Microfilms is part of ProQuest; as ECCO’s history continues, the private company that owns the microfilms will change many times.

“these documents were imaged in the late 1970s, transformed into microfilm during the 1980s” (Christy et al. 1) --- who imaged them? What were the images before they were microfilm??  

The second volume of the second edition of the STC was published in 1986.  

“In August 1989, Project Gutenberg completed its 10th book, The King James Bible, that was first published in 1611, with the standard text dated 1769” (Lebert)

“Then, through being involved in the University of Illinois PC User Group and with assistance from Mark Zinzow, a programmer at the school, Hart was able to recruit volunteers and set up an infrastructure of mirror sites and mailing lists for the project. With this the project was able to grow much more rapidly.” (History-Computer)

1989 is also when Project Gutenberg began to make use of OCR to generate base texts which were then proofread, rather than having the text typed from scratch. (Paywalled WSJ article)



### 2.3.4.  1990s ###  

In the 1990s, the ESTC began to expand its scope. “The USA team began cataloguing pre-1701 material in 1989, joined in the mid-1990s by the British Library team, and the resulting records were made available in the RLIN file from 1994.” (Norman). “In 1992, IESTC approved a further extension of the file to include serial publications. The USA team began work in 1994 on the cataloguing of serials within the scope of ESTC” (Norman). The ESTC continued to research new entries and improve existing ones, releasing a second edition of the file on CD-ROM in 1998.  

The second edition of the STC completed its publication in 1991, with a set of exhaustive indexes to its material. Its completion in 1991 also marked the end of the ability of its publisher and sponsor, the Bibliographical Society, to support it (Vander Meulen 269). “Accordingly, in 1999 the Society made an agreement with ESTC whereby the latter... would assume official responsibility for receiving new STC data” (Vander Meulen 270).  

“EEBO’s relationship with the original STC and Wing is straightforward and clear; EEBO’s relationship with electronic ESTC, on the other hand, is less well-known.20 A series of agreements made between ESTC and University Microfilms/ProQuest between 1989 and 1997 allowed EEBO to draw directly on ESTC’s existing bibliographical data. Consequently, / every search run on EEBO (with some exceptions) relies, in a fundamental sense, on bibliographical information originally supplied by ESTC -- but not in the form that one might expect. First, EEBO heavily edited ESTC’s data for its own purposes: certain categories of data were removed (e.g. collations, Stationers’ Register entrances), some information was amended (e.g. subject headings), and some was added (e.g. microfilm- specific details). Second, there is no formal mechanism for synchronising the data between the two resources. Occasionally, snapshots of data are sent by EEBO to ESTC but there is no guarantee that a correction or revision made to an ESTC entry will be replicated in the corresponding EEBO entry or vice versa: neither ESTC nor EEBO will necessarily know when the other has made a correction. As both resources continue to amend and expand their bibliographical data for their own purposes, there is an increasing likelihood of significant discrepancy between the two resources. Finally, although EEBO continues to microfilm and digitise, there is no absolute one-to-one correspondence between the pre-1701 entries in ESTC and the materials on EEBO; there are -- and will always be -- items on ESTC not available on EEBO.” (Gadd 685-6)  

By 1997, Research Publications, Inc had become Primary Source Media. (LOC http://id.loc.gov/authorities/names/n98069963.html) In September 1998, “the Thomson Corporation \[merged\] three of its electronic and reference publishing subsidiaries---Gale Research, Information Access Company (IAC), and Primary Source Media---into a new company called The Gale Group.” (http://newsbreaks.infotoday.com/NewsBreaks/Thomson-Merges-Gale-IAC-Primary-Source-Media-into-The-Gale-Group-17999.asp)

Figure out when it went RPI -\> Primary Source Media -\> Thomson Gale -\> Gale Cengage  

“The Text Creation Partnership started, in 1999, as a collaboration between the university libraries of Michigan and Oxford, the Council on Library and Information Resources, and the publisher of Early English Books Online, Proquest. The aim was to create high quality ‘standardized, digitally-encoded electronic text editions’ starting with 25,000 titles from Early English Books Online.” (Gregg n. pag.)  

“When the internet became popular, in the mid-1990s, the project got a boost and an international dimension. Michael still typed and scanned in books, but now coordinated the work of dozens and then hundreds of volunteers in many countries.” (Lebert)

“In 1990, there were 250,000 internet users, and the standard was 360 K disks. In January 1991, Michael typed in Alice's Adventures in Wonderland, by Lewis Carroll (published in 1865). In July 1991, he typed in Peter Pan, by James M. Barrie (published in 1904).” (Lebert)

“By the time Project Gutenberg got famous, the standard was 360K disks, so we did books such as Alice in Wonderland or Peter Pan because they could fit on one disk. Now 1.44 is the standard disk and ZIP is the standard compression; the practical filesize is about three million characters, more than long enough for the average book.” (Hart “History and Philosophy”)

“Project Gutenberg gradually got into its stride, with the digitization of one book per month in 1991, two books per month in 1992, four books per month in 1993 and eight books per month in 1994. In January 1994, Project Gutenberg celebrated its 100th book by releasing The Complete Works of William Shakespeare.” (Lebert)

1994, Italian volunteer Pietro Di Miceli developed and administered the first Project Gutenberg website and started the development of the Project online Catalog. (“Credits”)



### 2.3.5.  2000s ###  

A third edition of the ESTC was published on CD-ROM in 2003 (Norman). In 2006, almost thirty years after the commencement of the project, the ESTC underwent another major shift: the database was made publicly available to be searched for free online. This inspired more rhapsodizing, this time from Tabor: “The freeing of ESTC ... now places in one location, for the consultation of anyone with internet access, the fullest and most up-to-date bibliographical account of ‘English’ printing” (367). At the same time, the ESTC began a project “to provide full title and imprint transcriptions for the eighteenth-century records” (Tabor 370).  

In 2003, Thomson Gale began making digital copies of the Eighteenth Century Collection microfilms available to subscribers online. The digital images were made from the microfilm masters, which were 400 dpi, and thus higher resolution than the microfilm copies, which Spedding reports were 300 dpi (440).

\[TODO-RESEARCH: what is up with parts I and II of ECCO? When was each available?\]  

In 2000, Distributed Proofreaders was founded. Also “In 2000, a non-profit corporation, the Project Gutenberg Literary Archive Foundation, Inc. was chartered in Mississippi, United States to handle the project's legal needs. Donations to it are tax-deductible. Long-time Project Gutenberg volunteer Gregory Newby became the foundation's first CEO.” (Wikipedia)

“A fast growth thanks to Distributed Proofreaders, a website launched in October 2000 by Charles Franks to share the proofreading of books between many volunteers. Volunteers choose one of the books listed on the site and proofread a given page. They don't have any quota to fulfill, but it is recommended they do a page per day if possible. It doesn't seem much, but with hundreds of volunteers it really adds up.” (Lebert)

“In 2002 Distributed Proofreaders became part of Project Gutenberg.” (Hosch) “By 2009 roughly half of all Project Gutenberg books had been handled by using Distributed Proofreaders.” (Hosch)

“As of 2018, the 36,000+ DP-contributed books comprised almost two-thirds of the nearly 60,000 books in Project Gutenberg.” (Wikipedia)



At some point Carnegie Mellon University agreed to administer the project’s finances.

It’s currently hosted by ibiblio at UNC Chapel Hill.



”The number of electronic books rose from 1,000 (in August 1997) to 5,000 (in April 2002), 10,000 (in October 2003), 15,000 (in January 2005), 20,000 (in December 2006) and 25,000 (in April 2008). ... The steady growth went on, with an average of 8 books per month in 1994, 16 books per month in 1995, and 32 books per month in 1996.” (Lebert) “If 32 years were necessary to digitize the first 10,000 books, between July 1971 and October 2003, 3 years and 2 months were necessary to digitize the following 10,000 books, between October 2003 and December 2006.” (Lebert) “In the first 11 weeks of 2004, Project Gutenberg added 313 new e-books. It took from 1971 to 1997 to produce the first 313 e-books---that's 11 weeks compared to about 26 years.” (Hane)

“In 2004 Project Gutenberg Europe and Distributed Proofreaders Europe were formed to facilitate the process of adding more non-English works.” (Hosch)

The books doubled again between 2006 and 2011, from 20,000 (2006) to 40,000 (2011). (Hosch)

The current collection is 60,000 ebooks. This falls short of Hart’s bombastic declaration in 2004 that “We want to grow the collection to 1 million free e-books and distribute them to 1 billion people for a total of 1 quadrillion e-books to be given away by the end of the year 2015.” (Hane)



Wikipedia: ”In August 2003, Project Gutenberg created a CD containing approximately 600 of the "best" e-books from the collection. The CD is available for download as an ISO image. When users are unable to download the CD, they can request to have a copy sent to them, free of charge.

“PG has been giving away CDs and DVDs; a volunteer mails them out for free on request.” (Hane)

In December 2003, a DVD was created containing nearly 10,000 items. At the time, this represented almost the entire collection. In early 2004, the DVD also became available by mail.

In July 2007, a new edition of the DVD was released containing over 17,000 books, and in April 2010, a dual-layer DVD was released, containing nearly 30,000 items.

The majority of the DVDs, and all of the CDs mailed by the project, were recorded on recordable media by volunteers. However, the new dual layer DVDs were manufactured, as it proved more economical than having volunteers burn them. As of October 2010, the project has mailed approximately 40,000 discs.  





As early as 2003, the TCP executive board meeting minutes reported that “Michigan has made agreements with Gale and Readex to support conversion of subsets of the Eighteenth Century and Evans Early American materials which will allow us to create a cross-searchable corpus of important historical texts ... The University of Michigan has reached agreements to create a subset of accurately keyed and encoded texts in conjunction with these projects, and aims to produce 6,000 early American and 10,000 18th century texts. In the near term, this will not affect production of EEBO texts because there is adequate capacity to expand beyond existing levels of production. In the long term, this will produce a large number of culturally significant texts, produced to a single standard, that are owned by the library community and complement the EEBO texts for these early historical periods.” (“Meeting Minutes 2003-10-22.”).

In 2004, “Jeff Moyer then updated the Board on its progress with the ECCO product which contains over 26 million pages and 155,000 volumes. To date, they have 60 customers including 6 Canadian and 11 international institutions. They have also done OCR for the ECCO product and are interested in how the TCP text will work and integrate with their OCR.” (“Meeting Minutes 2004-10-21.”).

“In 2005 the project expanded to include Gale-Cengage’s Eighteenth-Century Collections Online (as well as Evans Early American Imprints by Newsbank). However, while the EEBO-TCP project flourishes (with around 40,000 texts transcribed so far), the work on ECCO-TCP stagnated at around 2,000 texts. As well as the main partner institutions of Michigan and Oxford that oess to the eighteenth-century TCP texts, so I’ve listed them below, with a few comments.” (Gregg n. pag.)

“In 2005, the TCP executive board and staff sought to expand the TCP model to other databases of historical books, namely, Gale Cengage’s Eighteenth-Century Collections Online (ECCO) and Newsbank Readex’s Evans Early American Imprints (Evans-TCP). These projects never received quite the support attracted by EEBO-TCP, and in the end produced only about 8,000 texts, compared to the 60,000 produced by the latter, with another few thousand on the way.” (TCP “About”)

By October 2005, “the ECCO-TCP project has commenced with the release of a demo, and TCP sponsored an ECCO selection task force in August. ... Rich Foley reported that ECCO now covers 120 subscribers and a recent purchase from the JISC. The ECCO product has also sold well in Canada this past year. Rich also reported (relating to a question on Metadata) the release of a My Library product which is opening up access to their metadata at Gale and that he was interested in further doing case studies on how ECCO is used in research and in the classroom.” (“Meeting Minutes 2005-10-20.”)

In 2006, ECCO-TCP was struggling compared to the other TCP products. “Rich Foley reported that ECCO is one of the biggest products at Gale with eighty to ninety ARLs subscribing as well as small institutions. He also said that a focus at Gale was to work on more tools to facilitate undergraduate teaching of their products. ... Mark Sandler reported that the TCP budget shows mostly positive balances through 2007. The exception to that is ECCO but because it is still so early in the project, it seems likely that TCP will overcome those problems within the next few months. Nonetheless, the TCP project in EEBO, Evans, and ECCO face potential budget deficits in fiscal year 2008.” At this time, the TCP began to think about the end of the project: “the TCP should set a date to close the partnership (likely around 2010 given current commitments,” partly to address financial solvency. (“Meeting Minutes 2006-09-16”).

In 2007, all three TCP project reported successful sales, though ECCO’s news was the most vague: “Brandon Nordin also reported good news from Gale and along with Mary Sauer-Games announced that the EEBO and ECCO databases will now be cross-searchable so that users can go to either collection and find records from the other” ( “Meeting Minutes 2007-10-30”). Nonetheless, “Evans-TCP and ECCO-TCP sales have historically (for a variety of reasons, chiefly the presence of OCR text in both projects) been weaker than anticipated” ( “Meeting Minutes 2007-10-30”). And the end loomed nigh: “Currently finances are good through fiscal year 2008. EEBO-TCP is on target to complete 25,000 texts by the end of fiscal year 2008. Evans-TCP is likely, given current finances to complete around 6,000 texts. ECCO-TCP will complete around 1,300 texts. Therefore, the TCP, particularly in EEBO-TCP has been a success meeting most of its goals. Nonetheless, Evans-TCP and ECCO-TCP are still short of their goals of 6,000 and 10,000 texts respectively, and in fiscal year 2009, the TCP overall is facing a deficit of around $400,000 if it does not either reduce its current staff or bring in a large influx of money within the next six months” ( “Meeting Minutes 2007-10-30”). These are the last meeting minutes available online.

“EEBO-TCP met its goal of producing 25,000 books in 2009 (thereafter known as “EEBO-TCP Phase 1”), and then undertook work on a second phase to convert the first edition of each remaining unique monographic work in EEBO---another 40,000 or so books, for a total of around 70,000, if all hopes were realized.” (TCP “About”)

“Begun in 2009, Phase II both shrank and expanded the scope of EEBO TCP.  Selection became more discriminating and focused more on English-language (and Welsh- and Gaelic-language) texts to the exclusion of French and Latin titles, and also set aside the serials (periodicals) as a fit project for another time. But within the constraints of English-language monographic titles, it aspired to something approaching comprehensive treatment: EEBO Phase II planned to convert each and every unique work in Early English Books Online (usually the first edition), or an estimated total of around 45,000 books on top of the 25,000 completed in Phase I. This was an ambitious, and always risky, goal. As it happened, enough institutions joined Phase II to fund the completion of about 40,000 titles, of which about 35,000 have been released to date, the remainder slowly working their way through the production pipeline. (TCP “EEBO”)



“As of 2019, the total number of books available in Phase II came to 34,963, with a further release of several thousand additional titles tentatively scheduled for later in the year.  Short of an infusion of new funding, or the adoption of a new production model, this should bring the active work of the TCP to at least an interim conclusion.” (TCP “EEBO”)

“Because of these greater challenges facing ECCO-TCP, it is perhaps better described as a proof of concept than as a completed project. With the support of more than 35 libraries, the TCP keyed, encoded, edited, and released 2,473 ECCO-TCP texts. A further tranche of 628 texts was keyed and encoded but never fully proofed or edited. The texts in this group remain useful for many purposes, however, and bring the total of ECCO-TCP texts to over 3,000. In cooperation with Gale Cengage, these texts have been made freely available to the public.” (TCP, “Eighteenth Century Collections Online (ECCO) TCP”)



#### 2.3.5.1.  GB ####



“...one can certainly argue that the project is as old as Google itself. In 1996, Google co-founders Sergey Brin and Larry Page were graduate computer science students working on a research project supported by the Stanford Digital Library Technologies Project. Their goal was to make digital libraries work, and their big idea was as follows: in a future world in which vast collections of books are digitized, people would use a “web crawler” to index the books’ content and analyze the connections between them, determining any given book’s relevance and usefulness by tracking the number and quality of citations from other books. The crawler they wound up building was called BackRub, and it was this modern twist on traditional citation analysis that inspired Google’s PageRank algorithms -- the core search technology that makes Google, well, Google.” (“Google Books History”)



2002: “A small group of Googlers officially launches the secret “books” project. They begin talking to experts about the challenges ahead, starting with a simple but crucial question: how long would it take to digitally scan every book in the world? It turns out, oddly enough, that no one knows. In typical Google fashion, Larry Page decides to experiment on his own. In the office one day, he and Marissa Mayer, one of our first product managers, use a metronome to keep rhythm as they methodically turn the pages of a 300-page volume. It takes a full 40 minutes to reach the end. Inspired by the extraordinary digitization projects underway all around the world -- the Library of Congress’s American Memory project, Project Gutenberg, the Million Book Project and the Universal Library, to name only a few -- the team embarks on a series of site visits to learn about how they work. As part of this fact-finding mission, Larry Page reaches out to the University of Michigan, his alma mater and a pioneer in library digitization efforts including JSTOR and Making of America. When he learns that the current estimate for scanning the university library’s seven million volumes is 1,000 years, he tells university president Mary Sue Coleman he believes Google can help make it happen in six.” (“Google Books History,” 2016)



2003: The team works to develop a high-speed scanning process --- “A team member travels to a charity book fair in Phoenix, Arizona, to acquire books for testing non-destructive scanning techniques. After countless rounds of experimentation, the team develops a scanning method that’s much gentler than current common high-speed processes. ... At the same time, the team’s software engineers make progress toward resolving the tricky technical issues they encounter processing information from books that contain odd type sizes, unusual fonts or other unexpected peculiarities -- in 430 different languages.” (“Google Books History,” 2016)



2004: formal partnership with Bodleian library “to digitize the library’s incomparable collection of more than one million 19th-century public domain books within three years.” (“Google Books History,” 2016)

“In October, Larry and Sergey announce “Google Print” at the Frankfurt Book Fair in Germany. The first publishers to join the program: Blackwell, Cambridge University Press, the University of Chicago Press, Houghton Mifflin, Hyperion, McGraw-Hill, Oxford University Press, Pearson, Penguin, Perseus, Princeton University Press, Springer, Taylor & Francis, Thomson Delmar and Warner Books. In December, we announce the beginning of the “Google Print” Library Project, made possible by partnerships with Harvard, the University of Michigan, the New York Public Library, Oxford and Stanford. The combined collections at these extraordinary libraries are estimated to exceed 15 million volumes.” (“Google Books History,” 2016)



“Every weekday, semi trucks full of books would pull up at designated Google scanning centers. ... The books were unloaded from the trucks onto the kind of carts you find in libraries and wheeled up to human operators sitting at one of a few dozen brightly lit scanning stations, arranged in rows about six to eight feet apart. ... Each one could digitize books at a rate of 1,000 pages per hour. The book would lie in a specially designed motorized cradle that would adjust to the spine, locking it in place. Above, there was an array of lights and at least $1,000 worth of optics, including four cameras, two pointed at each half of the book, and a range-finding LIDAR that overlaid a three-dimensional laser grid on the book’s surface to capture the curvature of the paper. ... What made the system so efficient is that it left so much of the work to software. Rather than make sure that each page was aligned perfectly, and flattened, before taking a photo, which was a major source of delays in traditional book-scanning systems, cruder images of curved pages were fed to de-warping algorithms, which used the LIDAR data along with some clever mathematics to artificially bend the text back into straight lines. At its peak, the project involved about 50 full-time software engineers.” (Somers)



2005: “In keeping with our mission to organize the world’s information and make it universally accessible and useful, we donate $3 million to the Library of Congress to help build the World Digital Library, which will provide online access to a collection of rare and unique items from all around the world. We also extend our pilot scanning program with the Library, which includes digitizing works of historical value from the Library of Congress Law Library. Google renames “Google Print” Google Books, which more accurately reflects how people use it. The team also responds to the controversy over the Library Project by engaging in public debate about its underlying principles.” (“Google Books History,” 2016)

Public debate: <https://web.archive.org/web/20160204071159/https://googleblog.blogspot.com/2005/10/point-of-google-print.html>



2006: “We launch a series of product enhancements to make Book Search more useful and easier to use. First, we expand access to the public domain works we’ve scanned by adding a download a PDF button to all out-of-copyright books. A few months later, we release a new browsing interface that makes it easier to browse and navigate Book Search. The new interface is also accompanied by new About this Book pages which use Google algorithms to populate pages with rich related content on a book -- initially, related books, selected pages and references from scholarly works. In the fall, four new libraries join the Library Project: the University of California, University Complutense of Madrid, the University of Wisconsin- Madison and the University of Virginia.” (“Google Books History,” 2016)



2007: “Using the new UI as a launching point, we experiment with new ways for people to interact with books.

Places in this Book: A mashup with Maps lets people browse books by locations mentioned in the text (later, we release an experimental KML layer for Google Earth that does the reverse -- the user picks a location, and we map books to it).

Popular Passages: We create a new way to navigate between books, tracking the use of a single passage through a collection of books.

My Library: We help people harness the power of Google search within their own personal book collections. Users begin to curate and share their personal libraries, reviews and ratings with others.

New homepage (initially US only): We give people more jumping off points for exploring the books in our index.

... we add a “View plain text” link to all out-of-copyright books. T.V. Raman explains how this opens the book to adaptive technologies such as screen readers and Braille display” (“Google Books History,” 2016)

“By December, the Book Search interface is available in over 35 languages, from Japanese to Czech to Finnish.  Over 10,000 publishers and authors from 100+ countries are participating in the Book Search Partner Program.” (“Google Books History,” 2016)

“In May, the Cantonal and University Library of Lausanne, and Ghent University Library join the Book Search program, adding a substantial amount of books in French, German, Flemish, Latin and other languages, and bringing the total number of European libraries partners to six. ...  Over 10,000 publishers and authors from 100+ countries are participating in the Book Search Partner Program.  The Library Project expands to 28 partners, including seven international library partners: Oxford University (UK), University of Complutense of Madrid (Spain), the National Library of Catalonia (Spain), University Library of Lausanne (Switzerland), Ghent University (Belgium) and Keio University (Japan).”(“Google Books History,” 2016)



The “Google Books History” page ends with 2007, with the final words of “As we look to the year ahead, we continue to develop our technology and expand our partnerships with publishers and libraries all around the world. Stay tuned...” but the page spent more than a decade with no further updates. Some time between August 2019 and March 2020, the page was edited so that it no longer had a year-by-year breakdown of milestones. Now, after briefly telling the anecdote about the 1996 origin of Google Books, the “history” says ”Fast forward to today:

After more than a decade of evolution, innovation and strong partnerships, Google Books has helped to make more than 40 million books discoverable, in more than 400 languages.

And we're not done -- not until all of the books in the world can be found by everyone, everywhere, at any time they need them.” (“Google Books History,” 2020)



2008, the year that Google stops being quite so proud of the rapid progress of Google Books, is also the year that they begin to face legal repercussions for their “scan first and ask questions later” approach to mass digitization. Somers has characterized this process as equivalent to the burning of the library of Alexandria: “When the most significant humanities project of our time was dismantled in court, the scholars, archivists, and librarians who’d had a hand in its undoing breathed a sigh of relief, for they believed, at the time, that they had narrowly averted disaster” (Somers).



Wikipedia: “May 2008: Microsoft tapered off and planned to end its scanning project, which had reached 750,000 books and 80 million journal articles.\[92\]

Wikipedia: “October 2008: A settlement was reached between the publishing industry and Google after two years of negotiation. Google agreed to compensate authors and publishers in exchange for the right to make millions of books available to the public.\[9\]\[93\]

Wikipedia: “November 2008: Google reached the 7 million book mark for items scanned by Google and by their publishing partners. 1 million were in full preview mode and 1 million were fully viewable and downloadable public domain works. About five million were out of print.\[18\]\[94\]\[95\]

Wikipedia: “December 2008: Google announced the inclusion of magazines in Google Books. Titles include New York Magazine, Ebony, and Popular Mechanics\[96\]\[97\]

Wikipedia: “February 2009: Google launched a mobile version of Google Book Search, allowing iPhone and Android phone users to read over 1.5 million public domain works in the US (and over 500,000 outside the US) using a mobile browser. Instead of page images, the plain text of the book is displayed.\[98\]

Wikipedia: “May 2009: At the annual BookExpo convention in New York, Google signaled its intent to introduce a program that would enable publishers to sell digital versions of their newest books direct to consumers through Google.\[99\]

Wikipedia: “December 2009: A French court shut down the scanning of copyrighted books published in France, saying this violated copyright laws. It was the first major legal loss for the scanning project.\[100\]



### 2.3.6.  2010s ###  

In 2011, the Center for Bibliographical Studies and Research at the University of California Riverside was awarded a planning grant from the Andrew W. Mellon Foundation to “redesign the ESTC as a 21st century research tool” (“Planning Grant”)[ add to bibliography], which was followed in 2013 by a larger two-year grant to execute software improvements to the ESTC.



#### 2.3.6.1.  ECCO ####



In late 2019, Gale began allowing access to a new interface, the Gale Digital Scholar Lab, which dramatically changed the forms of access available for ECCO texts. It became possible not only to see the underlying OCR for texts, but to run pre-built text mining on it, and to download the OCR as text files. The only limit to downloading is that only 10,000 texts may be downloaded at a time, but as long as the desired corpus can be defined as “collections” in chunks of 10,000 or less, any number of files can be downloaded. In a particularly dramatic departure from ECCO’s past practice and current norms, I was told that there were also no restrictions on sharing the downloaded files, even though downloading them in the first place required a library subscription.  

Wikipedia: “April 2010: Visual artists were not included in the previous lawsuit and settlement, are the plaintiff groups in another lawsuit, and say they intend to bring more than just Google Books under scrutiny. "The new class action," read the statement, "goes beyond Google's Library Project, and includes Google's other systematic and pervasive infringements of the rights of photographers, illustrators and other visual artists."\[101\]

Wikipedia: “May 2010: It was reported that Google would launch a digital book store called Google Editions.\[102\] It would compete with Amazon, Barnes & Noble, Apple and other electronic book retailers with its own e-book store. Unlike others, Google Editions would be completely online and would not require a specific device (such as kindle, Nook, or iPad).

Wikipedia: “June 2010: Google passed 12 million books scanned.\[12\]

Wikipedia: “August 2010: It was announced that Google intends to scan all known existing 129,864,880 books within a decade, amounting to over 4 billion digital pages and 2 trillion words in total.\[12\]

Wikipedia: “December 2010: Google eBooks (Google Editions) was launched in the US.\[103\]

Wikipedia: “December 2010: Google launched the Ngram Viewer, which collects and graphs data on word usage across its book collection.\[31\]

Wikipedia: “March 2011: A federal judge rejected the settlement reached between the publishing industry and Google.\[104\]



“At a time when the rest of Google was obsessed with making apps more “social”---Google Plus was released in 2011---Books was seen by those who worked on it as one of those projects from the old era, like Search itself, that made good on the company’s mission “to organize the world’s information and make it universally accessible and useful.” It was the first project that Google ever called a “moonshot.”” (Somers)



Wikipedia: “March 2012: Google passed 20 million books scanned.\[105\]\[106\]

Wikipedia: “March 2012: Google reached a settlement with publishers.\[107\]

Wikipedia: “November 2013: Ruling in Authors Guild v. Google, US District Judge Denny Chin sides with Google, citing fair use.\[109\] The authors said they would appeal.\[110\]

Wikipedia: “October 2015: The appeals court sided with Google, declaring that Google did not violate copyright law.\[111\] According to the New York Times, Google has scanned more than 25 million books.\[10\]

Wikipedia: “April 2016: The US Supreme Court declined to hear the Authors Guild's appeal, which means the lower court's decision stood, and Google would be allowed to scan library books and display snippets in search results without violating the law.\[112\]” (Wikipedia)



Wikipedia: “As of October 2015, the number of scanned book titles was over 25 million, but the scanning process has slowed down in American academic libraries.\[10\]\[11\] As of October 2019, Google celebrated 15 years of Google Books and provided the number of scanned books as more than 40 million titles.\[14\]”



Wikipedia: “Google has been quite secretive regarding its plans on the future of the Google Books project. Scanning operations had been slowing down since at least 2012, as confirmed by the librarians at several of Google's partner institutions. At University of Wisconsin, the speed had reduced to less than half of what it was in 2006. However, the librarians have said that the dwindling pace could be a natural result of maturation of the project -- initially stacks of books were entirely taken up for scanning whereas now Google only needed to consider the ones that have not been scanned already.\[49\] The company's own Google Books history page ends in 2007, and the Google Books blog was merged into the Google Search blog in 2012.\[113\] Despite winning the decade-long litigation in 2017, The Atlantic has said that Google has "all but shut down its scanning operation."\[20\] In April 2017, Wired reported that there were only a few Google employees working on the project, and new books were still being scanned, but at a significantly lower rate. It commented that the decade-long legal battle had caused Google to lose its ambition.\[113\]”



Material is still being sent to Google Books for scanning, however. In November 2019, University of Colorado Boulder announced that their library would be partnering with Google for books to be scanned, with copies appearing both in Google Books and in HathiTrust. (“Increasing Access with Google Books”)

“In total, the process is estimated to take two to four years to complete. With this estimation, Interim Director of Libraries Information Technology Michael Dulock approximated that if Google processed 200,000 books and each book was about 200 pages, this project will save the Libraries about $20 million. 

Another way to measure savings for the Libraries is with time. Dulock said that if Digital Media Services in the Libraries worked on this project full-time, with one staff member at 40 hours a week and five students at 20 hours a week, doing nothing else, it would take close to 100 years to complete.” (“Increasing Access with Google Books”)





“HathiTrust was launched in 2008 by the 11 University of California libraries and the 12-university consortium known as the Committee on Institutional Cooperation (CIC), with key support provided by the University of Michigan and Indiana University.” (Karels) “As of today \[October 13, 2008\], HathiTrust contains more than 2 million volumes and approximately ¾ of a billion pages, about 16 percent of which are in the public domain. Public domain materials will be available for reading online. Materials protected by copyright, although not available for reading online, are given the full range of digital archiving services, thereby offering member libraries a reliable means to preserve their collections.” (HathiTrust, “Major Library Partners Launch HathiTrust Shared Digital Repository”)

“When Google partnered with university libraries to scan their collections, it had agreed to give them each a copy of the scanning data, and in 2008 the HathiTrust began organizing and sharing those files. (It had to fend off the Authors Guild in court, too.) HathiTrust has 125 member organizations and institutions who “believe that we can better stewardresearch and cultural heritage by working together than alone or by leaving it to an organization like Google,” says Mike Furlough, the trust’s director.” (Van Helden)

“The vast majority of those digitized books-around 95 percent, as of mid-2017- had originally been scanned as part of the Google Books project; the agreements that Google Books entered into with the libraries typically stipulated that Google had to provide the library with a digital copy of each book scanned from that library.” (Bauder)

In 2010, just two years later, ”HathiTrust is now jointly owned and operated by 52 institutions from the U.S. and Europe, all focused on a common goal -- to build an extraordinary digital library that preserves and provides access to the cultural record. The new members to HathiTrust include the Library of Congress, Stanford University, Arizona State University, Massachusetts Institute of Technology, and University of Madrid, HathiTrust’s first international partner.” (Karels)

”In October 2015, HathiTrust comprised over 13.7 million volumes, including 5.3 million of which were in the public domain in the United States.” (Wikipedia)  

As of 2017, the delivery of free CDs has been discontinued, though the ISO image is still available for download.  

Created to pool resources/share costs to meet immense scholarly demand (focused on physical books)

MLA talk Jan 20 2021

The work of archivists is accepting and managing loss

![][ScreenShot2021-01-10at12.20.20PM]![][ScreenShot2021-01-10at12.22.09PM]

“Analog collections are *in a relative sense* dramatically less important than they were, but this does not mean *un*important”

![][ScreenShot2021-01-10at12.24.37PM]

Goes right to HathiTrust as a crucial “collective effort”

The real action is to *share funds*



## 2.4.  conclusions ##  

I contend that each database is best understood as a negotiation between the noncommercial values of textual reproduction and the commercial environment in which institutions much remain financially solvent. Each database has the goal of making valuable information available. After the 1990s, they are particularly influenced by the utopian ideal that digital reproduction at last made textual reproduction free. Each had to contend, however, with the fact that before a text can be reproduced digitally it must be *created* digitally, and that even if the material costs are entirely eliminated (which, of course, they are not) textual creation continues to have costs in labour. In Paddy Bullard’s “Digital Humanities and Electronic Resources in the Long Eighteenth Century,” which surveys the research completed and the resources used as of 2013, Bullard is also faced with the task of explaining why multiple services interact so poorly. Bullard, too, observes the core tension between public access vs private profit: 

Viewing the field of eighteenth-century digital humanities as a single prospect, it is the contrast between publicly funded, open-access sites, and privately owned, subscription-access resources that is most striking. Each side of the divide has much to learn from the other. Publicly funded academic projects must acquire the pragmatism and ambitiousness of scale that commercial developers have always shown. Commercial developers must adapt themselves more generously to the principles of scholarly openness and accuracy. They might also imitate the inventiveness of the open sector, its adaptability to the demands raised by different kinds of primary media. Both sides recognize the desirability of making their resources interoperable across the divide, and the business of interconnectivity will preoccupy all kinds of digital humanist in the coming decade. (756)

Bullard is correct to note that there are major disjunctions between databases like the *The British Book Trade Index* or careful online editions like *The Proceedings of the Old Bailey, 1674--1913*, compared to massive archives like ECCO. It seems odd, however, to attribute to ECCO *both* “ambitiousness of scale” *and* “pragmatism” as the lessons for noncommercial projects to imitate, since an ambitious scale is only plausibly pragmatic for a project with the money to sustain itself. Even odder is the idea that commercial developers might voluntarily choose to “adapt themselves more generously to the principles of scholarly openness and accuracy,” when the core business model of a private enterprise relies on its lack of openness, and the private access only seems worth purchasing when its marketers suppress all nuance about accuracy. As Bode observes, “the commercial imperatives of these enterprises arguably depend on them presenting these collections as comprehensive” (Bode World 47).[^cf30] In other words, Bullard has observed an underlying system of profit and non-profit in awkward competition, and examined the outputs of these systems in order to articular their particular virtues and describe what a ‘best of both worlds’ might look like if both parts of the system sought to collaborate together on how best to achieve maximally useful scholarly resources. What Bullard overlooks in this process is that not all parts of this system have the goal of achieving maximally useful scholarly resources.  

Bullard suggests tentatively that university presses might be site of bridging efforts between the non-profit and for-profit worlds, but where we can actually see an example occurring is in the Text Creation Partnership. The TCP attempted to intervene in the system with “a public-private partnership, led by libraries” (TCP, “About”); their materials emphasize the “librarian’s attitude toward content” which prioritizes the widest possible access and use. This “librarian’s attitude” is most evident in the (eventual) availability of all of the transcriptions in the public domain, despite the fact that the images they are based on remain privately restricted by the companies which own them. Their description of the “partnership,” however, continues to show signs of the strain in value systems when commercial and noncommercial goals are intertwined: “Through our partnership with private vendors, we had access to a huge trove of images from which to transcribe. In return, these companies were supplied with a full-text index to their images ---work which would have otherwise been difficult or expensive to produce.” In other words, through purchasing a service (access to images), the academic institutions received that service. These academic institutions carried out an enormous feat of labour at their own expense, using the service they purchased. Then, “in return,” they provided the results of their labour to the company, for the company to then further profit from the improvements to their service. Most telling, here, is the word “otherwise” in calling this “work which would have otherwise been difficult or expensive to produce.” The suggestion here is that, without the TCP, the companies themselves would not have been willing to undertake the encoding that was so desired by the users of their service. However, the TCP certainly did not make the task any less difficult or expensive. Instead, academic institutions absorbed the difficulty and expense on those companies’ behalf. I do not say that they were wrong to do so: on the contrary, the “librarian’s attitude” mirrors my own attitude, and it is surely to everyone’s benefit for a wonderful thing to exist even if that wonderful thing is not profitable. Rather, I highlight this rhetorical moment in the TCP’s self description to suggest that it takes two to collaborate, and that no amount of effort on the librarians’ part can change the core institutional drive of a private company. Companies like Gale are perfectly happy to help achieve maximally useful scholarly resources if doing so it also a good way to turn a profit, but this does not mean that they have the same goals as academic institutions. One of the three key aims of the TCP identified on the homepage is to “collaborate with commercial providers, rather than constantly bargaining and competing with them” (TCP “Welcome”). However, the TCP seems instead to have simply come up with a *better* bargain, one which creatively offers scholarly labour as a bargaining chip.  

The TCP’s very partial successes are perhaps most visible when we contrast the TCP with Project Gutenberg, which has operated since its inception without collaborating with the commercial sector. The Project Gutenberg mission has always been focused on freely giving away things which were acknowledged to have financial value. After all, Hart’s goal was to “pay back” the $100,000,000 he had been given in computing time --- giving away ebooks would generate and give away this financial value. But even in seeking to “pay back” there is an impulse behind Project Gutenberg which exceeds ideas of monetary value. The explanatory documents of the project, when it became a major volunteer undertaking in the 1990s, read like a manifesto:

“Encourage the Creation and Distribution of eBooks

Help Break Down the Bars of Ignorance and Illiteracy

Give As Many eBooks to As Many People As Possible” (History-Computer)

These goals are not so dissimilar to those of public libraries.



“As Barbara Quint, editor of Searcher, said to me after reading some Gutenberg texts: "It struck me how noble, how wonderful, how great-spirited the people who made this all possible were. The time, the tedium, the labor it would take to render a book digital from a print copy." Further, she noted, "It's all a reminder of how much of what is the best on the Web and the Net comes from the kindness of strangers.”” (Hane)  

What do these database histories mean for scholars of eighteenth century literature? First and foremost, these histories provide another reminder that these things do not exist prior to interpretation or intervention. It is not merely that they are *shaped* or *influenced* by their institutional contexts, implying small quirks or edge cases which can generally be ignored: they are *constituted in the first place* by those institutional contexts. Secondly, these histories suggest a course of action to be taken in response to the specific institutional factors constituting each database. Scholars periodically acknowledge the gaps between historical events as they occurred and the specific archive, database, or corpus that they are using as a proxy for the idealized concept of “the historical record,” but these acknowledgements typically take the form of a statement that some form of bias is assumed to exist, but that this bias is so unknowable and unavoidable that naturally we will just continue onward as if it was not present. Identifying the specific institutional process that led to the current digital infrastructure undermines efforts to brush off these details as unknowable: directly investigating the actual demographics of each resource’s holdings, as I do in future chapters, can also render these biases no longer unavoidable.  

Hart divides the works hosted on Project Gutenberg into three categories: “light literature,” “heavy literature,” and “references.”



A core early selection method was relevance to pop culture. It is desirable for *Peter Pan* to be accessible because people have just seen the movie *Hook*; it is desirable for Moby Dick to be accessible because it is referenced in *Star Trek*. Hart imagines reading motivated by natural curiosity, not building into an institutionally recognizable status as an ‘educated person.’



34 heavy literature; 42 light literature; 24 references



More recent additions to Project Gutenberg are less likely to have this kind of immediate name recognition. Looking at the first 100 titles added in 2020, for example, we see titles like “Shafting, Pulleys, Belting and Rope Transmission \[Subtitle: The Power Handbooks Library\]” and “The Archives of Dentistry Vol. 7 No. 12 December 1890.”

But I don’t want to imply that Project Gutenberg has run out of valuable books to digitize. From the lens of expanding the canon, Project Gutenberg has just added “Chains and Freedom \[Subtitle: or, The Life and Adventure of Peter Wheeler, a Colored Man Yet Living\]”    

The actual way that books enter Wikipedia is through the website Distributed Proofreaders. It operates a lot like LibriVox in the sense that a volunteer will ‘sponsor’ a particular book and take responsibility for coordinating other volunteers and shepherding it through the proofreading process. Generally they will physically acquire a pre-1921 book, scan, and OCR it. Proofreaders will then check a few pages at a time to correct OCR errors.



A sign of openness to a particular community: “Distributed Proofreaders regrets that we are unable to verify court-ordered community service because our system cannot adequately record time spent participating.”  

Check how many DP volunteers: https://www.pgdp.net/c/

Check most popular PG titles: https://www.gutenberg.org/browse/scores/top



# ch 2 - gender in databases (13k) #  

Supporting Guillory’s assertion that there simply are fewer women writing in the past, I feel fully justified in reading “Dr.” as a gender marker indicating a male author, even though such a reading is wholly untenable today.  

***Databases of Eighteenth-Century Literature, 1789-99***

Returning to the eighteenth century, and how it has been digitized: the baseline hypothesis, investigating a historical phenomenon, is that factors like systemic sexism are likely to have an effect. As I have shown above, the digital archives currently used by eighteenth-century scholars were created during their own historic moments, which influenced their priorities as texts were digitized. Resources which provide more information about particular works --- such as ECCO, which provides PDF scans of microfiche, or the TCP, which provides detailed XML-encoded transcripts --- inevitably provide their more thorough information about fewer texts. The ESTC is able to include so many works in its database in part because it includes so little about each work: not much more than the information found on the title page, and a list of libraries where the original can be consulted. Google Books (the source of HathiTrust’s images) achieved its scale in part by dramatically reducing the detail and precision of each work’s records. As the more resource-intensive archives created their digital surrogates, therefore, the texts they include are likely to be less indiscriminate, and more strongly influenced by a personal assessment of what works are most important. In many other areas, writing by women has been dismissed as less important or worthy of serious study than writing by men. If we compare the holdings of major digital archives, do the smaller and more resource-intensive archives show evidence of similar choices to systematically opt out of an investment in women’s writing?

Guillory’s *Cultural Capital* proposes that, if female authors, for example, do not make up a demographically accurate 50% of our literary syllabi, this is not necessarily because women’s writing has been excluded or repressed. Guillory draws a distinction between rhetoric “construing the process of canon formation as an exclusionary process essentially the same as the exclusion of socially defined minorities from power” --- that is, the patriarchal suppression of works by women, which can be addressed by now ‘representing’ the excluded works within the canon --- versus what he sees as the real cause, a historical fact that “women were routinely excluded from *access to literacy*, or were proscribed from composition or publication in the genres considered to be serious rather than ephemeral.” Applied to the digital database, the first explanation, which might be termed the ‘exclusion hypothesis,’ would manifest in the form of massive archives with large numbers of works by men and women, which get filtered through exclusionary scholarly processes into smaller, selective repositories where men now make up an oversized share. In the second explanation, which might be termed the ‘absence hypothesis,’ works by women will have a diminished role from the very beginning, since women were prevented from writing in the first place.

I have taken information from four resources most strongly targeted to scholars of eighteenth century literature, namely, the English Short Title Catalogue, Eighteenth Century Collections Online, HathiTrust, and the Text Creation Partnership. Since it would be infeasible in many ways to compare the full holdings of all these resources, we much take a consistent sliver from each resource, and compare within those parameter. Let us examine works published in England, between 1789 and 1799. After removing misclassified works, the ESTC contained 51,090 titles, ECCO 26,848 titles, HathiTrust 8,220 titles, and the TCP only 525 titles. ECCO, in all its capaciousness, therefore contains only 52.5% of the titles listed in the ESTC. The TCP provides access to an almost negligible 1% of the works published. At this scale, let us follow the fate of a larger figure than Charlotte Smith herself, and see how “the named woman” fares.

Smaller, less comprehensive archives do generally dedicate a higher percentage of their space to male authors. As Figure 3 shows, 43% of the 51,090 titles in the ESTC from this decade are identifiably by men, which rises substantially to 69% for HathiTrust and 67% for the TCP. 



![][pastedGraphic]

Figure 3: The percentage of works published in England 1789-99 with male authors, as identified by the ESTC, ECCO, HathiTrust, and the TCP.

However, as Figure 4 shows, the increase in male authors is not at the expense of identifiable female authors, who make up a stable or increasing percentage of the works as databases grow more specialized--- including an astonishing 22% of the TCP corpus.

![][pastedGraphic-1]

Figure 4: The percentage of works published in England 1789-99 with male versus female authors, as identified by the ESTC, ECCO, HathiTrust, and the TCP.

The initial number for women, of course, is extremely low: a mere 3%. This is far below the percentage of women found in genre-specific surveys. For example, during the years 1789-99, 20.2% of the new novels written in England were attributed to female names on their title pages and prefaces. The ESTC and ECCO have a lot of medical texts, sermons, natural histories, legal opinions, and other genres from which women are de factor excluded. HathiTrust’s slight increase in female authorship might show a collections-level bias where university libraries prefer slightly more interesting or recognizably “literary” materials like novels, poetry, and drama, all of which allowed for female authorship. But even if the TCP were assumed to contain exclusively novels, at 22% female authorship it would be statistically overrepresenting women. The TCP is the one operating based on the most recent decision-making, since even HathiTrust is constrained to the library collecting decisions of past decisions, and thus doesn’t necessarily reflect the priorities of 2000s scholars.

In Figure 5, however, we can see that the overlooked group, unable to compete with male authors for archival attention, is not “female authors” but “authors which are not associated with an individual personality.” The category “unsigned” here captures works to which no name at all was signed (essentially, anonymous works, though in this period of course the absence of a name does not imply the concealment of a name as it would in the contemporary definition of “anonymous”) as well as pseudonymous works which offered no clues to an author’s gender (e.g. “A friend of peace”). “Organizations” include missives from various branches of the government, reports from scholarly and charitable organizations, catalogues from various companies, and other works which are easily rejected as “non-literary.” The identifier “blank,” for ECCO, appears to capture both unsigned works and works by organizations.



![][pastedGraphic-2]

Figure 5: The relative authorship status of works published in England 1789-99, as identified by the ESTC, ECCO, HathiTrust, and the TCP. Works by “org” are attributed to organizations as their authors, such as the House of Commons. “Unsigned” works have no information in the author field. “Blank” in ECCO captures both unsigned works and works by organizations, which I have not yet been able to distinguish. Works by “initials” include those by, e.g., W.H. Ireland, even when the database has provided a clearly gendered expansion of the initials (e.g., a listing of “W\[illiam\] H\[enry\] Ireland”), to reflect the fact that the original author listing was of ambiguous gender. From my casual inspection, in nearly all cases of attributed to initials where a known author name is also supplied, the author is male. An author will be identified as “male” or “female” rather than as “initials” if, in addition to the initials, a gendered title is provided, e.g., “Mrs. R.” The “uncategorized” works are those which I have not manually assessed; anecdotally, these appear to consist mostly of pseudonyms, mostly male.



There are two ways to explain the disappearance of unsigned authors from smaller corpora. The first explanation is that unsigned authors have, essentially, faced discrimination due to their status as unsigned: out of a fixed pool of unsigned works, fewer were chosen for further scholarly dissemination than from the pool of signed works. The second explanation is that the further dissemination of a work, or scholarly investment in a work, reduces the likelihood that it will *remain* unsigned in the database. For an author to appear as “unsigned” here, they need not only be absent from the title page, but they also need to have avoided a later scholarly attribution of authorship. Names are placed into the relevant database fields wherever possible, supplying new information that cannot be found on the title page and may have been entirely unavailable to eighteenth century readers. Ann Radcliffe’s first novel, for example, was published unsigned, and the next several editions identified her as the “authoress” of the previous works, and only with the third edition of *The Romance of the Forest* did the name “Ann Radcliffe” appear on the book. All of the earlier editions, however, are consistently linked to Ann Radcliffe as the author in the internal database, and in the data I was able to access. There is obvious practical use in disseminating author identifications --- indeed, there is very little point in determining the authors of pseudonymous works if this information is not fully incorporated into the scholarly record. But this particular implementation also occludes how eighteenth century readers actually encountered author information. An identified name overwrites the information recorded on the title page. In this way, scholars have worked to infrastructurally eliminate the “unsigned” author.

Rather than finding bias against authors based on the social category of gender, then, I have found a much stronger bias in favour of a particular concept of the author itself. These resources assume, in their very infrastructure, that the “author” must be reducible to a single, known individual. ECCO so thoroughly rejects the idea that organizations could be considered as “authors” that it does not record them in the “author” field. The underrepresentation of unsigned or corporate authors cannot be mapped onto the multicultural rhetoric of political representation, the way Guillory has described with gendered representation. Instead, the meaning of this finding stems from the very *lack* of a broader social importance to “unsigned” or “non-individual” authors as a class of humans: the conclusions we can draw here are not about the world at large, but about the institutional processes of academic research.

***“Representativeness” in Databases***

Returning to the idea of “representativeness” --- what it means for a database to “represent” or be “representative of” a population --- returns us, also, to the still-fraught relationship between distant reading and the conception of the literary canon. Twenty years after Guillory, we are still debating the need for “literary criticism ... to conceptualize a new disciplinary domain,” now in the context of computation. The reconceptualization of literary study itself is at the core of Franco Moretti’s coinage of ‘distant reading’: the problem for which “\[r\]eading ‘more’ seems hardly to be the solution” is the problem of conceiving of a “world” literature, rather than the “canonical fraction, which is not even one per cent of published literature.” His new methods are meant to enable literary studies to examine a new object. The field of distant reading has been moving away from Moretti himself. However, it is still shaped by the attempt to redefine the disciplinary domain of literary studies. In many cases, the new domain is no longer the “canon” but the “corpus,” a collection of texts which are studied en masse for macroanalytical insights. Katherine Bode, for example, in “The Equivalence of ‘Close’ and ‘Distant’ Reading,” argues that Franco Moretti and Matthew Jockers replicate the approaches of New Criticism with their corpora, and calls for “a new scholarly object of analysis” that directly examines historical and textual context of corpora as representations of “literary systems.” Lauren Klein, too, treats the textual corpus as the new object of literary analysis requiring curation, contextualization, and interpretation. Her critique argues that “it’s not a *coincidence* that distant reading does not deal well with gender, or with sexuality, or with race,” but also that these failings are not inevitable: “it’s not that distant reading *can’t* do this work,” she insists, “it’s that it’s yet to sufficiently do so.” Bode, too, despite her strong critique of distant reading as it has been practiced by Moretti and Jockers, does not blame distant reading itself. Distant readers like Moretti and Jockers, she argues, “while claiming direct and objective access to ‘everything,’ ... represent and explore only a very limited proportion of the literary system, and do so in an abstract and ahistorical way.” Klein, like Bode, calls for “more corpora---more accessible corpora---that perform the work of recovery or resistance” to allow research “beyond quote ‘representative’ samples, which tend to reproduce the same inequities of representation that affect our cultural record as a whole.” This framing re-creates, at the site of the corpus, the identical narratives of exclusion and representation which were previously located in critiques of the canon.

The relocation of the debate from the canon to the corpus, on the surface of it, is not without grounds. Challenges to the technological accessibility of texts have created new hierarchies, and a new “great unread.” Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. However, what we learn from these case studies, of Charlotte Smith and of the 1790s at large, is that the decisions made about inclusion and exclusion from digital databases simply do not mirror the decisions made in syllabi or anthologies.

I contend that each database is best understood as a negotiation between the noncommercial values of textual reproduction and the commercial environment in which institutions much remain financially solvent. Each database has the goal of making valuable information available. After the 1990s, they are particularly influenced by the utopian ideal that digital reproduction at last made textual reproduction free. Each had to contend, however, with the fact that before a text can be reproduced digitally it must be *created* digitally, and that even if the material costs are entirely eliminated (which, of course, they are not) textual creation continues to have costs in labour. In Paddy Bullard’s “Digital Humanities and Electronic Resources in the Long Eighteenth Century,” which surveys the research completed and the resources used as of 2013, Bullard is also faced with the task of explaining why multiple services interact so poorly. Bullard, too, observes the core tension between public access vs private profit: 

Viewing the field of eighteenth-century digital humanities as a single prospect, it is the contrast between publicly funded, open-access sites, and privately owned, subscription-access resources that is most striking. Each side of the divide has much to learn from the other. Publicly funded academic projects must acquire the pragmatism and ambitiousness of scale that commercial developers have always shown. Commercial developers must adapt themselves more generously to the principles of scholarly openness and accuracy. They might also imitate the inventiveness of the open sector, its adaptability to the demands raised by different kinds of primary media. Both sides recognize the desirability of making their resources interoperable across the divide, and the business of interconnectivity will preoccupy all kinds of digital humanist in the coming decade.

Bullard is correct to note that there are major disjunctions between databases like the *The British Book Trade Index* or careful online editions like *The Proceedings of the Old Bailey, 1674--1913*, compared to massive archives like ECCO. It seems odd, however, to attribute to ECCO *both* “ambitiousness of scale” *and* “pragmatism” as the lessons for noncommercial projects to imitate, since an ambitious scale is only plausibly pragmatic for a project with the money to sustain itself. Even odder is the idea that commercial developers might voluntarily choose to “adapt themselves more generously to the principles of scholarly openness and accuracy,” when the core business model of a private enterprise relies on its lack of openness, and private access only seems worth purchasing when its marketers suppress all nuance about accuracy. As Bode observes, “the commercial imperatives of these enterprises arguably depend on them presenting these collections as comprehensive.” In other words, Bullard has observed an underlying system of profit and non-profit organizations in awkward competition, examined the outputs of each approach in order to articulate their particular virtues, and finally described what a ‘best of both worlds’ might look like if both parts of the system sought to collaborate together on how best to achieve maximally useful scholarly resources. What Bullard overlooks in this process is that not all parts of this system have the goal of achieving maximally useful scholarly resources.

Bullard suggests tentatively that university presses might be the site of bridging efforts between the non-profit and for-profit worlds, but where we can actually see an example occurring is in the Text Creation Partnership. The TCP attempted to intervene in the system with “a public-private partnership, led by libraries;” their materials emphasize the “librarian’s attitude toward content” which prioritizes the widest possible access and use. This “librarian’s attitude” is most evident in the (eventual) availability of all of the transcriptions in the public domain, despite the fact that the images they are based on remain privately restricted by the companies which own them.

Their description of the “partnership,” however, continues to show signs of the strain in value systems when commercial and noncommercial goals are intertwined: “Through our partnership with private vendors, we had access to a huge trove of images from which to transcribe. In return, these companies were supplied with a full-text index to their images ---work which would have otherwise been difficult or expensive to produce.” In other words, through purchasing a service (access to images), the academic institutions received that service. These academic institutions carried out an enormous feat of labour at their own expense, using the service they purchased. Then, “in return,” they provided the results of their labour to the company, for the company to then further profit from the improvements to their service. Most telling, here, is the word “otherwise” in calling this “work which would have otherwise been difficult or expensive to produce.” The suggestion here is that, without the TCP, the companies themselves would not have been willing to undertake the encoding that was so desired by the users of their service. However, the TCP certainly did not make the task any less difficult or expensive. Instead, academic institutions absorbed the difficulty and expense on those companies’ behalf. I do not say that they were wrong to do so: on the contrary, the “librarian’s attitude” mirrors my own attitude, and it is surely to everyone’s benefit for a wonderful thing to exist even if that wonderful thing is not profitable. Rather, I highlight this rhetorical moment in the TCP’s self description to suggest that it takes two to collaborate, and that no amount of effort on the librarians’ part can change the core institutional drive of a private company. Companies like Gale are perfectly happy to help achieve maximally useful scholarly resources if doing so it also a good way to turn a profit, but this does not mean that they have the same goals as academic institutions. One of the three key aims of the TCP identified on the homepage is to “collaborate with commercial providers, rather than constantly bargaining and competing with them.” However, the TCP seems instead to have simply come up with a *better* bargain, one which creatively offers scholarly labour as a bargaining chip.

What do these database histories mean for scholars of eighteenth century literature? First and foremost, these histories provide another reminder that scholarly materials do not exist prior to interpretation or intervention. It is not merely that they are *shaped* or *influenced* by their institutional contexts, implying small quirks or edge cases which can generally be ignored: they are *constituted in the first place* by those institutional contexts. Second, these histories suggest a course of action to be taken in response to the specific institutional factors constituting each database. Scholars periodically acknowledge the gaps between historical events as they occurred and the specific archive, database, or corpus that they are using as a proxy for the idealized concept of “the historical record,” but these acknowledgements typically take the form of a statement that some form of bias is assumed to exist, but that this bias is so unknowable and unavoidable that naturally we will just continue onward as if it was not present. Identifying the specific institutional process that led to the current digital infrastructure undermines efforts to brush off these details as unknowable: directly investigating the actual demographics of each resource’s holdings can also render these biases no longer unavoidable.

***Conclusion***

As archivists and librarians often note, “the archive” is not only a concept to be brought into play with literary theory, but a specific and materially constructed space, shaped by archivist labor and constraints. A digital archive is no different. To explain the current contents of online databases like ESTC, ECCO, HathiTrust, and the TCP, we must look to histories that begin before the internet itself. Recognizing that they are historically constructed resources, we can be alert to selection factors that cause all of them, even the most comprehensive, to differ from the imagined prior whole of “literature” which we might wish for them to represent. More importantly, attention to their particular details reveals that the systemic selection factors that have defined their gaps and exclusions do not always map onto the demographics we dedicate the most attention to recovering. Corporate business models are likely to play as large a role as social biases. Scholarly infrastructure can create new demographics like “the unsigned author” which do not constitute an identity category in the real world. And sometimes success creates its own new puzzles: it is a valuable accomplishment of the TCP, for example, that it has increased the availability of writing by women --- but it seems prudent for researchers to assume that using the TCP to gather samples of works published in England during the late eighteenth century will yield samples which do not reflect the population. Unless researchers takes explicit steps to the contrary, these samples will not reflect eighteenth century history, but their own.  

Women made up a nearly negligible portion of the titles published in England during the 1790s. If one were to choose a title at random, it would be more likely to be written by a man named John than by a woman. But we do not choose titles at random--- even in large-scale digital humanities research. We neglect the genres from which women are most strongly excluded: medical texts, legal texts, sermons, and other discourses tied to specialized (male) occupations. We prefer fiction and poetry, the forms most open to women. Even then, we overrepresent women. I want to be very explicit that the overrepresentation of women is not inappropriate: it strikes me as a very positive sign that we might have grown less sexist in the intervening two hundred years. Instead, the logic of “representation,” which underpins discussion of literary canons and syllabi, is not a good fit for understanding the infrastructural impact of contemporary digital databases. 



Discovering that women account for only 3% of attributed authors at the end of the eighteenth century 



\- 3% is really small

\- Scholarly work overrepresents women

\- It is fine and appropriate to overrepresent women, per Guillory

--- A core contention of feminist scholarship is that works by women have a heightened relevance or historic interest to us today; scholarly attention does not need to exactly replicate past attention

------ We don’t have to be recovering “popular” / “successful” literature to make it interesting/worthwhile



We have not truly grappled with the challenge that the eighteenth century poses to our idea of “the author.”



# ch 3 - genre in databases (13k) #



# ch 4 - random sample (13k) #  

The version turned up by the random sample was 24p. ;  12⁰. The version in ECCO is 36p. ;  18⁰. 



Mostly the rules are about paying appropriate fees (there is a blank for the amount to be written in by hand!) and the logistics of horses, training, and uniforms. There is a lot of training available and drills required, including a lot of surveillance/record-keeping of who is in town to be called to serve.

Emphasis on the operations of particular stables.

There are also 150 “Dismounted Cavalry”! Who carry carbines or rifles with bayonets and are “freed from the Expence of Horse Keeping.” (34) --- but can keep their horses in the LHV stables, and get first refusal when there are openings for more mounted volunteers. Suggests a strong ‘social club’ element?  

She’s describing a lunar eclipse that occurred 14 Feb, 1794 AD 22:21



It opens with a lot of discussion of London’s vices and God’s goodness, and claims that the destruction of France is proof of God’s punishment for the sins. The eclipse she claims to have predicted (though I can’t turn up the earlier publication). She describes how a newspaper published a report of a strange boy pointing out an omen to a lady, and several ladies described having the same experience on similar bridges. Goodall says the omen predicts war (a pretty easy prediction since England had just gone back to war with France), and further predicts fighting along the Rhine and in Spain which will kill many people. It ends with a (clumsy) poem version of the prose.



The ESTC mentions two physical holdings--- one in the Bodleian, one in the Somerset Archaeological and Natural History Society. The Bodleian copy includes it as part of a bound volume of 87 chapbooks. The SANHS database doesn’t show a copy.

The ESTC record mentions that “TAUa copy bears MS. date 1794” but I don’t know what the TAUa copy is.  

My random sample suggested a 1794 Liverpool edition 8p. ;  12⁰. This particular version of the ballad has not been digitized, but I located five other copies in ECCO from the 1790s:

CW0117256783 N.p., \[1790?\] 1 sheet : ill. ; obl.1/2°

CB0126648281 S.n., \[1790?\] 8p. ; 8° (adds The Chimney Sweeper)

CB0127621441 \[s.n.\], Printed in the year 1796 8p. ; 12° (adds Johnny Coup)

CB0127895166 \[Edinburgh? : s.n.\], 1799. 8p. : ill. ; 12° (adds Johnny Coup)

CB0126649552 Edinburgh : Printed by J. Morren, \[1800?\]. 8p. : ill. ; 12° (adds The Yorkshire Beauty)



Of these, four seem similar in format to the 1794 Liverpool edition, being eight duodecimo pages. All four of these 8p 12⁰ editions include another ballad after Babes. They also all include illustrations on the first pace (though only 2 of the 4 have the illustration mentioned in the collocation). I therefore suspect that the 1794 Liverpool edition also likely includes a second ballad, and an illustration.



6783 and 5166/1441 and 8281 and 9552 are all distinct from each other, with many minor variations in wording throughout and different stanza breaks.

The “told his wife” line is distinctive---

6783: “Then told his wife and all he had,”

8281: “And told his wife, and all he had,”

1441: “He told his wife and all his friends”

5166: “He told his wife and all his friends,”

9552: “He told his wife and children all,”



5166 and 1441 *almost* appear to be using the identical type (tho with different title & text decoration at the top of the ballad) but looking more closely, the difference in comma on the wife line suggests that 5166 is a new typesetting but it is based on a copy of 1441.

These both add Johnny Coup as a the second ballad but the two Johnnies are NOT identical; different title & decoration, and also 5166 squeezes an extra stanza onto the first page of Johnny (but then DOESN’T repeat the chorus at the end?). The title page illustration also differs --- a similar image of a man on an ox (??) but not the same woodblock, 5166 is perhaps a rough mirrored copy of 1441?



ESTC offers a ‘uniform title’ of “Children in the wood; or the Norfolk gentleman’s last will and testament” for the ballad.  

1862 June, N.M.: Can anyone tell me where this tale comes from? Is it factual?

\[Editor supplies Sharon Turner’s speculation that it’s about Richard III\]



1866 February, William Pinkerton: says the ballad is anonymous, rebuts the attempt to connect it with Yarranton’s play, says Jerningham wrote a poem about the ballad’s author

1866 March, F.C.H., info on Edward Jerningham’s burial place and the location of the ballad

1866 March, W. Pollard: F.C.H is wrong about absolutely everything about the West Walton Church in the 3^rd^ S. ix. 208 note

1866 April, C.W., Barkley: Watton is the right place, a guy I fished with told me about a house

1866 April, A.D.: F.C.II. Is in error, not West or East Walton, but Watton and Wayland/Wailing Wood might be right

1866 April, F.C.H (immediately below): “quite submissive to the correction of Mr. W. Pollard” --- was just repeating a friend’s story, and anyway N&Q miswrote Walton for intended Watton



1872 Dec, M.D.: I remember a different version of Babes in the Wood, here it is

1873 Jan, W.H. Patterson: There’s a version a lot like M.D.’s in an 1863 book, which has the musical notes he asked for

1873 Feb, Harrison William: M.D.’S version was published in 1849 (some snarkiness about M.D.’S ignorance and poor memory)



1880 July, Edgar MacCulloch: asks about the date, suggests that it alludes to 1589 Portuguese expedition by Drake & Norris

1880 August, Henry B. Wheatley: answers the Q a bit snidely about the play it’s supposedly based on and also suggesting that it’s not about Richard III



1967 March, R.D. Barrett-Leonard: “They are said to be members of the De Grey family, but which ones?”  

Piper identifies generalization as a common and important method in literary studies, which plays a distinct role in literature compared to history or social science. He provides compelling evidence for “an inverse relationship between the scale of claims and scope of evidence provided” (71), in which “rhetoric stands in for evidence” and “interestingness becomes true (or the truth is only valued because it is interesting)” (16). \[This is a different way of describing the same phenomenon that others like Guillory and Sedgwick and Love have identified, in the figure of the critic as the authority in themselves.\] Piper turns to some ideas about ‘openness’ as the alternative way to reframe our relationship to our field--- by this he means an “expressive openness” that he sees as central to the methods of literary study. This expressive openness takes three forms: the questioning mind (“a rhetorical stance that is at once both critical and open. It doubts but also wonders” (66)), the expanding mind (“the valuation of the ‘novel’ or ‘unforeseen,’ things that have not been thought before” (67)), and the loosening mind (“ a capacity to undo associations and make room for novel connections” (67)). The goal of all this openness would be to “build confidence in our collective forms of knowledge about the world all the while continuing to imagine alternative versions” (72).



Piper and generalization is related to what I am doing with my random sample evidence because I am openly and perversely choosing insufficient, limited, arbitrary evidence and trying to see what I can make of it anyway.

In that light it seems like what I am saying might be ‘look--- I can achieve a plausible generalization even with bad evidence, suggesting that all work in the field is mere charlatanry!’ but I actually want to do something like ‘look--- this works, suggesting that we haven’t successfully defined what ‘good evidence’ is.’ The idea of the random sample is that I really do expect ‘1790s-ness’ to appear in *all* of 1790s lit; if I decide that the 1790s are ‘about’, for example, a gendered renegotiation of capitalism, then that aboutness will appear in anything I pick. But when I say it that way I get back to the circularity/always-already of close reading...  

The first story is by Oliver Goldsmith and takes more than a page before its “joke” becomes entirely evident. Reading this short story it becomes easier to see the Vicar of Wakefield as also having a comic bent to its sheer quantity of miseries..... Though I still feel like there is some real sincerity there (as indeed there is some sincerity in The Disabled Solder in its implicit social critique)



A few pieces indicate their sources - Goldsmith is a selling point on the title page, and a few on the inside indicate that they are taken from the spectator, but others don’t. How much piracy was involved?



All six of the works which have some kind of authorship attribution are advertised on the title page. There are three other pieces advertised on the title page which don’t have a specific author mentioned in the text: Parish-Jobbing, Character of a Sot, and The Cards Spiritualized (which does name the soldier supposedly responsible, Richard Middleton, in the body of the piece). This reinforces the idea that the pieces named on the title page are expected to be somewhat familiar to readers.

Everything advertised on the title page is at least a full page long. Everything not advertised is a half-page. (This also means of course that everything with an author is at least a full page long.) Since many of the half-page pieces are inserted to make up space after the end of one of these longer pieces, this suggests that the half-page jokes are somewhat interchangeable filler.  

“Consolation for the scorbutic, scrofulous, leprous, &c : and worthy also the serious perusal of the medical profession. Being an abstract from a few of those remarkable cures which illustrate Mr. Hayman's 'Treatise, explaining the nature and affinity of scorbutic diseases.’”



This is an 8p. ;  8⁰ extract; ESTC notes that it “contains testimonials and a list of persons treated” and categorizes its “Genre/form” under “Advertisements.” The only physical copy noted is at Niedersachsische Staats und Universitatsbibliothek in Gottingen, Germany. This, it mentions, is “Bound with the 'Gentleman's Magazine', vol. 63, part 2, 1793” --- a volume which is also present in HathiTrust! The OCR is too low-quality to find the advertisement by searching key words.

The index does list a Hayman on p 961 but this was the obituary of a William Hayman :(

As far as I can tell, the Gentleman’s Magazine does not include this kind of advertisement within it, so it must have been some particular reader in Germany who chose to bind them together.

I contacted the holding archive in Germany, writing a message in bad google-translate German!   

The first volume is divided into 40 sections, on a range of topics related to the body, the senses, and disease. He classifies bodily and sensory motions as "irritative," "sensitive," "voluntary," and "associate." He presents theories on the production and classes of ideas, and seeks to explain the causes and mechanisms of sleep, reverie, vertigo, and drunkenness. He then discusses anatomy, especially the operation of the circulatory system and various glands. Chapter 29, "The Retrograde Motions of the Absorbent Vessels," is Erasmus Darwin's translation of his late son [Charles Darwin](https://en.wikipedia.org/wiki/Charles\_Darwin\_(medical\_student))'s dissertation. These anatomical chapters are followed by four chapters on diseases, which draws on his classification of four types of motion to identify four types of diseases: those of irritation, of sensation, of volition, and of association. Two chapters, "Of the Oxygenation of the Blood in the Lungs and Placenta" and "Of Generation" develop his theories about human reproduction, including observations related to evolution. The final chapter in the first volume is a reprint of a paper by another of Erasmus Darwin's sons, [Robert Darwin](https://en.wikipedia.org/wiki/Robert\_Darwin), about ocular spectra.

The second volume is focused solely on classifying diseases into classes, orders, and genera. The book is divided into four major sections, based on the four classes of disease: diseases of irritation, sensation, volition, and association.  

In comparison to all the other texts, the appeal of the Vicar of Wakefield is obvious: it’s funny on purpose, while also, I remain convinced, being completely in earnest about its moral values. An illustrative joke is the one about making sure to lend something to one’s least pleasant houseguests, so they won’t ever return: it’s a bit wry or cynical as a punchline, but in no way undermines the house’s hospitality, or the idea of hospitality itself as a virtue.



The novel was first published in 1766; this 1792 edition comes 26 years after its first publication, and 18 years after Goldsmith’s death in 1774. Not sure how many printings have elapsed since 1766.



It opens with an index of chapter summaries, and contains absolutely no prefatory materials --- which is, frankly, a real oddity! A sign it was a well-know book?  

T194212 is a 24-page “letter” which the ESTC lists as being on the subject of “Trinity,” and has the same contents as T212697 which the ESTC says was “Issued in the same year as another edition, which had the titlepage reset to include the author statement.” T212697 is unsigned, T194212 attributed to William Jones, and both linked in the ESTC to the author “Jones, William, 1726-1800.”  

This is an unsigned single-sheet document. I know far more about the piece of paper than the contents: the ESTC describes it as 1 sheet ;  1/2⁰ with dimensions of 34.6 x 20.8cm and horizontal chain lines. There is also an uncontrolled note saying that the record is based on “Lpro copy with other material from 1792”

The shelfmark, HO.42/23 \[632\], points in the London National Archives to “HO 42. Letters and papers. Piece described at item level.” This consists of a whole lot of correspondence sent to the Home Office, which has actually been scanned! Things are stamped with numbers in the upper-right.... And OH MY GOD 632 SHOWS A SCANNED COPY OF THE ACTUAL DOCUMENT, FREE TO THE PUBLIC AND RIGHT HERE, I CANNOT BELIEVE IT!!!! Image 940 shows the front, and image 941 shows the back. The item has its own record at http://discovery.nationalarchives.gov.uk/details/r/C10614791

To find this image, I had to check the “HOLDINGS DETAILS” pane of the ESTC record (a field which somehow I only recently discovered), search the shelfmark at the  London National Archives, and guess that \[632\] referred not to a page or image number, but to an item.

“Lpro” probably means London/local Public Records Office (former name for National Archives), per Cai  

Connecting Darwin to Goodall: Darwin was “one of the leading members of the celebrated Lunar Society of Birmingham” (Poynter 112) --- what does that mean?  

One of the major findings of this research methodology is a reminder that digital records are based on a relentless materiality. The method itself, in some ways, boils down to predicting the location of some barriers to research, and then smashing my face directly against those barriers in order to characterize their location, shape, and solidity. The hunt for the more-neglected titles hit very similar barriers. The Hayward advertisement has only one recorded copy, in Germany; elation when looking up the shelfmark reveals that it is bound with the Gentleman’s Magazine and I can find a HathiTrust scan of that exact volume; growing dread as a detailed examination of the 800-page facsimile makes it clear that the advertisement did not appear within the magazine but was bound with it separately by an idiosyncratic German person; resignation as I use Google Translate to compose an apologetic message in German for the Niedersachsische Staats und Universitatsbibliothek, where someone will have to go dig out the book and personally inspect it for the inclusion. Or, for the East India accounts --- growing elation as I realise that one of the holding libraries is UCLA, where material conditions are in my favour, since I have a friend studying there; hopes that are dashed by the absolute bar on special collections access during COVID-19.  

Underwood has particularly emphasized that the unique role of literary studies, as opposed to history or sociology, is that narratives provide pleasure. Of the ten items I attempted to consult, Erasmus Darwin’s *Zoonomia* is on paper one of the easiest to work with: a readily available facsimile in ECCO, and a substantial body of secondary work to consult. However, the thing about a 1,200 page loosely-organized medical text is that it is entirely lacking in the pleasure of narrative. Even to lightly skim each page took a substantial investment of time and willpower. Trying to fully understand the ideas proposed --- and especially to reconcile them with what I came to realise was a very haphazard and largely forgotten high school education biology --- was a challenge. In other words, the text itself can be a barrier.



A review of *The Essential Writings of Erasmus Darwin* explicitly describes the barrier to study posed by *Zoonomia*’s length: “Anybody who has tried to read Erasmus Darwin’s *Zoonomia* or his *Botanic Garden* will be familiar with the *longeurs* of those extraordinary compositions and be prepare to turn with relief to the brief selections of important passages which are offered here.” (Poynter 111) The value of the abridgement is that it can “introduce more readers to one of the more colourful characters in English scientific history than have ever been gained by the original texts.” (Poynter 112)  

This dissertation has been centrally concerned with textual selection. In this final chapter, I wish to experiment with a perverse and speculative form of textual selection: true randomness. The value of speculative computing, as a methodology in a liminal space between the humanities and computer science, lies in its ability to be useless in a thought-provoking way. Of course one would not seriously suggest that we study literary history through texts chosen at random... but what would happen if we did it anyway? To write this chapter, I used a random number generator to choose \[X\] texts from the ESTC. \[LITERARY BLUF - last thing to write\]  

In earlier chapters, especially chapter 1, the 1790s provided a backdrop to explore my real subject of contemporary literary databases. In this chapter, however, my real subject is 1790s print culture in England; I am curious about how this subject might be illuminated differently through my unusual sampling method (in the vein of speculative computing) but several of my sampling principles are defined around the goal of actually acquiring sufficient titles to answer my questions. First, for example, I decided to work with my “post-cleaning” sample, to exclude non-English or non-1790s works. Although it is interesting to note the “false positives” that appear, their presence in the sample would limit my ability to comment meaningfully on my actual subject, 1790s print in England. I also decided that I would re-sample from ECCO instead of the ESTC if I could not find a way to personally read at last five of the works in the ESTC sample.

I preferred to select my texts from the ESTC because it is the largest/broadest corpus from which I can sample. This would allow me to perform another case study “spot check” like the exploration of Charlotte Smith’s works in chapter 1: of the works I read from the ESTC, which are also available in other resources, and why? Sampling from ECCO instead would eliminate the possibility of finding works present in, eg, HathiTrust but *not* ECCO, and reduces the ability of this experiment to illuminate the database ecosystem. Nonetheless, I prefer in this chapter to limit my observations about databases in order to increase my ability to discuss the 1790s in England.

I considered balancing my sample chronologically, e.g., taking one works from each year 1789-99. Doing so would reduce the chance that my random sample would overrepresent more productive years. However, I decided that such chronological overrepresentation would be exactly the kind of phenomenon I’d like to discover.  

To take my sample, I used my “ESTC records - cleaned” spreadsheet from OpenRefine, which included 51,090 rows of data. I installed the “GOKb Utilities extension” for OpenRefine (https://github.com/ostephens/refine-gokbutils). I created a column called “Random number,” using the function randomNumber(0, 51090) (per https://stackoverflow.com/questions/46063173/how-to-make-a-random-sample-in-openrefine). However, when I sorted the data numerically, I saw that this did not give them all unique numbers (the first 10 went 1, 2, 3, 3, 5, 6, 6, 7, 7, 8) so this was not what I needed.

So then I created a new column called “Row number,” using the GREL formula “row.index + 1” (which, despite the +1, filled the column with the exact row number that OpenRefine had assigned).  First I decided to reorder the rows permanently by the “Random number” column; even though that method hadn’t given each row a random unique identifier, it did sort them more randomly than any order sorting method could have accomplished. I spot-checked the new “Row number” column and it appeared to go appropriately from 1 to 51090.

Then it was time to choose 10 random numbers between 1 and 51090 (inclusive). I used the random.org Random Integer Set Generator to generate 1 set with 10 unique random integers, with each integer between 1 and 51090 (inclusive). This generated the following set: 3585, 6427, 11770, 13646, 15284, 15442, 21963, 37041, 44564, 48755. I then looked up each of those row numbers in OpenRefine to find my titles.  

**3585**

Monograph

T219851

unsigned

The excellent old ballad of the babes in the wood : or the Norfolk gentleman's last will and testament

Children in the wood; or the Norfolk gentleman's last will and testament

England ; Liverpool

1794

Availability: may not be able to find this exact edition, but ECCO has many other eds. Only physical copy is in Manchester





**6427**

Monograph

T194212

Jones, William, 1726-1800 \[person\]

A letter to three converted Jews : lately baptized and confirmed in the Church of England. By ... William Jones

England ; London

F & C. Rivington

1799

Availability: not readily available, but some related works may be discoverable. Only physical copy is in Durham, UK



**11770**

Monograph

T82736

unsigned

Satirical, humourous, and familiar pieces. Prose

England ; Ludlow

Champante & Whitrow, London ; George Nicholson, Ludlow. Sold by T. Knott

1799

Availability: ECCO link in ESTC. Physical copies at British Library and Oxford.



**13646**

Monograph

N38682

Great Britain, Army, London and Westminster Light Horse Volunteers \[organisation\]

Rules and regulations of the corps of Light-Horse Volunteers of London and Westminster; carefully extracted from the minutes of the general meetings

England ; London

1797

Availability: not linked in ESTC, but appears to be available in ECCO as ESTC T131622 --- another edition in a different size. Physical copy in the London National Archives.



**15284**

Monograph

T161475

Hayman, John \[person\]

Consolation for the scorbutic, scrofulous, leprous, &c : and worthy also the serious perusal of the medical profession. Being an abstract from a few of those remarkable cures which illustrate Mr. Hayman's 'Treatise, explaining the nature and affinity of scorbutic diseases.' Golden-Square, London, Ist October, 1793

England ; London

1793

Availability: does not appear to be available. Only physical copy in Gottingen, Germany.



**15442**

Monograph

T111867

East India Company \[organisation\] ; Great Britain, Parliament, House of Commons \[organisation\]

An account of the prime cost of all cargoes purchased in India, and shipped for Europe, in the year 1793-4; together with the commercial charges at each presidency, not added to the invoice

Proceedings. 1795-04-17

England ; London

1795

Availability: May not be directly available, but some info in minutes digitized in Hathi, https://babel.hathitrust.org/cgi/pt?id=mdp.39015056730925&view=1up&seq=448



**21963**

Monograph

T146200

Goldsmith, Oliver, approximately 1730-1774 \[person\]

The vicar of Wakefield : a tale, by Oliver Goldsmith

England ; London

Ogilvy & Speare

1792

Availability: ESTC links to ECCO



**37041**

Monograph

T202021

Goodall, Susannah \[person\]

Extraordinary appearance of the moon! : Which was perceived to be in a violent rocking motion, for several minutes; after which was seen clearly passing round the orb, immense armies of horse and foot with bloody streamers flying, to the great terror and astonishment of thousand of spectators, who were witnesses of this wonderful alarming omen! to which is added, calculations, judicial and astrological observations, by which the true events signified thereby are foretold. Susannah Goodall, pupil to the celebrated Don Farnando Furioso. Doctor of divinity, physic, and astrology. Who foretold all the late wonderful events and bloody battles which came to pass at Toulton, Dunkirk, and various parts of France and Flanders

England ; London

1794

Availability: ESTC links to ECCO



**44564**

Monograph

T113901

Darwin, Erasmus, 1731-1802 \[person\]

Zoonomia : or, the laws of organic life. ... . By Erasmus Darwin, M.D. F.R.S. Author of the Botanic Garden

England ; London

J. Johnson ; St. Paul's Church-Yard

1794

Availability: ESTC links to ECCO & Google Books; also held at U of T



**48755**

Monograph

N40981

unsigned

The principles of modern reformers exposed

England ; Sheffield

J. Crome

1792

Availability: can’t find  

Mostly the rules are about paying appropriate fees (there is a blank for the amount to be written in by hand!) and the logistics of horses, training, and uniforms. There is a lot of training available and drills required, including a lot of surveillance/record-keeping of who is in town to be called to serve.

Emphasis on the operations of particular stables.



“The Horses must be bay, brown, chestnut, or black, and not under fifteen Hands high; Stallions are not admitted, and Mares only allowed till a convenient Opportunity offers of exchanging them.” (7)



“It is considered to be inconsistent with our Institution to contribute, *in the Name of the Corps*, to any public or private Subscription, every Gentleman having an Opportunity of contributing in his individual Capacity according to his Inclination and Abilities.” (29)



There are also 150 “Dismounted Cavalry”! Who carry carbines or rifles with bayonets and are “freed from the Expence of Horse Keeping.” (34) --- but can keep their horses in the LHV stables, and get first refusal when there are openings for more mounted volunteers. Suggests a strong ‘social club’ element?



The version turned up by the random sample was 24p. ;  12⁰. The version in ECCO is 36p. ;  18⁰.   

se  

Listed as “author of The Botanic Garden”



Opens with a poem (viii)

![][ScreenShot2020-12-08at1.40.54AM]



He is trying to explain disease itself, and also sort of teach critical reading to counter fake news? (2)

![][ScreenShot2020-12-08at1.42.30AM]



Then it’s practically modern art!!!

![][ScreenShot2020-12-08at1.44.47AM]



What a way to “define your terms” (7)

![][ScreenShot2020-12-08at1.47.18AM]





P 28 is the culmination of a long section about where ideas come from (references to Locke) --- rebuts a counter argument that sensations occur in the brain rather than the organs of sense, reaffirms that all ideas have their origin in sensation



Erasmus has a much more vivid imagination than I do:

![][ScreenShot2020-12-20at4.35.21PM]

P 50 pine-apple

![][ScreenShot2020-12-20at4.37.10PM]

P 51 guilty wretches



Darwin on the cultivation and uses of ideas:

![][ScreenShot2020-12-20at4.44.12PM]



P 62

![][ScreenShot2020-12-27at2.41.37AM]

P 92-3

![][ScreenShot2020-12-27at8.06.45PM]![][ScreenShot2020-12-27at8.06.33PM]





P 111

![][ScreenShot2020-12-27at9.54.05PM]



P 114: things might exist without being solid, but they still exist in space

![][ScreenShot2020-12-30at2.57.30AM]



P 132 - what lady is he talking about?

![][ScreenShot2020-12-31at2.36.37AM]



P. 133 identity

![][ScreenShot2020-12-31at2.39.58AM]



P 143 elephant stories

![][ScreenShot2020-12-31at2.46.27AM]



P 157 knife on china --- a experience that I still feel viscerally

![][ScreenShot2021-01-01at3.35.54AM]



P 189 bowels

![][ScreenShot2021-01-01at8.58.52PM]



P 186 comparison of movement to reading poetry

![][ScreenShot2021-01-01at8.55.50PM]

P 192 explanation for his claim on p 186

![][ScreenShot2021-01-01at9.03.09PM]



P 200

We have dreams so we don’t get brain-fever when we wake up

![][ScreenShot2021-01-03at1.33.01AM]



P 201-202 a totally bananas experiment about the accumulation of sensorial power in the eyes



![][ScreenShot2021-01-03at1.35.28AM]![][ScreenShot2021-01-03at1.35.54AM]



P 206 - 207: reverie makes us lose track of time, that’s why Shakespeare is good without theatrical unities

![][ScreenShot2021-01-03at1.37.34AM]![][ScreenShot2021-01-03at1.37.42AM]



P 217 - 218 --- oxygene air and sleep

![][ScreenShot2021-01-04at1.18.43AM]



P 231-232 seasickness

![][ScreenShot2021-01-04at1.21.19AM]![][ScreenShot2021-01-04at1.21.25AM]



P 240 drunkenness caused by tricking your stomach into thinking you’re full

![][ScreenShot2021-01-04at1.28.07AM]



P 250 on indolent people whistling because they’re not thinking:

![][ScreenShot2021-01-13at7.59.25PM]



P 251: repetition is why we like certain poetic features --- he has a lot more to say about repetition in the arts

![][ScreenShot2021-01-16at6.22.28PM]



P 259: a succinct summary of his theory of nerves / circulation

![][ScreenShot2021-01-16at8.35.01PM]



P 276 a person who can spit up berries on command: maybe a sleight of hand trick?

![][ScreenShot2021-01-16at8.42.18PM]



P 279 powerfully bad diarrhoea as evidence

![][ScreenShot2021-01-16at8.44.05PM]



P. 290-291 two cases where they use venesections and bleeding as a treatment for haemorrhage



P. 302 he includes his dead son’s dissertation

![][ScreenShot2021-01-16at9.05.10PM]

Charles describes a lot of experiments and cites a lot of other people

P. 309-310 experiments with eating asparagus and then peeing & bleeding



P 346 the end of Charles’s dissertation

![][ScreenShot2021-01-17at5.08.36PM]



P 351 ancient goose liver recipes

![][ScreenShot2021-01-17at5.26.36PM]







P 355 Erasmus doesn’t think dark eyes are beautiful, they are a debility

![][ScreenShot2021-01-17at5.37.06PM]



P. 356 to --- explaining why Indigenous peoples die under the lash instead of being made to work; also small Asian hands

![][ScreenShot2021-01-17at5.38.49PM]



P 357-8 too much sensitivity

![][ScreenShot2021-01-17at5.41.17PM]



Describes some Indigenous people discovered by the Spaniards who are of this temperament



P 358-9 very brief description of the temperament of increased voluntarily--- associated with the king of Sweden, and tied to the qualities that distinguish men from brutes



P 360 - circumstances can cause changes in temperament

![][ScreenShot2021-01-17at5.44.19PM]



P 375-376 earth might look like saturn from afar![][ScreenShot2021-01-17at6.00.01PM]



P 397-399 --- theorizing about contagious diseases and why we only get smallpox once



P 416 defining voluntary vs involuntary



P 426: pleasure of ideas

![][ScreenShot2021-01-24at7.21.33PM]



P 433 - references a french philosopher who takes for granted ennui as the default condition of life --- tied to the 18thC consensus that suicide would be nice?

![][ScreenShot2021-01-24at7.33.30PM]



P 439 a woman with two minds --- described in the index as a woman with two sould

![][ScreenShot2021-01-24at7.36.34PM]



P 456 - he is totally wrong about the most fertile part of the menstrual cycle

![][ScreenShot2021-01-24at7.46.36PM]



P 457 - called out for being a night owl

![][ScreenShot2021-01-24at7.47.55PM]



P 467 - trying to explain why we don’t live forever

![][ScreenShot2021-01-24at7.51.00PM]



P 534 - another inclusion of somebody else’s work

![][ScreenShot2021-01-28at1.13.39AM]



P 536 very detailed instructrions for cool ocular illusions

... How is any of this stuff different from the first time he talked about ocular illusions?



P 559 really insightful and detailed observation that afterimages color mix differently than sunlight (eg like ‘multiply’)



P 567 begins a section of “additions”, which includes many pages on vertigo (suggesting revisions after printing began)



P 577 is an index!!



Then p 587 has directions for the binder!! And errata, which are different from additions --- only 3 typos



Ends with a notice that he is putting off the second volume because he is busy and has other stuff to do

![][ScreenShot2021-01-28at2.26.55AM]

  

P 144-145

![][ScreenShot2020-12-31at2.47.39AM]



P 145-146

![][ScreenShot2020-12-31at2.48.57AM]

![][ScreenShot2020-12-31at2.49.14AM]



  

P 484 and 485 - acknowledges a counterargument to his (largely correct-seeming) theory of generation, namely, that if each sperm is generative, nature has come up with a wasteful design--- but this is the same as the large number of seeds that trees make / eggs that fish lay.



P 489 - goes hard on the idea that only males produce embryos and females just sustain them (reiterated p 528-9):

![][ScreenShot2021-01-24at8.20.29PM]



P 500 you can change the glands to interfere with gender

![][ScreenShot2021-01-25at1.03.31AM]



P 503- some hints at natural selection, though in many ways otherwise he is lamarckian

![][ScreenShot2021-01-25at1.19.56AM]

P 505

![][ScreenShot2021-01-25at1.26.13AM]



P 508 hermaphroditism and the creation of human sexes

![][ScreenShot2021-01-25at1.29.16AM]



P 519 if snails and worms could impregnate themselves they wouldn’t be able to evolve



P 520 - 521 how babies are conceived of different sexes (with good “examples” on p524) --- it’s easy to imagine women with dicks!

![][ScreenShot2021-01-28at1.04.56AM]

![][ScreenShot2021-01-28at1.05.17AM]



P 522 dismissing women’s role in creating children

![][ScreenShot2021-01-28at1.07.32AM]  

P 106 - reference to Darwin’s bodily experience as he writes (holding a flower)

![][ScreenShot2020-12-27at9.39.56PM]





P 148 gives the date of “this morning”

![][ScreenShot2020-12-31at2.52.20AM]





P 159 personal anecdote about a bird he doesn’t like

![][ScreenShot2021-01-01at3.37.26AM]



P 191 draft circulated in manuscript --- what actress is this talking about?

![][ScreenShot2021-01-01at9.01.06PM]



P. 402 verbatim notes from his cases of Miss L and Miss H I Feb 1791 (boring except that they track details over time and suggest having been copied directly from elsewhere); p 404 notes from March 1793  

P 352-353 the story of prometheus!!

![][ScreenShot2021-01-17at5.28.00PM]

![][ScreenShot2021-01-17at5.31.00PM]



He lists the story of Prometheus in the index, which seems pointed (p 583)  

P 57

![][Hu4qt]



P 58

![][ScreenShot2020-12-27at2.33.01AM]



P 58

![][ScreenShot2020-12-27at2.33.35AM]

P 59

![][ScreenShot2020-12-27at2.33.51AM]

![][ScreenShot2020-12-27at2.37.28AM]



P 59 definition of sensibility:![][ScreenShot2020-12-27at2.38.21AM]  

P 103: sensibility in plants

![][ScreenShot2020-12-27at8.35.42PM]



P 105

![][ScreenShot2020-12-27at9.38.54PM]



P 107

![][ScreenShot2020-12-27at9.42.05PM]



  

P 184 it is planning that defines the human

![][ScreenShot2021-01-01at8.52.55PM]



P 169: animal contracts

![][ScreenShot2021-01-01at3.44.43AM]



P 183 on the reasoning of insects

![][ScreenShot2021-01-01at8.50.51PM]  

P 60

![][ScreenShot2020-12-27at2.39.58AM]  

ECCO’s physical description doesn’t mention pagination errors, just: "2v.,plates ; 4°.” --- Same as ESTC’s description --- **Are these errors in the printed book, or in the microfilm?**



Pages 18 and 19 both appear twice! It goes 17 18 19 18 19 20

Another error --- it goes 61 26 63 26 63 64 --- looks like microfilm error? (63s are identical)



![][ScreenShot2020-12-27at2.49.10AM]![][ScreenShot2020-12-27at2.48.53AM]![][ScreenShot2020-12-27at2.49.16AM]![][ScreenShot2020-12-27at2.49.05AM]



  

P 81

![][ScreenShot2020-12-27at7.50.54PM]  

P 95

![][ScreenShot2020-12-27at8.11.55PM]



P 96

![][ScreenShot2020-12-27at8.19.58PM]



P 440 “good news, affecting stories or agreeable passions” offered as cures  

P 514-515 compares mulattos to mules

![][ScreenShot2021-01-26at12.50.20AM]



P 569 “addition”

![][ScreenShot2021-01-28at2.21.02AM]  

P v -- striking his own path in classification

![][ScreenShot2021-01-28at2.45.49AM]

Divides diseases into species and genera based on proximate casues and effects in the sensorium



P 23 - some comments on scorbutic and scrofulus diseases caused by eating too much salt meat

![][ScreenShot2021-01-28at3.29.06AM]



P 29 - treat colds with warm water, enemas, and opium

![][ScreenShot2021-01-29at7.06.54PM]



P 32 arguing against the idea that sweat is an excrement and thereby relevant to medical treatment/causes of disease



P 45 advertises for a particular product

![][ScreenShot2021-01-29at7.43.04PM]



P 46 makes me think about how much I like it that each disease is followed with research questions for “future work”



P 61-2 lists “surprise” as a disease!



P 71 mentions an alarum clock



P 73 - frequent miscarriages, cured by opium



P 74 - one of the only things not cured by opium is scurvy

![][ScreenShot2021-01-29at8.14.05PM]



P 80-81: pimpled! They may be pressed out by the finger-nails, or treated with warm water or ether. “Blister on the part?”



P 82 avoid gray hair by avoiding old age, though warm baths

![][ScreenShot2021-01-29at8.19.59PM]



Between pages 88 ad 89 --- in this one the plates have been inserted in the right place



P 87 begins a long discussion of cures for scoliosis, which he identifies as due partly to a softness of the bones, and focuses on ways of keeping the spine physically straight during the day--- including a comment that chairs at boarding schools need backs or desks for girls to lean on



P 91 comments on beauty of the figure as illustrated in other images

![][ScreenShot2021-01-29at9.44.33PM]  

P xi preface ends with a plea to readers, dated “Derby, Jan 1 1796”

![][ScreenShot2021-01-28at2.49.22AM]  

![][ScreenShot2020-11-20at11.58.31PM]  

| Location | Bodleian Library Harding A 127 (57) |
| :----- | :----- |
| Title | Extraordinary appearance of the moon! : Which was perceived to be in a violent rocking motion, for several minutes; after which was seen clearly passing round the orb, immense armies of horse and foot, with bloody streamers flying, ... / Susannah Goodall, pupil to the celebrated Don Farnando Furioso. ... Who foretold all the late wonderful events and bloody battles which came to pass at Toulon, Dunkirk, and various parts of France and Flanders |
| Imprint | London?\] \[s.n., \[1794?\] |
| Description | 8 p. : 1 ill. ; 12⁰ |
| Binding | 19th-century half goat. Patterned cloth boards. By Henry Sotheran & Co., Piccadilly |
| Provenance name | Sotheran, Henry |
| Provenance note | Binder’s label |
| Size | 19 cm |
| Bound with | With 87 other chapbooks. Binder’s title: Trials |



# conclusion (5k) #



# Raw Writing #  

“One implication of the publication history of short-title catalogues is that they have been deemed functional and valuable even before they were complete. (That estimation is crucial, for their full completion is for all practical purposes impossible.) Judging that even a preliminary form of the records was useful to scholars, the planners of ESTC determined to conduct its development ‘in full public view’ and to make the incomplete file available ‘warts and all’ (in the words of Henry Snyder and Michael Crump, responding to criticism by Peter Blayney)” (Vander Meulen 270).  

Increasingly, projects do in fact actively theorize and address their datasets.



Mark Vareschi and Mattie Burkert, for example, in “Archives, Numbers, Meaning: The Eighteenth-Century Playbill at Scale,” have an argument to make about eighteenth century theatrical genres, but also “theorizing a new dataset of our own creation as a *description* that mediates and transforms our relationship to the objects it describes” (598). ”Our study shows the need to attend to the unstable ontologies present in cultural objects, as well as in the datasets that describe them.” (613)  

The meaning of “popular” shifts depending on what is conceived of as its “opposite.” Is it an aesthetic category, popular vs highbrow, Gothic novel vs poetry? Is is a sociological/economic term, popular vs elite, ballads and chapbooks vs books? Is it tied to reception, popular vs unpopular, Udolpho vs The Two Emilys? Or tied to legacy, popular vs forgotten, Lyrical Ballads vs The Lemon? I am interested in all of these varying definitions of popularity because they are all tied to what *actually* interests me, an attempt to define and characterize “normal.”

Literary study is typically interested in outliers, because they are, well, *interesting*. I’m interested in the “normal” because I think it is a necessary context for any claims or analysis about that which is exceptional. For example, it’s not appropriate to read too much into the fact that Radcliffe published Athlin & Dunbayne unsigned, since this was “normal” --- a default position, rather than an informative expression of intentionality. So, one kind of “normal” that interests me is this idea of the default: what might we expect? The default normal can be defined at multiple levels of granularity. We are handed a two-volume work, what might we expect from it at default? We are looking at a poem written by a man in 1792, what might we expect from it at default? Making these naive default guesses will be tied to another meaning of “normal,” that of “most prevalent.” From this perspective “normal” serves more as a synonym of “common.” What do we find the most of? Again, this can be defined at multiple levels of granularity. (Is most-ness a trait of normality, or of exceptionality....?) Can we better characterize the background against which outlier works were written and read?

What is a "normal" footprint in the print culture of this decade? (i.e., what are the boundaries a work has to surpass to be unusually popular or unusually unpopular?)

In both cases, it is difficult to describe “normal for an eighteenth century audience” through the mediating lenses of our databases. One key question, in fact, is “is there anything that seems normal in these databases which would not seem normal to the eighteenth century public?” I anticipate denaturalizing some aspects of these databases, e.g., the way that the flatten distinctions between different buying publics (social classes, time periods, tastes).

I’m also interested in “disreputable” literatures, which have often been some versions of popular or some versions of normal --- this is an essentially ethical attachment, or perhaps an inherent contrariness; any time a work is dismissed for any reason, I wish to question the importance of the evaluation metrics which rejected it, and seek whatever may be of value in the rejected work. This is one way a reparative impulse underlies the project: so what is these things were common, or cheap, or bad --- that doesn’t mean they *have* to be worthless. The arbitrariness of the literary canon is that essentially any artistic product can be imbued with value by its audience. (Guillory has a lot on this.) I want to explore how we can value that which has not been valued.  

“But however essential it is for data-rich literary history, modeling cannot be the sole foundation for the field. **Models of literary systems are not simply arguments** about the existence of and connections between literary works in the past; **they are arguments made with reference to the disciplinary infrastructure**---the bibliographies and collections, analog and digital---that transmit evidence of past works and relationships to the present. Modeling, even when integrated with descriptive bibliography as I have described, does not reflect on this transmission. Models embody a scholar’s arguments, whereas disciplinary infrastructure is an effect of multiple arguments: a sequence of assumptions, decisions, representations, and remediations. Such histories of transmission shape how the researcher can explore, and what she can know of, the historical context that disciplinary infrastructure appears to represent. To adequately perform literary history, data-rich projects must investigate these histories of transmission and how they constitute the documentary record.” (Bode 42)



“The distinctions or gaps between the context signified by collections and the exemplars used in signification **might partly arise from, but are not simply the consequence of, successive exclusions of documents**, as the Stanford Literary Lab Pamphlet 11 suggests. In chapter 1 I noted that, in defining “the published,” “the archive,” and “the corpus” as progressively smaller selections, those authors admit the constructed nature Page 44 →of literary data. Yet they also argue that mass-digitization largely avoids those exclusions, such that “the corpus of a project can now easily be (almost) as large as the archive, while the archive is itself becoming---at least for modern times (almost) as large as all of published literature” (Algee-Hewitt et al., 2).

Even with their account of the considerable practical challenges involved in accessing versions of specific literary works, this description of mass digitization drastically diminishes the degree of exclusion involved in constructing such collections. ” (Bode 43-4)



“Digitized collections are partial in another way, in that combining the holdings of multiple analog collections tends to obscure the individual histories of the contributing collections and their implications for the form, scope, and critical capacity of the resulting digital one.” (Bode 44)



“Collections have always been constituted in this way. In analog collections, documents are represented and remediated through the cataloging systems that organize holdings and the interfaces that interpret them: the card catalogs, special collection indexes, or online library catalogs that provide a method of searching and a type, form, and detail of metadata. ... A key difference between analog collections and digital ones is that literary historians rarely, if ever, treat the former (a given library, for instance) as proxies for literature as it circulated and was understood in the past, whereas digital collections such as Google Books or HathiTrust are sometimes assumed to be representative in this way.” (Bode 44)



“Due to their multiplicity and complex interaction, the components involved in producing digital collections expand access to the historical record in certain ways, even as they increase the likelihood of unrealized and significant disjunctions between the access we intend and the access we achieve. **Digital humanities scholars recognize that digital infrastructure shapes knowledge production, and digital literary historians have responded with explicitly curatorial approaches** to constructing and exploring digital documents and collections.” (Bode 45)



“These digital humanities projects highlight **four features** that I believe should also **underpin the modeling of literary systems in data-rich literary history**. First is a critical assessment of the **relationship between the historical context analyzed and the digital collection(s) used** for analysis; second is detailed attention to the **relationship between the documents included in the digital collection(s) and the terms in which they are represented**; third is explicit discussion of the **means by which data are extracted and modeled**; and fourth is a **published record of data** arising from that extensive history of transmission. Existing projects in data-rich literary history often (though by no means universally) demonstrate the second and third of these features. The need for data publication, and for platforms and modes of review to support it, is also increasingly recognized and enacted.7 But data-rich literary history projects rarely consider how the disciplinary infrastructure analyzed relates to the historical context investigated. The lack of shared standards for data publication---and, more specifically, of a framework for combining these four features in investigating and representing the transmission and transformation of historical evidence to and in the present---problematizes the field’s capacity to advance historical knowledge.” (Bode 46)



“Proprietary mass-digitized collections such as Google Books, Early English Books Online, and The British Newspaper Archive (owned by Google, ProQuest, and findmypast, respectively) are increasingly used in humanities research. But their scope and scale---let alone the histories of transmission that produce them---can be very difficult to discern; indeed, **the commercial imperatives of these enterprises arguably depend on them presenting these collections as comprehensive**.” (Bode 47)  

One of Underwood’s contentions is that “we understand \[literary\] history, locally, very well,” (31) and the role of computational study is to examine the large-scale trends which would join together the individual moments of literary history. This dissertation, however, attempts to study a very local moment with a larger body of evidence than is usually feasible to examine.  

The problem of textual selection--- the paired difficulty and importance of deciding which few books one will actually read--- is an urgently meaningful problem in the “real world,” outside the realms of academia. A common solution is one which will likely alarm and distress most scholars who have dedicated themselves to thinking through canonical selection, or even the construction of a syllabus: a solution which could perhaps be called ‘radical impatience.’ Consider, for example, the following advice in a blog post, “100 Ways to Live Better,” item number 6, the first entry in the category “Mind”:

There are more great podcasts than you’ll ever have the time to listen to. If it sucks after 10 minutes, skip half an hour ahead. Still boring? Delete and move on. Obviously, do the same for books.

There is much for a literary scholar to find distressing here. The operant metric that another’s ideas “suck” if they are “boring” in the first ten or thirty minutes. The fact that “obviously” the same metrics apply for books as for podcasts. Indeed, even the fact that podcasts so strongly come *before* books as the default way to encounter new ideas. On the website, each word in the first sentence provides a link to a specific podcast recommended by the writer, indicating a proliferation of individual desirable podcasts, but “books” are a monolith and a footnote.



Later we see another metric for textual selection:

Should you watch that movie / play that game / read that book? Use the ratio:

(\[# who rated it 5/5\] + \[# who rated it 1/5\]) / \[# who rated it 3/5\].

This doesn’t apply to everything, but it applies to many things, including media. There are too many options out there to waste time on mediocrity, and everything great will be divisive.

Paired with the first piece of advice, this would suggest a process of seeking out divisive and controversial works, determining within ten minutes whether one is strongly in sympathy or strongly opposed, and continuing only with works which evoke strong sympathy.

Of the seven mentions of the word “book,” two are in fact the word “Facebook.” Two are the pieces of advice quoted above. Two are suggestions to find “good audiobooks, and/or a dog” to form a habit of going on walks, or “a good app or guidebook” to practice meditation. And the last is the advice that “If you’ve been waiting for months for someone to create an event and invite you, whether it’s a book discussion or a BDSM orgy, just throw one yourself,” in which “a book discussion” functions rhetorically as an extreme example of an unusual and improbable kind of social event, paired against its assumed opposite extreme.  

“Of the data-rich literary history projects discussed in this chapter, Underwood and Sellers’s work on changing standards of literary prestigePage 51 → most consistently enacts the curatorial elements present elsewhere in digital humanities, not least in terms of data publication. As the authors explain in an online working paper on the project, its most time-consuming element was not training their supervised model but constructing their dataset: identifying the different subgenres---poetry, prose, fiction, and drama---present in HathiTrust (“How,” 6). As well as publishing the datasets and code used in their article (“Code”), in collaboration with HathiTrust Underwood takes the major, additional step of releasing the outcome of analysis of that mass-digitized collection for others to use. This takes the form of “word counts for 101,948 volumes of fiction, 58,724 volumes of poetry, and 17,709 volumes of drama” published from 1700 to 1922, as well as yearly summaries of word frequencies for each genre. Underwood refers to this dataset as a “collection” to differentiate it from a “corpus” because “I don’t necessarily recommend that you use the whole thing, as is. *The whole thing* may or may not represent the sample you need for your research question” (“Dataset” np).

Signifying growing recognition of the importance of data publication, the Underwood/HathiTrust collection is an important undertaking for data-rich literary history in at least two ways. In presenting a dataset designed for literary history, it offers a shared foundation for research. Working with it, researchers can ask a range of questions based on a reliable, standardized dataset and engage with each other’s arguments in terms not only of results produced but of data investigated. In characterizing that collection as the holdings of “American university and public libraries, insofar as they were digitized in the year 2012 (when the project began), Underwood also frames a major mass-digitized collection---HathiTrust---in terms of its history of transmission (“Dataset” np). However general this framing, Underwood thus explicitly associates the dataset he publishes with a sequence of production and reception that profoundly affects its capacity to support historical analysis. In their article, Underwood and Sellers acknowledge that this history of transmission shapes their findings, noting that their model “makes more accurate” predictions for American poetry collections because HathiTrust “mainly aggregates the collections of large American libraries” (“Longue” 338).” (Bode 50-51)



“Seeking to define the scope of their dataset, Underwood and Sellers note that HathiTrust “may represent more than half of the titles that were printed” because it contains “about 58% of titles recorded in standard bibliographies.” Yet their own “work on fiction” with this collection belies the apparently solid basis of this estimate, finding that HathiTrust contains “many titles left out of” existing bibliographies (“How” n11). Underwood and Sellers thus indicate a significant lack of overlap between established bibliographical records and the holdings of a major digital library, although they do not highlight the significance of this finding nor explore its implications for their own study or for literary history broadly (whether conducted by computational or noncomputational means). Emphasizing that we cannot know the documentary past except through the knowledge infrastructure we create to interpret it, this disjunction that Underwood and Sellers discover highlights the potentially major gaps in all existing forms of description and interpretation: **neither the analog nor the digital record offers an unmediated and comprehensive view of the documentary past; both are partial, and not necessarily in complementary ways.**” (Bode 53)  

Echoing Underwood’s provocative question, “do we understand the outlines of literary history?” (the title of chapter one of *Distant Horizons*), I ask: do we know what was common in the 1790s? Catherine Fleming, for example, says “There is no consensus about what percentage of the books sold in England during the long eighteenth century were translations” (10), though estimates of 15 to 35 percent indicate that their presence was substantial.  

“ProQuest is to be commended for its attitude to the wider scholarly community.27 EEBO is a commercial product but nonetheless there is an encouraging and genuine wish to engage with its users. This ranges from the active monitoring and rapid responding to queries submitted via its ‘Webmaster’ form to informal and formal consultations with students, scholars, and other users. EEBO representatives appear at -- and often sponsor -- academic events. Content is frequently corrected, updated, expanded and enhanced (such as the new ‘EEBO Introduction Series’); the searching mechanism continues to be improved; the project to produce full-text transcriptions (the Text Creation Partnership) is an academic venture, not a commercial one. Unlike Jackson’s microfilm photographer, lurking in his lair with his livid lights and chemical smells, the present providers of EEBO seem to be rather more interested in -- and responsive to -- contemporary scholarship.” (Gadd 688)  

“It is perhaps the inconsistency of the OCR readings, obvious from this rough assessment, that makes Gale bashful about the restricted files. The editors of the ESTC confess to their public resource being a construction site in a way that the proprietors of private, commercial websites like ECCO prefer not to. But few really mind these days, especially if improvements are seen to be ongoing. Gale should be more relaxed about the incompleteness of their work, though perhaps not quite so relaxed as they are with the Burney Collection.” (Bullard 756)  



“The Text Creation Partnership was conceived in 1999 between the University of Michigan Library, Bodleian Libraries at the University of Oxford, ProQuest, and the Council on Library and Information Resources as an innovative way for libraries around the world to:

* pool their resources in order to create full-text resources few could afford individually

* create texts to a common standard suitable for search, display, navigation, and reuse

* collaborate with commercial providers, rather than constantly bargaining and competing with them” (TCP “Welcome”)

Those four named organizers --- Michigan, Oxford, ProQuest, and CLIR --- are the tip of the iceberg for institutional buy-in. At “Our scholarly partners,” TCP lists *two hundred* institutions which were involved in at least one of EEBO-TCP Phase I, EEBO-TCP Phase II, ECCO-TCP, Evans-TCP, many of which were involved in multiple of these projects. “This list does not include higher-education institutions based in the UK, all of which are counted as EEBO Phase 2 partners thanks to consortial funding” (TCP “Our scholarly partners”). The “FAQ” page still shows traces of the enormity of this undertaking. One question reveals that each partner library committed $60,000 to the venture, an amount which funds the keying and conversion of only around 250-300 books --- a large number, but a drop in the bucket compared to the approximately 73,000 books which the TCP as a whole has encoded.

A ten-person working group developed an encoding standard. The University of Michigan Library staff developed instructions to use this encoding standard. And then the text creation began. “Texts were selected each month at Michigan, page-images were supplied by ProQuest, marked-up transcriptions were submitted by the vendors, and quality control and editing undertaken at U-M Library and soon also at Bodleian Libraries in Oxford and subsidiary sites at the National Library of Wales, Aberystwyth, and at the University of Toronto.” (TCP “About”)

  

Two questions in the FAQ, “Why would I buy something that is achievable only if others do the same?” and “Why would I buy something that is going to become freely available?” taken together imply the speculative and ambitious nature of the original project. In the official answers provided to these evidently frequently asked questions, there is a sense that the project posed a prisoner’s dilemma: every individual institution’s “best” move, from a game theory perspective, was to contribute nothing to the project and then reap the benefits of everyone else’s work, but if every institution did so, then no one would benefit. A close reading of both responses illuminates an interesting tension in values:

Why would I buy something that is achievable only if others do the same?

Mere calculation may have disinclined some libraries from joining. TCP partnership was always less a purchase than an (admittedly risky) investment, since all of Michigan’s projections for the TCP corpus depended on a certain optimistic assumptions about how many other institutions would join.  Some libraries may have joined out of faith in Michigan’s track record, or because of a long-standing connection with the University or its staff. Some out of an idealistic belief in the collaborative model that TCP represented or in the public value of the product it promised. Some perhaps out of a cost-benefit risk estimate. For all the partner libraries, however, TCP membership was in effect a commitment to fellow libraries to share the burden and reward of this work. Partner libraries contributed to the cost of producing tens of thousands of painstakingly produced electronic editions of early English works. Each new library that joined made it possible for the project to key books that we otherwise would not, improving the corpus for everyone.

Why would I buy something that is going to become freely available?

This question too has no obvious answer that will please everyone, and indeed this question may have influenced some potential partners to refrain from joining. The structure of the TCP, with its provisions for exclusive access for a time, followed by public release, was something of a balancing act, designed to encourage membership by both those who were unwilling to wait ten years or so for access to the texts on behalf of their students and faculty, and those who believed in the creation of an unrestricted public resource and were willing on altruistic grounds to contribute to it.  Regardless of their motives for joining, the success of the EEBO-TCP depended on the support of partner institutions. The partnership fee directly funded the conversion of new books,  and greatly affected the rate at which the work was carried out. By joining up,  a library not only gained immediate access to the texts,  and not only contributed to making a larger, more comprehensive corpus for everyone, but also measurably affected the pace, and advanced the completion date, of the project--and thereby advanced the date at which  the texts would be released to the public.

The answers weakly attempt to provide the game-theory argument, but also carry the sense that the true answer, which they would like to give, has nothing to do with game theory and rationality, and everything to do with transcendent values of scholarship itself. In the minutes to the 2001 meeting of their executive board, they open with: “the project should determine more clearly if it is ‘a partnership or a product.’ Comment was offered that it is a ‘partnership to create a product’” (TCP Executive Board, 2001)



The 2003 meeting minutes concluded with the observation that “considering where it was even two years ago, it is stunning to think that EEBO-TCP now produces texts comparable to any commercial product in a very cost-effective way. It was agreed that the TCP concept is on the cusp of revolutionary changes in research and teaching and that it will continue to grow and extend the foundation which the project has built.” (Meeting Minutes 2003-10-22.) (These minutes also talk a lot about spending, and about how to sell EEBO and TCP.)



In the 2004 minutes: “Since the TCP has evolved from one project in cooperation with ProQuest to now three projects in cooperation with three different commercial publishers, it is useful to consider how the Board might adapt to accommodate the changing situation. ... The representatives of the three companies began the discussion by highlighting the things they would feel uncomfortable revealing in front of their competitors. These included pricing, their contributions to the TCP, and general marketing strategy. It was agreed that the Board should attempt to structure its meetings so that all members could be present and, if at some point there was a need to divulge sensitive information, the Board could hold an executive session in which that could be discussed.”  

“Members of the Board also felt that TCP should complete an equity review of all of its employees to see what salaries are being offered from other universities and whether TCP’s salaries are in-line with what is being offered elsewhere. It was suggested that TCP might want to do a 10% raise across the board for all reviewing staff that remain with the project for five years.” (TCP Executive Board, Meeting Minutes 2005-10-20) In the next year, “Mark also reported on one item from the previous Board meeting, salary reviews. Maria and Shawn investigated the possibilities of raising salaries for TCP reviewers within the University of Michigan, and compared salaries at Michigan and Oxford. It was found that the salaries of the reviewers were comparable with expectations. Paul also reported that the reviewers who left had reasons other than salary (family issues, better opportunities within their field of study, etc.). Therefore, it was decided not to raise all reviewers salaries at once, but to continue doing merit increases that the University of Michigan does every year.” (TCP Executive Board, “Meeting Minutes 2006-09-16.”)  

Wikipedia: “In September 2011, the Authors Guild sued HathiTrust (Authors Guild, Inc. v. HathiTrust), alleging massive copyright violation.\[12\] A federal court ruled against the Authors Guild in October 2012, finding that HathiTrust's use of books scanned by Google was fair use under US law.\[13\] The court's opinion relied on the transformativeness doctrine of federal copyright law, holding that the Trust had transformed the copyrighted works without infringing on the copyright holders' rights. That decision was largely affirmed by the Second Circuit on June 10, 2014, which found that providing search and accessibility for the visually impaired were grounds to consider the service transformative and fair use, and remanded to the lower court to reconsider whether the plaintiffs had standing to sue regarding HathiTrust's library preservation copies.\[14\]”

See: <https://www.publishersweekly.com/pw/by-topic/digital/copyright/article/54321-in-hathitrust-ruling-judge-says-google-scanning-is-fair-use.html>

See: <https://www.hathitrust.org/authors_guild_google>



The current copyright policy states that "Many works in our collection are protected by copyright law, so we cannot ordinarily publicly display large portions of those protected works unless we have permission from the copyright holder. Where we have the right to show page images of works, we will make every effort to do so. We are currently displaying works that are in the public domain (such as US works published before 1925), uncopyrightable works (such as works of the US government), or works where we have permission from the copyright holder. If we cannot determine the copyright or permission status of a work, we restrict access to that work until we can establish its status.” (HathiTrust, “Help - Copyright”)



  

In 2005 the TCP was thinking about themselves as competing with Google: “John Price-Wilkin, Associate University Librarian for Library Information Technology & Technical and Access Services, was a guest of the Board to talk about issues relating to the Google initiative and TCP’s role in promoting “enhanced” product to the library and academic community. TCP does have an important role in noting that OCR text, though good for many things, cannot serve all purposes scholars need, and TCP should continue to argue for structured electronic text for at least a portion of the collection.” (TCP Executive Board, “Meeting Minutes 2005-10-20.”)



”When the library at Alexandria burned it was said to be an “international catastrophe.” When the most significant humanities project of our time was dismantled in court, the scholars, archivists, and librarians who’d had a hand in its undoing breathed a sigh of relief, for they believed, at the time, that they had narrowly averted disaster.” (Somers)



“He offered the library a deal: You let us borrow all your books, he said, and we’ll scan them for you. You’ll end up with a digital copy of every volume in your collection, and Google will end up with access to one of the great untapped troves of data left in the world. Brin put Google’s lust for library books this way: “You have thousands of years of human knowledge, and probably the highest-quality knowledge is captured in books.” What if you could feed all the knowledge that’s locked up on paper to a search engine?

By 2004, Google had started scanning. In just over a decade, after making deals with Michigan, Harvard, Stanford, Oxford, the New York Public Library, and dozens of other library systems, the company, outpacing Page’s prediction, had scanned about 25 million books. It cost them an estimated $400 million. It was a feat not just of technology but of logistics.” (Somers)



“The human operator would turn pages by hand---no machine could be as quick and gentle” (Somers)



“It was the first project that Google ever called a “moonshot.” Before the self-driving car and Project Loon---their effort to deliver Internet to Africa via high-altitude balloons---it was the idea of digitizing books that struck the outside world as a wide-eyed dream. Even some Googlers themselves thought of the project as a boondoggle. “There were certainly lots of folks at Google that while we were doing Google Book Search were like, Why are we spending all this money on this project?,” Clancy said to me. “Once Google started being a little more conscious about how it was spending money, it was like, wait, you have $40 million a year, $50 million a year on the cost of scanning? It’s gonna cost us $300 to $400 million before we’re done? What are you thinking? But Larry and Sergey were big supporters.”” (Somers)



“Out-of-print books, almost by definition, were commercial dead weight. If Google, through mass digitization, could make a new market for them, that would be a real victory for authors and publishers. “We realized there was an opportunity to do something extraordinary for readers and academics in this country,” Richard Sarnoff, who was then Chairman of the American Association of Publishers, said at the time. “We realized that we could light up the out-of-print backlist of this industry for two things: discovery and consumption.” But once you had that goal in mind, the lawsuit itself---which was about whether Google could keep scanning and displaying snippets---began to seem small time. Suppose the Authors Guild won: they were unlikely to recoup anything more than the statutory minimum in damages; and what good would it do to stop Google from providing snippets of old books? If anything those snippets might drive demand. And suppose Google won: Authors and publishers would get nothing, and all readers would get for out-of-print books would be snippets---not access to full texts. The plaintiffs, in other words, had gotten themselves into a pretty unusual situation. They didn’t want to lose their own lawsuit---but they didn’t want to win it either.” (Somers)

But the ambitious class-action settlement was nixed by the Department of Justice, so Google wouldn’t have a monopoly. “No one is quite sure why the DOJ decided to take a stand instead of remaining neutral. Dan Clancy, the Google engineering lead on the project who helped design the settlement, thinks that it was a particular brand of objector---not Google’s competitors but “sympathetic entities” you’d think would be in favor of it, like library enthusiasts, academic authors, and so on---that ultimately flipped the DOJ. “I don’t know how the settlement would have transpired if those naysayers hadn’t been so vocal,” he told me. “It’s not clear to me that if the libraries and the Bob Darntons and the Pam Samuelsons of the world hadn’t been so active that the Justice Department ever would have become involved, because it just would have been Amazon and Microsoft bitching about Google. Which is like yeah, tell me something new.”” (Somers)

““This is not important enough for the Congress to somehow adjust copyright law,” Clancy said. “It’s not going to get anyone elected. It’s not going to create a whole bunch of jobs.” It’s no coincidence that a class action against Google turned out to be perhaps the only plausible venue for this kind of reform: Google was the only one with the initiative, and the money, to make it happen. “If you want to look at this in a raw way,” Allan Adler, in-house counsel for the publishers, said to me, “a deep pocketed, private corporate actor was going to foot the bill for something that everyone wanted to see.” Google poured resources into the project, not just to scan the books but to dig up and digitize old copyright records, to negotiate with authors and publishers, to foot the bill for a Books Rights Registry. Years later, the Copyright Office has gotten nowhere with a proposal that re-treads much the same ground, but whose every component would have to be funded with Congressional appropriations.”(Somers)

  

are they really making money or do they just think they are



#### 3.1.1.1.  ESTC history of steady improvement ####



Vander Meulen says that “The history of ESTC is in fact the record of steady developments. Some have been conspicuous---for instance, the physical progression from a printed prototype to microfiche, CD, online access via the vendors Blaise Line and RLIN, and universal online availability through the British Library.” (Vander Meulen 270) Many more have been less visible, in constant improvements to the accuracy and detail of the records.   

“Part I includes 135,000 printed works, comprising more than 26 million scanned facsimile pages.” (Gale, “Part I”) “From books and directories to bibles and sheet music to sermons and pamphlets, Eighteenth Century Collections Online, Part II features a variety of materials to provide a critical tool for both faculty research and classroom use. With more than fifty thousand new titles of previously unavailable or inaccessible materials, Eighteenth Century Collections Online, Part II is an essential addition for current owners of Part I.” (Gale, Part II). Each part (sold separately) also describes itself in breathless language assuring users of the comprehensiveness and breadth of the resource --- even as its existence in two parts belies this.



“Although "Eighteenth Century Collections Online, Part 2 adds six million pages that include previously unavailable or inaccessible titles," the latest promotional leaflet claims a total of "26 million pages of text" for ECCO - instead of 32 million - which is the same total that appeared in the original promotional leaflet for what is now understood to be ECCO Part 1. See "About Gale Digital Collections," accessed 1 March 2009, http://www.gale.cengage.com/DigitalCollections/; Eighteenth Century Collections Online (2008), accessed 1 March 2009 http://www.gale.cengage.com/pdf/facts/ECCO.pdf; and "Eighteenth Century Collections Online: The eighteenth century's most important revolution has just begun" (2005”) (Spedding 450)   

I love this because it makes complete sense -- it fits perfectly with PG's mission! -- but it also reflects how totally uninterested PG is in forming any kind of authoritative collection of literature. All PG wants to do is make .txt files that other people want to download.



#### 3.1.1.2.  Hathi governance ####







“HathiTrust, as a co-managed and co-funded collaborative of academic and research libraries, relies on its members to govern its programmatic, financial, and strategic directions. Membership established the governance model in 2012 and comprises the following entities: 

The Board of Governors

The Program Steering Committee

The governing bodies operate under the HathiTrust Bylaws. In addition to the formal governing bodies, multiple working, advisory, and task-based groups further enable member libraries to participate in and guide the direction of HathiTrust. As necessary, members vote on various policies or changes using the weighted voting system established in 2011.” (HathiTrust, “Governance”)



“The budget of HathiTrust is a separately maintained budget held within the University of Michigan budget system and managed by the Executive Director with oversight from the Executive Committee. Some additional financial components of the operation are represented by commitments in kind by participating members. The University Library’s financial procedures are subject to audits by the University of Michigan Office of University Audits.” (HathiTrust, “Governance”)



Wikipedia: “The partnership includes over 60 research libraries\[7\] across the United States, Canada, and Europe, and is based on a shared governance structure. Costs are shared by the participating libraries and library consortia.\[8\] The repository is administered by the University of Michigan.\[9\] The Executive Director of HathiTrust is Mike Furlough.\[10\] The HathiTrust Shared Print Program is a distributed collective collection whose participating libraries have committed to retaining almost 18 million monograph volumes for 25 years, representing three-quarters of HathiTrust digital book holdings.\[11\]”  









Current partners: “- Distributed Proofreaders. DP allows people to share in the tasks of proofreading, verifying and formatting eBooks for Project Gutenberg.

\- iBiblio, the public's library. iBiblio is our main eBook distribution site, holds our Web pages, and offers a variety of supporting services.

\- Project Gutenberg Consortia Center: Project Gutenberg Consortia Center (PGCC). Collections of collections, with numerous languages and formats. Sponsored by the World eBook Library. Host of the self.gutenberg.org self-publishing portal.” (“Partners, Affiliates and Resources”)  

In 2004, there was some discussion of spinning off a PGII with the World eBook Library. “The co-founders say that PG 2 came from the need to include existing e-book collections, such as those found in schools, universities, and professional and religious organizations. Many such e-books do not fall into the acceptable criteria of Project Gutenberg. "For those books, PG 2 was created to find a home. PG 2 is a consortium of collections. Our vision is to create an additional portal where a broader variety of intellectual objects may be accessed.”” (Hane)

Did this go anywhere?



It also brought to light the fact that Hart himself had the trademark for PG, rather than the nonprofit. When did that change?  

Because the texts are available without restrictions, a number of \[spin-off\] websites exist, some of which extend PG’s mission and some of which are slightly exploitative. On the most purely beneficial side are website “mirrors,” which duplicate the contents of the website in order to distribute the costs of hosting and to make sure that the texts remain accessible even if the “primary” Project Gutenberg website goes down. Also in keeping with Project Gutenberg’s core mission are projects which take the very plain text of PG works and make them more appealing or accessible in different formats. LibriVox, for example, is a similarly volunteer-driven effort which produces audiobooks of public domain works. Many organizations produce polished ebooks of PG works, and offer them for sale or for free in individual ebook reader libraries such as the Apple iBooks store or the Kindle store.  

All directly quoted from “Partners, Affiliates and Resources”







2	Sister Projects

2.1	Project Gutenberg of Australia

2.2	Project Gutenberg of Canada

2.3	Projekt Gutenberg DE

2.4	Project Gutenberg Europe

2.5	Project Gutenberg Self Publishing Portal

2.6	Projekt Runeberg

2.7	ReadingRoo.ms



3	Affiliates

3.1	ClassicalArchives.com

3.2	The Internet Archive

3.3	Librivox.org

3.4	LiteralSystems

3.5	ManyBooks.net

3.6	The Online Books Page

3.7	Outernet

3.8	RocketReader.com

3.9	Wattpad



4	Links to locations that provide software, tools, or Project Gutenberg eBooks in different formats

4.1	Andrew Sly's List of Canadiana in Project Gutenberg

4.2	Breeno.org

4.3	The Early Canadiana Online Project

4.4	GutenMark

4.5	Mobipocket

4.6	MobileRead

4.7	Noveltrove

4.8	thefifthimperium.com

4.9	Wikibooks



### 3.2.1.  intro ###  

Ian Gadd argues that critiques of digital databases are often not based on the right grounds: “the real risk of scholarly misuse of \[Early English Books Online\] is less to do with the physical features of early printed books that it fails, one way or another, to represent (problematic though these are) and more to do with a lack of an informed knowledge of what exactly EEBO *is*.” (Gadd 682) The observation applies far beyond EEBO: although it is easiest to critique digital resources for their failures to perfectly replicate the full tactile experience of the books they simulate, or for failing to contain every possible book, those critiques are fundamentally futile. \[What other critiques are possible?\] This chapter examines the history, textual selection, and implicit model of six databases: the ESTC, ECCO, ECCO-TCP, Project Gutenberg, Google Books, and HathiTrust. I explore how each database encodes its assumptions about what literature is, who it is for, and how it should be used. I historicize these models in the context of the institutional infrastructure behind the creation of each resource, particularly the economic factors driving development. I argue that the differences between each database’s literary model are best understood as the result of different strategies to navigate conflict between commercial and anti-commercial values. In parallel, I follow Charlotte Smith through each of these databases, to explore the impact of their different literary models. Very different works by Smith are available in each resource, and in very different formats. Smith’s most major works are not readily available, suggesting that these literary archives lag behind scholarly consensus about her importance. The chapter concludes with a discussion of new developments in Optical Character Recognition (OCR) technology, which can be used to transcribe texts from page images. \[OCR BLUF.\]







### 3.2.2.  database models ###  

Mark Merry’s Designing Databases for Historical Research as a rich entrypoint for historians to learn about database construction, in particular the chapter on “Conceptual models of database design” that contrasts “source-oriented” and “method-oriented” models:

“The Source-oriented model of database design dictates that everything about the design of the historical database is geared towards recording every last piece of information from the sources, omitting nothing, and in effect becoming a digital surrogate for the original...

\[The method-oriented model\] is based on what the database is intended to do, rather than the nature of the information it is intended to contain...

Method-oriented databases are quicker to design, build and enter data into, but it is very hard to deviate away from the designed function of the database, in order to (for example) pursue newly discovered lines of enquiry.

Ultimately, historians will need to steer a middle course between the two extreme models, perhaps with a slight tendency to lean towards the Source-oriented approach. When making decisions about what information you need from your sources to go into the database, it is important to take into account that your needs may change over the course of a project that might take a number of years.” (Lincoln)

 



#### 3.2.2.1.  ESTC model ####  

One reason that it can be informative to close-read the data structures of a resource like the ESTC is that a resource’s categories of knowledge are driven by the *uses* to which it expects that knowledge to be put. Examining the implicit assumptions that will make a given organization of knowledge seem logical, we can work backwards to the purpose of mission of the initial knowledge creation. Thus Tabor describes the data structure and the mission of the ESTC in a single statement: “ESTC’s most basic bibliographical function is to provide, for each edition, a description of the ideal copy, meaning the most complete and correct manifestation of that edition as the printer and publisher intended it” (369). Korshin further elaborates the use envisioned for this information: “the ESTC's cataloguing rules have been devised in such a way that a scholar anywhere in the world can tell, from the ESTC entry, whether the copy of the book in his or her library is the same or different from the one listed in ESTC” (211). Both “edition” and “ideal copy” are terms defined around the interests of a specialist audience of bibliographers, which bear inexact but important relationships to the formulation of an ESTC record.

An “edition” is a group of copies of a work which are understood to be interchangeable with each other (Tabor 369),[^cf31] though in practice different levels of granularity are applied in distinguishing between editions. The ESTC sometimes has separate entries for groups within an edition “when certain separately planned marketing units can be identified within the edition, such as reissues, imprint variants, and large versus regular-paper copies” (Tabor 369). Karian describes that “\[s\]ometimes the ESTC contains additional records if there are multiple *states* of an edition (a different state results from cancels or minor changes to the setting of type)” (289). Or, in “the later eighteenth century, when reprints from standing type became more common, ESTC cataloguers have occasionally granularized down to the level of individual impressions” (Tabor 369). As a result, Karian argues persuasively that ESTC records should not be treated as synonymous with “editions,” “issues,” or “titles,” since the same definitions of those boundaries may not be applied consistently. The specific question he poses is “What is the unit that the ESTC uses?” (289), and important question, to which the answer cannot really be “editions,” despite the best attempts of the ESTC bibliographers. Instead, he says “one should refer instead only to the ESTC record, a unit created by the ESTC and having no meaning outside the ESTC” (Karian 289).[^cf32]

The “ideal copy,” too, represents an interpretation. Because the ESTC is essentially a movel based on limited samples of an imagined lost prior whole --- “the most complete and correct manifestation of that edition as the printer and publisher intended it,” as Tabor termed it (369) --- a new sample can change the model. As Tabor describes, “\[a\]s additional reports of copies arrive, it may be that the ideal description must change in response. For instance, the existence of a half-title may only emerge on the evidence of the seventh copy reported. A half-title would then be added to the description of the ideal copy, and the six previously matched physical copies will receive notes recording that they are imperfect in this respect” (370). The ideal copy, like the database itself, thus represents a moving target.

  

So, how do these ideas of the edition and the ideal copy shape the data structures employed in the building of the ESTC? Consulting an individual ESTC record in the online database, as we can see in Figure 4, reveals a lot of information all pointing ‘outside’ of the ESTC itself. It begins with six details which will be present for every title: the “System Number” and “Citation Number” uniquely identifying the record; the author; the title; the publication information; and a physical description. It then displays any uncategorized “notes,” which in the case of *The Emigrants* (1793) consist of two additions to the physical description. \[Add other examples of “general notes”?\] The entry then points ‘outward’ to two “Surrogates”: the microfilm, and the electronic reproduction of the microfilm which is collected in ECCO. A very brief description is made of the work’s content --- its subject is “English poetry --- 18^th^ century” and its genre/form is “Poems” --- which is the only information provided about the *work* rather than the *book*.[ There’s a lot more to be examined re: these subject headings, especially if I do topic modelling for contents.



“Because the ESTC was formed from three different projects over many decades, the existence of subject headings varies. For books that are from the periods 1475-1640 (from the original Short-Title Catalogue created by Pollard and Redgrave) and 1641-1700 (from Donald Wing’s short-title catalogue), subject headings exist. But subject headings were not initially created for the eighteenth-century records in the ESTC, and so subject headings rarely occur for the items in ECCO.” (Karian, “Guide” 3)



How many of my records have subject headings? What is the ESTC’s ontology?] The remainder of the record is an extensive list of libraries which hold copies of the edition, divided into the three geographic regions of “British Isles,” “North America,” and “Other,” followed by a direct link to the ECCO copy referenced above.

This, however, is only how the ESTC *displays* its contents. Clicking another tab makes visible the MARC tags in which the data itself is stored. The MARC tags encode information at a slightly more refined level of detail. For example, the publication location in the standard view is listed as “Publisher/year” and displayed as the string “London : printed for T. Cadell, in the Strand, 1793.” A human can parse that string, but as the MARC version of the same information reveals, it is made up of three points of information that have been combined. The MARC data is listed as “260,” which is the MARC standard code for “Publication, Distribution, etc.” The line itself is displayed as “\|a London : \|b printed for T. Cadell, in the Strand, \|c 1793” --- indicating three separate pieces of information in the subfields “a - Place of publication, distribution, etc.”, “b - Name of publisher, distributor, etc.”, and “c - Date of publication, distribution, etc.” The separation of these points of information in the underlying MARC data is what allows the online database to conduct searches based on publisher, publication location, and date of publication. Even this is a reformatting of the underlying MARC code, which would read “##$a London :$b printed for T. Cadell, in the Strand,$c 1793” --- with the two “#” symbols at the beginning encoding that this is the first edition.[^cf33] It is, of course, only sensible for the ESTC to reformat its MARC code for display: MARC stands for MAchine Readable Catalogue, and machines and humans have very different needs as readers. However, what this exploration reveals is that \[???[ Is it that the categories of information are made less explicit as they are translated for humans, relying on the expert reader’s interpretive skill?]\].  

Figure 4: A screencap of the ESTC record for Charlotte Smith’s *The Emigrants* (1793).  

There are several different ways to search ESTC records. The “Search” button takes a user to the “Basic Search” function, from which there are also links to “Advanced Search,” “Browse,” and “Browse Libraries List” (which takes the user to the identical page as “Browse” but with “Library name” pre-selected as the index to browse). Once you have found a work of interest, however, several new forms fo searching become available, implied in the hyperlink formatting: almost any field in the entry can be clicked to reach other matching ESTC entries.



#### 3.2.2.2.  ECCO model ####



An edition being “included” in ECCO looks different from its inclusion in the ESTC --- whereas the ESTC lists just one record for each multivolume work, ECCO lists each volume separately, with links to the other volumes available in the “full citation” for the volume.



“Such consideration for users is sadly rather less visible with EEBO’s eighteenth-century equivalent ECCO. Unlike EEBO, ECCO presents users with a single, cropped page. In so doing, it has taken the opportunity to remove every blank page that ever appeared in an eighteenth-century book (pace its claim to provide ‘digital images of every page of 150,000 books published during the 18th Century’).” (Gadd 688)



“Unlike EEBO, ECCO includes an underlying text-transcription of its entire collection, which users can search but cannot access in any other way. According to ECCO’s online guide, the full-text transcriptions are generated using computerised ‘optical character recognition’ of the digitised images of \[start page 689\] each page, with ‘proprietary software created by the vendor to improve OCR accuracy, including the ‘correction’ of old English f/s ligatures and other spelling and character variants’; in addition, there are elaborate quality control systems:

For every digitized page of data, eight specific items are sampled for accuracy and correctness. Each page is visually scanned for glaring errors or omissions. Every 20th page is read in its entirety. (‘About Eighteenth Century Collections Online’)

With this in mind, it comes as somewhat of a surprise to discover that, according to ECCO, the word ‘fuck’ or versions thereof appear over 28,000 times in print in the eighteenth century. Finally, unlike EEBO, feedback from users does not seem to be much valued: the technical support e-mail address provided on its help pages no longer seems to be valid.” (Gadd 688-9)



“In various different ways, ECCO is less open with its users than is usual in academia. It likes to tidy away noisy information. The microfilms on which it is based reproduce images of the openings of books, but ECCO chops each opening into two single-page images and dispenses with all the detail (the rulers placed against type, the blank pages, and the indicators of physical structure) that is so precious to bibliographers. Since 2009, ECCO documents have been supported by MARC descriptions, drawn (presumably) from the ESTC and supplemented by Gale’s own ‘Subject’ entries. Users can search by subject, and yet the MARC files are not directly accessible for checking by readers using ordinary institutional subscriptions. Similarly, there is still no accurate way to identify by class- or shelf-mark the actual copy of a book that ECCO reproduces. An indication of the source collection is always given, but the larger libraries that the original microfilm photographers tended to favor often keep multiple copies of editions, and it is seldom possible to discover which copy has been used without a personal visit to the archive. This omission has several consequences: readers are further distanced from experience of the original material object; local features (such as MS annotations on the original) are inadequately documented; and the widespread phenomenon of stop-press variants within editions of hand-press era books is forgotten. Most frustratingly of all, ECCO does not allow users access to the scanned optical character recognition (OCR) documents that its full-text searches run on.

This secretiveness inevitably arouses suspicions about the accuracy of the scans.” (Bullard 755)



A glowing 2004 review of ECCO in the “Database & Disc Reviews” section of Library Journal says “The Advanced Search is so powerful it gave me sensory overload.” (LaGuardia 124)



#### 3.2.2.3.  TCP model ####



Although the ECCO-TCP now seems obviously built for text-mining distant reading, in fact it is largely organized around searching and consulting individual works.



“ECCO natively supports OCR-based full-text searching of this corpus. This is significant because it meant that unlike EEBO-TCP (which produced searchable text where there was previously none at all), ECCO-TCP could only hope to produce *more accurate* text (and more reusable text) than what was already available. The larger size of ECCO (because of the great increase in printing and greatly enhanced chances of survival of printed works in the 18th century) also made it a different proposition: nothing so ambitious as EEBO-TCP coverage was feasible for ECCO-TCP. ... Because of these greater challenges facing ECCO-TCP, it is perhaps better described as a proof of concept than as a completed project. With the support of more than 35 libraries, the TCP keyed, encoded, edited, and released 2,473 ECCO-TCP texts. A further tranche of 628 texts was keyed and encoded but never fully proofed or edited. The texts in this group remain useful for many purposes, however, and bring the total of ECCO-TCP texts to over 3,000.” (TCP “ECCO”)



#### 3.2.2.4.  Hathi model ####



Wikipedia: “PageTurner is the web application on the HathiTrust website for viewing publications.\[17\] From PageTurner readers can navigate through a publication, download a PDF version of it, and view pages in different ways, such as one page at a time, scrolling, flipping, or thumbnail views.\[17\]\[18\]”



See: <https://www.hathitrust.org/technology>

See: <https://search-proquest-com.myaccess.library.utoronto.ca/results/58AF728D91BD440DPQ/false?accountid=14771>



#### 3.2.2.5.  PG model ####



The structuring principle of Project Gutenberg is its missions to make books available for pleasure reading. I argue that its core concept, analogous to the “edition” in the ESTC, or the “book” in ECCO and HathiTrust, is the “story.” Many of the priorities of Project Gutenberg which seem incompatible with scholarly approaches to textual history are explained by thinking of Project Gutenberg as being structured around “stories” rather than “books.”



“At the end of 1993, Project Gutenberg's eTexts were organized into three main sections: a) "Light Literature", such as Alice's Adventures in Wonderland, Peter Pan or Aesop's Fables; b) "Heavy Literature", such as the Bible, Shakespeare's works or Moby Dick; c) "Reference Literature", such as Roget's Thesaurus, and a set of encyclopaedias and dictionaries. This organization in three sections was abandoned later for a more detailed classification.” (Lebert)  

The article ”Quantitative patterns of stylistic influence in the evolution of literature” uses Project Gutenberg --- do mathematicians not know what a good source of literature is, or do they know better than us?



Cite Hammond’s book re: comparing modernists to bestsellers --- he can’t always find bestsellers, it depends on whether bestsellers were enjoyed enough for someone to bother to type them up



#### 3.2.2.6.  GB model ####  

Google Books prioritizes low-quality information over *no* information. The algorithmic extraction of publication dates from title pages, for example, can never be perfect. But algorithms give their predictions with certainty estimates: if accuracy was a higher priority, Google Books could calibrate the algorithm to simply provide no answer when none of the possibilities cross a given certainty threshold.



Per <http://languagelog.ldc.upenn.edu/nll/?p=1701> , they actually OVERWRITE metadata provided by partners with their algorithmic information!! They could very easily *not*.



“At its peak, the project involved about 50 full-time software engineers. They developed optical character-recognition software for turning raw images into text; they wrote de-warping and color-correction and contrast-adjustment routines to make the images easier to process; they developed algorithms to detect illustrations and diagrams in books, to extract page numbers, to turn footnotes into real citations, and, per Brin and Page’s early research, to rank books by relevance. “Books are not part of a network,” Dan Clancy, who was the engineering director on the project during its heyday, has said. “There is a huge research challenge, to understand the relationship between books.”” (Somers)



#### 3.2.2.7.  database selection ####  

“It has been difficult to consider playbills at scale because they were excluded from the catalogs that form the basis for mass digitization efforts. The absence of playbills from Eighteenth-Century Collections Online (ECCO), as well as from its pre-1700 counterpart Early English Books Online (EEBO), is a result of the decision, as reported by R. C. Alston, not to include them in the English Short Title Catalog (ESTC) on which those collections are built.” (Vareschi and Burkert 600 --- citing R. C. Alston, “The Eighteenth-Century Non-Book: Observations on Printed Ephemera,” in *The Book and the Book Trade in Eighteenth-Century Europe*, ed. Giles Barber and Bernhard Fabian (Hamburg: Dr. Ernst Hauswedell & Co., 1981), 343--60, quote on 344--45.)



“Alston recalls that the unusually high survival rate of playbills / actually worked against their inclusion in the ESTC during its creation in the 1970s. Often thought of as fragile in their ephemerality, playbills actually “seem to have been preserved more consistently than any other category of ephemera.”17 The compilers of the ESTC decided to catalog approximately 250,000 ephemeral materials like pam- phlets and ballads, but balked at including playbills, which would have added nearly 50,000 items from the British Library and the Victoria and Albert Museum alone.18 This decision has had serious ramifications for the study of British culture in the ensuing thirty years: ballads and pamphlets are now the basis of numerous serious studies by scholars in literature and history, while playbills are not.” (Vareschi and Burkert 600-601)

“Alston’s account reminds us that **playbills are infrequently studied today because they were** ***made*** **less accessible and less institutionally supported than other kinds of items**. While they once shared a visual field with the other cheaply printed materials of early modern London, such as libels and advertisements,20 today they are nearly invisible to scholars who can access those other cheap prints with the click of a button.” (Vareschi and Burkert 600-601)  

“while ESTC may be based on two thousand public and private libraries worldwide, the Eighteenth Century microfilm series is based on books from only a tiny fraction of that number - almost certainly less than twenty libraries, and rarely anywhere other than the British Library, the Bodleian, Harvard, and the Hunt” (Spedding 440)



The “microfilm series is not a random - and therefore randomly representatve--- selection of items from ESTC. Texts have been selected for filming on the basis of criteria that are rarely mentioned, but which include ease of access for filming (initially, items at the British Library) and the desire to avoid duplication of texts. That is, by the desire to get the biggest bang for Gale’s buck.” (Spedding 441)



“There may also be commercial considerations at work. Alt not conducted a systematic search for items from the British L Case" (its collection of erotic material), it seems that little of tha the Eighteenth Century microfilm series, and the material that h has only been quite recently added.37 Consequently, much of this ing from ECCO. The reason for this may be that much Private Ca as late as 1989, not represented on ESTC,38 but it may also be be Private Case was microfilmed by Adam Matthew Publications in under the title Sex and Sexuality 1 640-1 940 . That is, the eighteent terial in the Private Case may have been withheld from the Eigh microfilm series (and consequently ECCO) to ensure the profitab Sexuality. Similar, and similarly hidden, criteria seem to affect o such as EEBO and Goo” (Spedding 441)



Gale proudly declares that “this collection contains every significant English-language and foreign-language title printed in the United Kingdom between the years 1701 and 1800.” (“Eighteenth Century Collections Online”) This claim is easily overturned with a single counterexample, such as the first edition of Charlotte Smith’s *Elegiac Sonnets*, a significant title which is absent.



“Full-text searching across all 26 million pages enables users to explore a vast range of books and directories, bibles, sheet music, sermons, advertisements, and works by both celebrated and lesser-known authors. Researchers will also find rare works from women writers of the eighteenth century, collections on the French Revolution, and numerous editions of the works of Shakespeare.” (Gale, “Part I”)  

”subject heading assignment is pragmatic and heuristic rather than an exercise in truth and accuracy. ...normally the question to be asked is whether they are good or bad, helpful or irrelevant, rather than true (accurate) or false (incorrect). The assignment of subject headings cannot and need not conform to the same standards that apply to descriptive bibliography in the tradition of W. W. Greg and Fredson Bowers. What descriptive accuracy is to cataloging and bibliography, *consistency* is to the assignment of subject headings.” (Garrett 69)



“While the English Short Title Catalogue (ESTC) contains Library of Congress Subject Headings (hereafter: LCSH) for books from the English-speaking world printed before 1701, entries for material dating from 1701 to 1800 are not subject indexed.” (Garrett 70)

“An already difficult situation has been aggravated recently by the sudden (and otherwise hugely welcome) availability of a new and mammoth library of searchable eighteenth-century text online through Thomson Gale’s Eighteenth Century Collections Online (ECCO).” (Garrett 70)

“Northwestern harvested records from WorldCat containing subject headings for eighteenth-century titles in ECCO.” (Garrett 71)

They found that roughly 60% of the material located in a search for “East India Trading Company” was only found due to the subject headings, and would not have been found by a simple keyword search. (Garrett 73-4)

“Shortly after completion of this manuscript, The British Library announced that it intended to add LCSH to all 18th-century records in the ESTC by autumn 2007. The subject headings extracted by Northwestern from the WorldCat database will constitute an important part of this enhancement project.” (Garrett 77)  

What is *in* the TCP? Well, when active transcription was taking place, “users (especially those affiliated with partner libraries) were welcome to request works from EEBO that had not yet been keyed, and that their requests would go to the top of the queue” (TCP “FAQ”). So --- the TCP contains whatever individual works happened to interest particular scholars.



The TCP, unlike the ESTC and ECCO, intentionally avoids including multiple editions of a given work. This decision was a pragmatic one motivated by “limited funding” and a sense of scarcity: “Simply put, for every book that we chose to convert, a different book does not get converted: duplication, even partial duplication, has its costs” (TCP “FAQ”). Since the TCP never envisioned itself as a fully complete collection, the priority in textual selection “was always to capture as many different works and as great a variety of text as we could, usually focusing on the first edition of each work”(TCP “FAQ”). To a certain extent, this lack of duplication can be useful for text-mining: it places all texts on an equal playing field, rather than double-dipping on some works. However, they “have keyed additional editions where there is sufficient justification for doing so, and a user has made a case for it,” so the corpus cannot be assumed to contain *no* duplicated works (TCP “FAQ”).



“Selection of works to transcribe for EEBO Phase 1 was initially based on named authors mentioned in the New Cambridge Bibliography of English Literature.  Though this tended to bias selection a bit toward canonical, or at least attributed, works, anonymous works may also have been selected at this stage if their titles appeared in the bibliography. The New Cambridge Bibliography of English Literature was chosen as a guideline because it included foundational works as well as less canonical titles related to a wide variety of fields, not just literary studies. In any case, this initial reliance on the New Cambridge soon gave way to a series of deliberate attempts to cast a wider net, for example by selecting works exemplifying a particular theme (food, drugs, piracy, witchcraft), or fitting a particular format (broadsides, pamphlets, etc.)  The intention was to supplement methodical selection with more or less random selection based on arbitrary criteria in order to expand the generic diversity of the corpus. Requests for particular works by faculty at partner institutions were also taken into consideration and, if feasible, placed at the head of the queue. A user willing and able to make a case for a given work almost always prevailed over other considerations.” (TCP “EEBO”)



“Discussion then began on how to further develop the TCP project and insure that we can create 25,000 texts. In order to do this, the Board felt that it needed more information about the complete number of texts possible to include in the corpus, a dollar amount per title required to complete the project, a report on the total number of institutions, and the gap between EEBO subscribers and EEBO-TCP partners. Shawn Martin will work with Mary Sauer-Games to gather this information and report back to the Board. Some members thought that EEBO-TCP should go back to already existing partners and ask for a second round of funds to complete the project. In order to do this, Board members felt EEBO-TCP should look very carefully at what titles it is selecting and come up with summaries of the number of desirable titles to convert and initiate a strategy that could persuade libraries that it would be worth another years commitment to complete, for example, all of the Thomason tracts, or all of a particular genre or canonical category.” (TCP Executive Board, “Meeting Minutes 2005-10-20.”)



“Shawn Martin then discussed the selection process for all of the TCP projects. Though there are commonalities between them, selection runs differently for all 3 projects and each project runs fairly independently. Therefore, it becomes a question of how much should TCP coordinate collection between the three projects and how should TCP manage duplication. Scholars prefer that TCP duplicate titles between EEBO, Evans, and ECCO; librarians prefer to avoid duplication. The Board felt that it should receive a report of all duplicated materials, that TCP create an oversight group of librarians to coordinate the three projects, and where feasible TCP should try to minimize duplication.” (TCP Executive Board, “Meeting Minutes 2005-10-20.”)



“To users working with the EEBO-TCP texts, the ECCO-TCP texts may form a useful adjunct, since for the latter some attempt was made to select works by authors who straddled the divide between the 17th and 18th centuries, the thought being that authors whose earlier works we had included in our 17th-century corpus could be “completed” by having their later works included in our 18th-century (ECCO-TCP) corpus. That helps account (for example) for **the heavy representation of Defoe** in ECCO-TCP.” (TCP, “Eighteenth Century Collections Online (ECCO) TCP”)  





“There are three portions of the Project Gutenberg Library, basically be described as:

Light Literature; such as Alice in Wonderland, Through the Looking-Glass, Peter Pan, Aesop's Fables, etc.

Heavy Literature; such as the Bible or other religious documents, Shakespeare, Moby Dick, Paradise Lost, etc.

References; such as Roget's Thesaurus, almanacs, and a set of encyclopedia, dictionaries, etc.

The Light Literature Collection is designed to get persons to the computer in the first place, whether the person may be a pre-schooler or a great-grandparent. We love it when we hear about kids or grandparents taking each other to an etexts to Peter Pan when they come back from watching HOOK at the movies, or when they read Alice in Wonderland after seeing it on TV. We have also been told that nearly every Star Trek movie has quoted current Project Gutenberg etext releases (from Moby Dick in The Wrath of Khan; a Peter Pan quote finishing up the most recent, etc.) not to mention a reference to Through the Looking-Glass in JFK. This was a primary concern when we chose the books for our libraries.

We want people to be able to look up quotations they heard in conversation, movies, music, other books, easily with a library containing all these quotations in an easy to find etext format.” (Hart “History and Philosophy”)  

The founding logic of Project Gutenberg resonates strikingly with Bordieu’s call to “*universalize in reality the conditions of access*” (qtd in Guillory 340, emphasis original) to literature.

The first Project Gutenberg texts are almost a parody of important texts: The Declaration of Independence, The King James Bible. These are the texts assumed to be urgently desired by “99% of the general public” (Hart “History and Philosophy”). They are then followed, however, by a work which has rarely been central to the institutional hierarchies of cultural capital: Alice in Wonderland. As Hart describes his choices of what texts to transcribe next, he seems to be describing a version of what Guillory hoped for, “another kind of game” in which texts can compete for cultural capital, a game “with less dire consequences for the losers, an *aesthetic* game” (Guillory 340, emphasis original).



“Project Gutenberg selects etexts targeted a bit on the "bang for the buck" philosophy ... we choose etexts we hope extremely large portions of the audience will want and use frequently. We are constantly asked to prepare etext from out of print editions of esoteric materials, but this does not provide for usage by the audience we have targeted, 99% of the general public.” (Hart “History and Philosophy”)



### 3.2.3.  OCR ###  

How accurate does OCR need to be? This depends on how the OCR will then be used.  

The existence of a carefully hand-corrected transcription of *The Emigrants* in ECCO-TCP provides an opportunity to check the reliability of the OCR in both ECCO and HathiTrust. I will proceed from the assumption that the ECCO-TCP files are 100% accurate, and that any differences between the OCR and ECCO-TCP represents an OCR error.[^cf34] Before beginning the experiment, my hypothesis was that both ECCO and HathiTrust would differ from each other in where and how they are inaccurate, but would have similar accuracy overall. I suspected that they were likely around 50% accurate, plus or minus 10% --- I wouldn’t be surprised if they were worse, but would be quite surprised if their accuracy was 80% or higher.[ What level of accuracy do people usually want for OCR research?] Acquiring the plaintext files from all three sources required some hunting for some hidden options and some workarounds; rendering them suitable for comparison required some modifications of each file, described more fully in Appendix B. Although Gale Digital Scholar Labs prominently provided an “OCR Confidence” of 95%, the first glance at the document was not very promising. To my surprise, Juxa calculated a relatively low “change index” for each text compared to the TCP witness: ECCO had a .16 change from base (i.e., 84% accuracy), and my normalized HathiTrust document had only a .09 change from base (i.e., 91% accuracy).[^cf35] This surprised me, and suggests that skepticism of OCR in eighteenth-century text mining may no longer be appropriate.  

To make these comparisons concrete, consider the first page of Smith’s dedication, as it is captured by OCR in ECCO and HathiTrust, and in the ECCO-TCP transcript:



TO WILLIAM COWPER, Es DEAR SIR, THERE is,- I hope, some propriety in my addrefing a Com- potion to you, which would,never perhaps have existed, had I not, amid the heavy prefure of many sorrows, derived infinite consolation from your Poetry, and some degree of animation and of confidencefrom your efieen. . 'he.following performance isfarfrom aspiring to be con- fidered as an imitation of your inimitable Poem, " THE " TASK;" I am perfeetly sensible, that it belongs not to a feeble andfemninine hand to draw the Bow of Ulyfes.,Theforce, clearness, and sublimity ofyour admirable Poem; the felicity, almost peculiar to your genius, of givingto the moJ familiar objegls dignity and eset, I could never hope to,a reach (ECCO)



T O WILLIAM com/PER, Ess. DEAR SIR, THERE is, I hope, ſome propriety in my addreſſing a Com- poſition to you, which would never perhaps have exiſted, had I not, amid the beavy preſſure of many ſorrows, derived infinite conſolation from your Poetry, and ſome degree of animation and of confidence from your ºfteem. The following performance is far from aſpiring to be con- ſidered as an imitation of your inimitable Poem, “ The “TAsk;” I am perfºy fººl, that it belongs not to a feeble and feminine band to draw the Bow of Ulyſſes. The force, clearneſ, andſublimity of your admirable Poem; the felicity, almoſt peculiar to your genius, of giving to the moſt familiar obječís dignity and effečf, I could never hope to 3. - Reach (HathiTrust)



TO WILLIAM COWPER, ESQ.

DEAR SIR,



THERE is, I hope, some propriety in my addressing a Com\|position

to you, which would never perhaps have existed, had

I not, amid the heavy pressure of many sorrows, derived

infinite consolation from your Poetry, and some degree of

animation and of confidence from your esteem.



The following performance is far from aspiring to be con\|sidered

as an imitation of your inimitable Poem, "THE

TASK;" I am perfectly sensible, that it belongs not to a

feeble and feminine hand to draw the Bow of Ulysses.



The force, clearness, and sublimity of your admirable Poem;

the felicity, almost peculiar to your genius, of giving to the

most familiar objects dignity and effect, I could never hope to (ECCO-TCP)



Figure 5 shows how Juxta highlights the words which vary between these three copies.

Both of the OCR copies contain errors in individual letters which render the whole word interpretable by a human but not by text mining software, as in the case of “beavy” for “heavy.” The ECCO copy struggles with the fact that ſ is not an available character, sometimes substituting an f, as in “prefure” for “preſſure.” Both leave out spaces between words, creating new tokens like “isfarfrom” and “andſublimity,” though HathiTrust is less prone to this error.

Other features of the OCR copies are accurate to the page image but would nonetheless interfere with text mining. The hyphenation of “Com- poſition,” for example, would prevent it from rendering as a single word, though here even the careful TCP copy would introduce the same problem, since the line break is encoded as “Com\|position.” Before the TCP copy could be used for text mining, the \| characters would likely need to be removed --- not too different from removing the hyphenation from the ECCO and Hathi copies. Most difficult to resolve is the fact that OCR naturally attempts to capture *all* text on the page, including the signature mark and catch word. In ECCO these appear as “,a reach” and in Hathi they are “3. - Reach” whereas TCP more appropriately leaves these out. Unlike the problems with hyphenated words, there is no way to correct for the inclusion of catchwords in a document, since there is no predictable way to identify them --- but keeping them in the document will cause any text-mining software to count these words twice.

The usual “text cleaning” procedures would further prepare these OCR texts for text mining by transforming all words to lowercase, removing all punctuation, and, in most cases, deleting all words which don’t match a predefined dictionary of valid words. A scholar working with the HathiTrust OCR would almost certainly add to this a step converting the ſ character to an s, as discussed above, in order to make the dictionary comparison feasible. The result of this ‘cleaning’ would likely look something like the following:



to william dear sir there is i hope some propriety in my a potion to you which would never perhaps have existed had i not amid the heavy of many sorrows derived infinite consolation from your poetry and some degree of animation and of your he following performance aspiring to be con as an imitation of your inimitable poem the task i am sensible that it belongs not to a feeble hand to draw the bow of clearness and sublimity admirable poem the felicity almost peculiar to your genius of the familiar dignity and i could never hope to a reach (ECCO, as it would likely appear after text “cleaning”)



 

william dear sir there is i hope some propriety in my addressing a position to you which would never perhaps have existed had i not amid the pressure of many sorrows derived infinite consolation from your poetry and some degree of animation and of confidence from your the following performance is far from aspiring to be considered as an imitation of your inimitable poem the task i am that it belongs not to a feeble and feminine band to draw the bow of ulysses. the force of your admirable poem the felicity almost peculiar to your genius of giving to the most familiar dignity and i could never hope to 3 reach (HathiTrust, as it would likely appear after text “cleaning”)





Strikingly, these ‘clean’ texts are now further from legible to human eyes, as OCR errors which a reader could mentally correct (such as “beavy” for “heavy” are now entirely removed.  

![][juxta-emigrants-p1]

Figure 5: Juxta’s “Heat Map” visualization of the “base” witness of the first page of *The Emigrants* (i.e., the ECCO-TCP version carefully prepared by scholars), highlighting words which differ in the two witnesses of the ECCO OCR and the normalized HathiTrust OCR. A darker highlight indicates that the word varies in more than one witness.  

![][juxta-emigrants-histogram]

Figure 6: A histogram, produced by Juxta, showing where the two ECCO and normalized HathiTrust witnesses show the most difference from the base ECCO-TCP copy. “Longer lines indicate areas of considerable difference, while shorter lines indicate greater similarity between documents.” (“A User Guide to Juxta Commons”)  

![][fig-ecco-emigrants-p1]

Figure 7: The facsimile of the first page of *The Emigrants* found in ECCO, which forms the basis of the ECCO OCR text.  

![][hathi-emigrants-p1]

Figure 8: The facsimile of the first page of *The Emigrants* found in HathiTrust, which forms the basis of the HathiTrust OCR text.  

“16 In his discussion of JSTOR's "intolerably corrupt" OCR text, Nicholson Baker suggests that the reason why the user is prevented from scrolling through this naked OCR output is that scholars "might, after a few days, be dis- turbed by the frequency and strangeness of its mistakes . . . and they might no longer be willing to put their trust in the scholarly integrity of the database."17 Baker's criticism of JSTOR, however, is based on an error rate (with editorial intervention) of just one typo in every two thousand characters.” (Spedding 439)

“The two OCR-captured texts average over 150 typos per 2,000 characters,22 a high enough error rate to render parts of the text completely unintelligible. It is not clear how typical this error rate is, and how much it declines with editorial intervention,23 but again the scale of the problem is clear.24 Consequently, the claim that OCR errors "may occasionally result in incorrect character capture, which may affect some \[ECCO\] full-text search results," seems wildly, even heroically, optimistic.” (Spedding 440)



### 3.2.4.  conclusion ###  

Like literary canons, these corpora --- especially smaller ones, like the Eighteenth Century Collections Online Text Creation Partnership --- are vulnerable to a critique of their selection methods on the grounds of representation. However, unlike the various changing literary canons of the past, digital corpora tend to conceal which particular titles have been selected as representative. I argue that Charlotte Smith’s inclusion in these resources lags behind a scholarly consensus which sees her as increasingly important and canonical in the period. Her partial inclusion in ECCO-TCP seems particularly likely to lead to ill-supported conclusions by researchers who might easily assume that their text-mining research is taking her works into consideration. However, since none of her sonnets are included, nor any of the politically radical novels which made up a substantial portion of her latter career, nor any of her natural history, some of her most important contributions to the literature of the period are not able to impact studies in which they would be relevant. In particular, a study of women’s writing through the lens of the ECCO-TCP would emphasize the most conventional and expected women’s writing from Smith, with four volumes of one of her more straightforward marriage plot novels.

Exploring the technical affordances of the copies of Smith’s works available in each database also shows why the distorted impression of Smith’s works reflected in the ECCO-TCP’s corpus is likely to persist and continue to be reproduced: without the foundation of a reliable but transformable text (in the form of a human-corrected transcription, rather than a page image or machine OCR), there is a nearly insurmountable technical barrier before any individual project. Even to assess the accuracy of the OCR texts in ECCO and HathiTrust, I must rely on ECCO-TCP. Guillory has already argued persuasively that representation in literary canons is a matter of selection, not of exclusion, that the default state for a given text is not to be included. For Guillory, this serves as a proof that sexism and racism are rarely the direct cause of a particular text lacking canonical status; the role of social oppression in limiting textual representation occurs before scholars make their choices, when classes of people are systematically excluded from the means of textual production in the first place, limiting what we may select from. In the case of digital corpora, also, I see that the rhetoric of “exclusion” is not accurate, and directs attention away from the more complex systems at play. Although I critique the failure of ECCO-TCP to include important and relevant works by Charlotte Smith, it does not seem that she has been excluded out of a prejudice against women’s writing. Most likely, The Emigrants and Celestina were chosen because copies were conveniently accessible to a particular scholar involved in the creation of the ECCO-TCP, perhaps even directly related to a research question which would motivate them through the mind-numbing process of retyping long volumes of prose. Once these works had entered ECCO-TCP, they will naturally be re-used for text mining research which implicitly trusts the original selection. In this way, representation in digital corpora is a matter of infrastructure.  

We are on the cusp of eighteenth-century OCR meeting the standards of twenty-first century OCR. What texts should be OCR’d, by whom, and what should be done with those text files?  

“Because the ESTC was formed from three different projects over many decades, the existence of subject headings varies. For books that are from the periods 1475-1640 (from the original Short-Title Catalogue created by Pollard and Redgrave) and 1641-1700 (from Donald Wing’s short-title catalogue), subject headings exist. But subject headings were not initially created for the eighteenth-century records in the ESTC, and so subject headings rarely occur for the items in ECCO.” (Karian, “Guide” 3)



### 3.3.1.  data cleaning ###  

The next phase of this project does not deal with these databases directly, but with my own curated samples. I have collected as much information as I can about all the works published in England 1789-99 held within each database. What follows is a detailed description of each of these samples, and of the further work which I employed to prepare these samples for computational experimentation  

“The more I listen to humanists working through data issues and challenges, I see a common tension arise:

1\. We know that all data is a reductive construction

2\. We also worry a lot about creating data that’s clean and

usable enough to share, which would seem to imply we’ve already compromised on point 1.” (Lincoln)



“Katie Rawson and Trevor Muñoz’s “Against Cleaning”, and the “Tidy Data” paper they cite by statistician Hadley Wickham. Although the former comes from a humanistic / librarian perspective, and the latter a statistical and programming perspective, I believe they are in fundamental agreement. ... Wickham’s paper, published in the Journal of Statistical Software, has been fairly called a manifesto, as it strongly advocates for more attention towards the “mundane data manipulation chores” that precede any kind of analytical work. “Tidy datasets are easy to manipulate, model and visualize, and have a speciy a large number of queries against the data with much less effort than if they had been working with, for example, a word document transcribing the stock books into paragraphs. In this way, Wickham’s “tidy data” goes hand in hand with the deep questions that Rawson and Muñoz pose about the notion of “clean” data.” (Lincoln)



“I leaned on the word “tidy” in part due to Marie Kondo’s *Tidying Up*, newly popularized in the United States from the Net*your* home, what matters most? Answering that question isn’t a prerequisite to starting to tidy - it’s an ongoing discovery that happens during the process of tidying.” (Lincoln)  

My own sliver of the ESTC was generously provided to me by the British Library in January 2017. It contains all items matching the query I specified, “(Words= alldocuments and W-year= 1789-\>1799 and W-Country of publica= enk),” which requests all documents published between 1789 and 1799 (inclusive) with a place of publication encoded as “England.” Running this search on the ESTC website at the time returned 52,001 records. The tools used to create the file, according to the librarian with whom I corresponded, returned 51,965 records, 36 records having gone missing; however, the file itself contains only 51,860, another 105 mysteriously lost. These 141 missing records are currently an unsolved mystery.

My records come from the British Library’s ESTC database, rather than the STAR file. The corpus itself consists of a csv file with fifteen columns of information.  

My first source of ECCO metadata consisted of MARC records, kindly provided by University of Toronto libraries (my thanks to Leslie Barnes!). I requested information for all works published 1789-99 in the UK (so, including Ireland and Scotland, but excluding America.) Later I changed my mind and didn’t want Ireland and Scotland any more, which created a problem for myself.

My ECCO metadata presented particular challenges. I had access to MARC records, which stands for MAchine Readable Catalogue. At several points, I read this data with my feeble non-machine eyes in order to guide my data processing.

Using MarcEdit, I converted these MARC records to csv files which could, in OpenRefine, be read, manipulated, and merged like my other corpora. Since I was not able to simply convert “all the MARC headings that exist” using MarcEdit, I used all numbers 1 to 999 and \[will\] delete empty columns. I frequently referred to the Library of Congress’s MARC info at “MARC 21 Format for Authority Data.” Cataloger's Reference Shelf, The Library Corporation. However, ECCO encodes much of its data in “unassigned” columns, rather than the standardized LOC categories, presenting some challenged.



These MARC records came in four files, named ECCO.mrc, ECCO1.mrc, ECCO2.mrc, and ECCO2-A.mrc. For some internal clarity I renamed the “ECCO.mrc” file to “ECCO0.mrc”.   

Several years in to the project, Gale released a new interface for their various collections, the Digital Scholar Lab, which provided new forms of access to ECCO texts. Through the Digital Scholar Lab, it was now permitted to download the OCR transcripts of facsimiles, though only 10,000 items could be downloaded at a time. To work within the Digital Scholar Lab’s restrictions, I created eleven “Content Sets,” one for each year 1789-99, and downloaded them individually. The query used to find each year’s works was “LIMITS: Archive (Eighteenth Century Collections Online) And Publication Date (1789).” All content sets were downloaded on June 2, 2020. For each content set, I downloaded the provided “metadata” .csv, and then used a simple Terminal command (cat \*.csv \>merged.csv) to merge them together.



The merged spreadsheet of all eleven years had 45,027 rows. Adding the number of files in each year gives a total of 45,017, so the merged spreadsheet has 10 extra rows --- probably repeating the title headings.

For reference, I ran a search with the query “LIMITS: Archive (Eighteenth Century Collections Online) And Publication Date (1789 - 1799),” which returned 44,121 results. This is a particularly unexpected mismatch, since it means items *did* appear when I searched for one year (e.g., 1797) but *didn’t* appear when I searched for a range of years that ought to have included that year. Unfortunately, since I can’t make a “content set” of all 44,121 items, there is no simple way to determine which 906 items mysteriously failed to appear.

Olli McMullin cleverly suggested that this mismatch might be due to serial publications, which appear in each of the years with which they are associated (counting them multiple times in the merged set) but only once in the full-span search. I investigated Helen Maria Williams’ *Letters Written in France* as a 1790s serial publication to see if this might be occurring. Indeed, *Letters on the French Revolution, written in France, in the summer of 1790, to a friend in England; containing, various anecdotes relative to that interesting event, and memoirs of Mons. and Madame Du F--. By Helen Maria Williams* (CW0104715153), which has the publication date “MDCCXCI\[-MDCCXCII\] \[1791-1792\],” appeared in both 1791 and 1921 content sets. This is not exactly a serial work, but rather a work of uncertain or flexible date, making the double-counting less appropriate. However, it can be addressed by removing duplicated Gale document ID numbers.  

Somewhat oddly, because the Text Creation Partnership texts are made freely available, they are also somewhat difficult to track down. There is no central location with all of them and their related information --- not even the TCP’s website. In \[YEAR\], the website contained many broken links to possible places to download the corpus, and the best guide was actually a blog post. By \[YEAR\], a general redesign of the TCP’s website added more up to date links for TCP texts and clarified what kinds of information are available about them in each place. However, the focus was on searching and reading TCP texts, or downloading corpora of their full texts. It is possible to download the XML “headers” of all ECCO-TCP files from a University of Michigan Dropbox (last updated Nov 5, 2012 by Paul Frederick Schaffner), though these headers consist of thousands of individual xml files.

I was also able to download, from a source I no longer recall, a csv of title information for all TCP works (including EEBO, Evan American, and ECCO); this csv includes years for me to filter out the 90s, but no publisher information that would allow me to focus on England.



To guess at the number of titles per year, I searched PhiloLogic4, which showed 511 titles for the range 1789-99. I searched individual years and the numbers added to 511, indicating that each work is only associated with one year.

The UMich site doesn’t allow this kind of search, even in the “Bibliographic” search mode --- the closest it can come is selecting specific decades from a dropdown.



Each volume has its own entry.  

**The first step was to make a “collection” in the HathiTrust system.**

To search the catalog based on metadata is the best way to find what I want, but to add works found in that way, I would have needed to open each individual file. The catalog search returned 4,026 items.

So instead I did a “full-text search” for a bogus search term but with my date range, added a restriction to England, and removed the search term. This method returned 8,220 full-text results, which I could add to my collection 100 at a time. I confirmed that I’d gotten them all by checking that the final collection also had 8,220 items.

HathiTrust also suggests contacting them to make custom collections --- which I will do, in part to see if it has a different result that the one I made manually.



I then downloaded the metadata for this collection as “Tab-Delimited Text (TSV)”

To count how many titles are included per year, I opened in OpenRefine and filtered “date” by each year in sequence, to see how many results were returned



Each volume has its own record.  

I attempted to find information about what is contained in Google Books, and rapidly discovered that there are no ways to search via bibliographic data, (though a search for a keyword can be filtered by publication date), nor are there any ways to identify how many titles were returned for a given query. Slightly more information was available for information about the Ngram viewer which was, **as of 2013**, not based on the ordinary Google Books holdings, but rather on specialized pre-selected and pre-processed corpora, created in 2009 and 2012 (“Google Books Ngram Viewer: Info”).

I’m not planning to download any Google Ngram data, I just want to know what (if anything) can determine about its sources, to shed light on use of the Ngram viewer.



“Below are descriptions of the corpora that can be searched with the Google Books Ngram Viewer. All corpora were generated in either July 2009 or July 2012; we will update these corpora as our book scanning continues, and the updated versions will have distinct persistent identifiers. Books with low OCR quality and serials were excluded” (“Google Books Ngram Viewer: Info”)



Possibly containing stuff 1789-99 (“Google Books Ngram Viewer: Info”)

* “English One Million” - “The "Google Million". All are in English with dates ranging from 1500 to 2008. No more than about 6000 books were chosen from any one year, which means that all of the scanned books from early years are present, and books from later years are randomly sampled. The random samplings reflect the subject distributions for the year (so there are more computer books in 2000 than 1980).”
* “British English 2012” / “British English 2009” - Books predominantly in the English language that were published in Great Britain.
* “English 2012” / “English 2009” - “Books predominantly in the English language published in any country.”
* “English Fiction 2012” / “English Fiction 2009” - “Books predominantly in the English language that a library or publisher identified as fiction.”


“Compared to the 2009 versions, the 2012 versions have more books, improved OCR, improved library and publisher metadata. The 2012 versions also don't form ngrams that cross sentence boundaries, and do form ngrams across page boundaries, unlike the 2009 versions. With the 2012 corpora, the tokenization has improved as well, using a set of manually devised rules (except for Chinese, where a statistical system is used for segmentation). In the 2009 corpora, tokenization was based simply on whitespace.” (“Google Books Ngram Viewer: Info”)

“Many more books are published in modern years. Doesn't this skew the results? It would if we didn't normalize by the number of books published in each year.” (“Google Books Ngram Viewer: Info”)





Also consult “Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution” for more info  

Project Gutenberg does not provide publication information about works (except for info about the publication of the ebook), so in order to identify which were published in England between 1789 and 1799, I would have to use my other corpora as a guide.

\[Is this something 18thConnect has done at all? Or would be interested in doing...?\]



Nonetheless it is possible to download “catalogs” of everything included in PG, in both plain text and XML and MARC versions. I downloaded plaintext lists for PG and PG-AUS (from “Offline Catalogs.”) and the XML catalog (from “Feeds”) on June 7, 2020.  

The columns are: “Type of resource” (“Monograph” or “Serial”); “ESTC citation number”; “Name” (e.g., of an author, editor or illustrator); “Dates associated with name” (generally, the years they lived); “Type of name” (“meeting/conference,” “organization,” or “person”); “Role” (e.g., “author,” “cartographer,” or “bookseller”), “All names”, “Title”, “Variant titles”, “Place of publication”, “Publisher”, “Date of publication” (a single year), “Date of publication (not standardised)” (e.g., a year in roman numerals, or a date which includes a month or day), and “Publication date range” (for serials). In other words, it includes the very basic information of author, title, publisher, and year, in a complex structure which belies the apparent simplicity of these “basics.” Some of the ESTC records included in this corpus do not necessarily match my selection criteria (England, 1789-99), which is inevitably true of every corpus collected, and which I discuss in more detail in SECTION, Data Cleaning.  

When working with large literary databases, scholars naturally wish to know what kind of literature they are working with. A high incidence of religious language in an eighteenth century corpus, for example, will mean something different depending on how many of the documents included in the corpus are sermons. However, the larger and more heterogenous the database, the less likely it is that all of its contents have been thoroughly and consistently identified. Eighteenth century texts predate systems such as the Library of Congress subject headings, so many of the holdings in widely-used resources such as the English Short Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), and ECCO Text Creation Partnership (ECCO-TCP) include no metadata regarding their contents’ subject matter.  

In designing this tool, I want to take advantage of one of the unique features of eighteenth century literature: long and descriptive titles. In most periods of literary history, the information that gets input in the “title” field of a database offers a scholar very little to work with. Eighteenth century books were advertised and sold without any visual cues as to their contents; they attracted readers through the strength of their title pages alone. Bibliographers have consistently chosen to record a large amount of this title page information. Their lengthy titles therefore intentionally communicate their genres, and, as my paper demonstrates, their messages can be distilled through topic modelling. I present my results for the 52,000 titles printed in England between 1789 and 1799, and discuss the resulting ontology of generic categories. As I demonstrate, inferring genres based on how texts describe themselves produces a very different picture of the categories of textual production than the post-facto descriptions that later generations of scholars have produced. I am then able to use these subject categories to compare the contents of several commonly-used digital resources, to identify how representative of eighteenth century literature as a whole each database is. Finally, I discuss how my methods might be adapted to address similar gaps in metadata in related fields.  

Which of these archives are the most "reliable", and which the most "distorted"? (obvs interrogate this framework)

* What’s *in* all these, anyway?
    * What does ECCO-TCP leave out compared to ECCO? Compared to ESTC? (Can I come up with adjustment factors?)
    * How do digital vs physical holdings compare?  

**Comedies vs tragedies performed**: the ratio of comedies to tragedies performed was an astonishing 14 to 1 in Paris (Theatre, Opera, and Audience in Revolutionary Paris: Analysis and Repertory by Emmet Kennedy, Marie-Laurence Netter, James P. McGregor, and Mark V. Olsen)

**Suarez numbers**

Some Statistics on the Number of Surviving Printed Titles for Great Britain and Dependencies from the Beginnings of Print in England to the year 1800, by Alain Veylit.   





  

“The Project staff found that many eighteenth-century books in hundreds of libraries around the world have never been / catalogued at all, or are described in a group heading,” especially for single-sheet items (Korshin 210-211). “Panizzis ‘Rules' lead to confusing entries and filing for anonymous entries or for items with corporate authorship. For these, and many other related reasons, Alston and Jannetta decided to write their own cataloguing rules, allowing their entries eventually to be converted into machine-readable form, but differing slightly from the standards for machine-readable cataloguing devised in this country (Library of Congress MARC) and in the United Kingdom (UK MARC). Modern machine-readable cataloguing has been devised to deal with cataloguing new books and serials; the ESTC's cataloguing rules have been devised in such a way that a scholar anywhere in the world can tell, from the ESTC entry, whether the copy of the book in his or her library is the same or different from the one listed in ESTC.” (Korshin 211)



“In ESTC the matching process hinges on five points of identity: the title, as far as it is given by ESTC; the edition statement; the imprint, again as far as it is given by ESTC; the pagination; and the format” (Tabor 370).  

Using the database:

“the needs of the specialist audience that has formed ESTC’s main constituency in the past and may reasonably be expected to continue as such: the explorers in the field who need detailed maps. In my task of mediating between the file and its users, I find that these people approach early books with questions regarding one or both of two broad topics: intellectual content and physical characteristics, the latter including the location of copies,” with most interested in “the physicality of books.” (Tabor 369)

“When everything is working right, as it does in ESTC more often than not, the bibliographical description is completely accurate as far as it attempts to go, and variations in the matched copies can be reliably determined by the presence or absence of copy-specific notes. If anything happens to disturb the links between the description and its attached holdings, including errors committed in the course of creating the description, the record gets broken and starts telling lies about physical copies or, worse, about the ideal copy” (Tabor 370)



“the ESTC is one of the best resources to identify relevant printed materials, determine their publishing histories, and find out where they can be examined” (Karian 283)  

The ESTC is not actually only one database: “The STAR file[ is this still true?], maintained at the ESTC editorial office in Riverside, California, has been functioning for years as a repository for revisions that are transmitted in periodic updates to the publicly accessible file. ... In consequence, though invisible and inaccessible to most of ESTC’s users, STAR contains the most up-to-date version of the file.” (Tabor 373) There are two kinds of information, in particular, which the STAR file contains and which is concealed from ESTC users: the true ‘verified’ status of individual edition matches, and edition-specific notes on known errors.

A “match” in the ESTC is a known copy of a book held in a library which serves as a representative of a particular edition. A match can be encoded as “verified” when an ESTC staff member consults the book to confirm that it corresponds to the edition in question, or “unverified” when this has not happened. But the STAR copy contains two additional verification statuses: when a contributor such as a librarian or a scholar submits a match, it is coded as a “web match.” “Because most outside contributors have received no training in the matching process from ESTC, it would be fair to say that these matches occupy a level of certainty somewhere between the ‘verified’ and ‘unverified’ levels” (Tabor 374). Automated uploads of various outside databases can also generate new matches, encoded as “catalogue matches;” these “have a comparatively low level of reliability” (Tabor 374). However, in the transition from STAR to ESTC, “The standard holdings display smoothes over these nuances; here both catalogue and web matches are translated to ‘verified’. This forces the file to make categorical statements of certainty even in obvious cases of considerable doubt” (374).

\[STAR annotations and DFONOTEs\]  

Transcription errors



Broken records

“A broken record occurs when information added to a record has not been checked against the copies already listed under that record” (Karian 285)



Inaccurate dates

‘The ESTC sometimes records the date as questionable, and sometimes records the date within a range. But when one does a large-scale search for records---for example, everything from 1720---there is no easy way to screen out items not definitively dated to 1720” (Karian 291)



“only records bibliographic information about surviving books ... After a careful study of book advertisements, inventories and bibliographies, it seems to this investigator that, excluding jobbing and newspaper printing, as much as 10 per cent of the printed record from 1701--1800 has not been incorporated into the ESTC. In other words, for up to 10 per cent of the editions printed in the eighteenth century, not a single copy is known to survive.” (Suarez 40)  

As Suarez notes, the ESTC is a unique resource for pre-19thC works: “Regrettably, although this volume of The Cambridge history of the book in Britain ends in 1830, it is not possible to perform a similarly comprehensive analysis for the first decades of the nineteenth century because we have no equivalent bibliographical control for this period” (Suarez 40).  

“To use a metaphor, some people prefer to explore the world through books of photographs with occasional schematic maps. ESTC, on the other hand, provides the equivalent of a detailed topographic map, but no pictures. Such technical tools have limited appeal, even to some specialists; but if you want to thoroughly learn the lie of the land, you will need one, and the more complete and accurate the better.” (Tabor)  

“An increasingly common trend, I am sorry to report, is that more and more people do not want ESTC at all --- they want ECCO or EEBO. The younger generation of scholars in particular, lured by full-text images and ransacking the Web for illustrations for their books and articles, are using these utilities as de facto bibliographic databases. They find that the stripped-down records and simplified indexes are good enough for their purposes. To a minority of them, the fact that other works, editions, and copies exist outside the Web is irrelevant, and perhaps even irritating.” (Tabor 368)  

The story of ESTC is one in which a closed ideal of authoritativeness is being relinquished slowly for the sake of more useful and modern qualities: evolution, accuracy, and depth. The printed versions of STC and Wing presented themselves as definitive bibliographies of English printing for the periods that they covered. Being native to the digital medium, by contrast, the nature of ESTC is that it is always a work in progress. What David McKitterick calls the ‘dispersed authority’ of the ESTC is represented by the existence of its daimon, a parallel database accessible only to the editors and librarians authorized to work on it, known as the STAR file.15 This is the repository for all updates and queries, all doubts about the reliability of edition designations and library matches, and for some time now commentators have been calling for it to be made accessible to ordinary users.16 At the time of writing, the ESTC21 blog indicates that this will soon happen: the catalogue is about to be redesigned so as to ‘harness user expertize to enrich data’.17 The database will function increasingly as an ‘electronic hub’ capable of searching digital reproductions of texts, and moves will be made to improve the accessibility of ESTC data for recapture by other websites.18 This brings us to the second reason for the ESTC’s preeminence. The hybridity of the resource ought to be its great weakness. The description of a typical copy from an edition in the upper part of its record is really a sort of ideal generalization, a specification that in fact describes many nonidentical objects. A thousand contingencies at the warehouse or the bookseller’s stall, at the binder’s shop or the library lead inevitably to variations within editions or impressions. So the authority of the upper record is under constant pressure from the material particularity represented by the lower part, the copies registered in the union catalogue. In fact, it is the pragmatic yoking of these two theoretically incompatible classes of information that gives the ESTC its versatility. The balance between ideal description and material referent is successful because this is true to ESTC’s nature as a bibliographical workhorse, focused on the business of practical specification. For my own part, I would be uncomfortable seeing the ESTC moving into the realm of content provision or text searching, because that balance and that focus would both be lost. They are essential to the identity of this indispensible public resource. (Bullard 750)  

Other work which has used the methodology of sampling includes 



Suarez: “Lacking the resources to conduct a detailed analysis of the entire ESTC from 1701 to 1800, I have resorted to sampling. Electing to examine all eighteenth-century records that appear in years ending in three -- 1703, 1713, 1723 and so on -- I have sought to avoid a number of cohort effects, most especially the cumulation of indeterminate records into years ending in ‘0’ or ‘1’ and, to a lesser degree, ‘5’.” (41)  

I created a “content set” called ECCO-1798 in Gale Digital Scholar Labs, by searching for all works in ECCO published “between” 1798 and 1798. This located 4,158 records. I downloaded the metadata for these 4,158 records as a CSV. I then used random.org’s “Random Integer Generator” to generate ten integers from 1 to 4,158 (inclusive), resulting in the following numbers: 1792, 2365, 159, 3511, 919, 170, 2136, 2259, 190, and 2242. I looked up the ten works appearing in those rows of the spreadsheet, without altering the order of the records from Gale’s default. (It is unclear to me what sorting method was used to organize them in the document.) I used the Gale Content Numbers of these ten works to create a new “content set” of just these ten titles.  

159 - Poetry; original and selected	Monograph	Monograph	Literature and Language I.	\[1796-98\]	Gale	London, United Kingdom		British Library	null	GALE\|CW0115892706	

170 - Sir, You are desired to meet the committee for improving the navigation of the River Thames, and for preventing encroachments on the said river, on board the navigation barge, at Staines, on Saturday, the 7th day of July 1798, at eight o'clock in the morning, and proceed from thence down the river at nine precisely, ...	Monograph	Monograph	Social Sciences II.	\[1798\]	Gale, a Cengage Company	Oxford, United Kingdom	Great Britain. Commissioners Appointed for Improving and Completing the Navigation of the Rivers Thames and Isis	Bodleian Library, University of Oxford	null	GALE\|CB0130118850	

190 - Thoughts concerning the proper principles of finance, that ought to be adopted at present, and in future, in support of the British government. Addressed To The Freeholders And Mercantile Interest Of Leeds, Wakefield, Halifax, Huddersfield, Bradford, Doncaster, Hull, And The Other Towns In The County Of York. By a freeholder of Yorkshire	Monograph	Monograph	Social Sciences I.	1798	Gale	Lawrence, KS, United States	James Cochrane	Spencer Research Library, University of Kansas	null	GALE\|CW0107793549	

919 - National blessings considered and improved, in a sermon, preached on Thursday, November 29, 1798. By Alex. Black, Minister, Musselburgh	Monograph	Monograph	Religion and Philosophy I.	1798	Gale	London, United Kingdom	Alexander Black	British Library	null	GALE\|CW0123387895	

1792 - False impressions: A comedy in five acts. Performed at the Theatre Royal, Covent Garden. By Richard Cumberland, Esq.	Monograph	Monograph	Fine Arts II.	1798	Gale, a Cengage Company	London, United Kingdom	Richard Cumberland	British Library	Eighteenth Century Collections Online	GALE\|CB0129794221	

2136 - The surprizing adventures, of Jack Oakum, & Tom Splicewell, two sailors, who went a pirating on the Kings' highway. How that the first \[prize\] they took gave information of their course, and being pursued by a whole squadron, Tom Spicewell was taken and condemned to be hanged \[:\] but by means of his beloved friend Jack Oakum, who interested with his Majesty, he was pardoned.. Also a copy of Jack's polite letter to the King, on the above occasion. To which is added, The merry revenge; \[our,\] Joe's stomach in June	Monograph	Monograph	Literature and Language I.	1798	Gale	Oxford, United Kingdom		Bodleian Library, University of Oxford	null	GALE\|CW0116560788

2242 - The American preceptor; being a new selection of lessons for reading and speaking. Designed for the use of schools. By Caleb Bingham, A.M. author of The Columbian orator, Child's companion, &c. \[One line of quotation\] Published according to act of Congress	Monograph	Monograph	Literature and Language II.	1798	Gale, a Cengage Company	Boston, MA, United States	Caleb Bingham	Boston Public Library	null	GALE\|CB0130828924	

2259 - The letters of Junius	Monograph	Monograph	Literature and Language I.	1798	Gale	Cambridge, MA, United States	Junius	Houghton Library, Harvard University	null	GALE\|CW0110410930	

2365 - Gil Blas corrigé; ou histoire de Gil Blas de Santillane. Par M. Le Sage. Dont on a retranché les expressions & passages contraires à la décence, ... & à laquelle on a ajouté un recueil de traits brillans, des plus célèbres poëtes françois. Par J. N. Osmond. ...	Monograph	Monograph	Literature and Language I.	1798	Gale	Oxford, United Kingdom	Alain René Le Sage	Bodleian Library, University of Oxford	Eighteenth Century Collections Online	GALE\|CW0116687637

3511 - A Plain narrative of facts respecting the trial of James Coigley; Including his letter to an Irish gentleman, in London, and A. Young's letter to G. Lloyd	Monograph	Monograph	Law II.	1798	Gale, a Cengage Company	Cambridge, United Kingdom		University of Cambridge Library	null	GALE\|CB0132172434	  

1053 - Le juge à paix, et officier de paroisse, pour la province de Quebec. Extrait de Richard Burn, chancellier du diocèse de Charlisle, & un des juges à paix de Sa Majesté, pour les comtés de Westmorland & Cumberland. Traduit par Jos. F. Perrault	M.DCC.LXXXIX.\[1789\]	London, United Kingdom	Richard Burn	

2464 - An apology for professing the religion of nature, in the eighteenth century of the Christian aera; addressed to the Right Reverend Dr. Watson, Lord Bishop of Landaff	MDCCLXXXIX. \[1789\]	Oxford, United Kingdom	David Williams

1211 - Some account of the discovery, made by the late Mr. John Dollond, F. R. S. which led to the grand improvement of refracting telescopes, in Order to Correct some Misrepresentations, in Foreign Publications, of that Discovery: with an attempt to account for the mistake in an experiment made by Sir Isaac Newton; on which Experiment, the Improvement of the Refracting Telescope Entirely Depended. By Peter Dollond, Member of the American Philosophical Society at Philadelphia	 M.DCC.LXXXIX. \[1789\]	London, United Kingdom Peter Dollond	

1086 - Reflections on the contentions and disorder of the corporation of Cambridge	M.DCC.LXXXIX. \[1789\]	Oxford, United Kingdom

51 - Nécessité de supprimer et d'éteindre les ordres religieux en France, prouvée par l'histoire philosophique du monachisme, ...	1789 London, United Kingdom

938 - Emma Dorvill. By a lady	MDCCLXXXIX. \[1789\]	London, United Kingdom

629 - Virtue and peace forever inseparable. A discourse at the interment of Capt. John Howard, of Hampton, June 17th, 1789. By Joseph Huntington, D.D. \[Three lines from Young\] A few thoughts appear in the copy which, for the sake of brevity, were omitted in preaching M.DCC.LXXXIX. \[1789\]	Washington, DC, United States	Joseph Huntington

1524 - A practical treatise on the gonorrhoea, and on the superior efficacy of the cure by injection. By Peter Clare, surgeon \[1789\] Boston, MA, United States	Peter Clare	

702 - Analyse raisonnée de la sagesse de Charron. premiere partie	M.DCC.LXXXIX. \[1789\]	Manchester, United Kingdom	Jean-Pierre-Louis de Luchet

1732 - Two discourses. I. On wisdom attainable by meditation of the vanity of human life ... II. Men more influenced by example than precept; ... Preached ... March 8, 1789, by the Reverend Samuel Hopkinson, ...	\[1789\]	London, United Kingdom	Samuel Hopkinson  

I could end by describing what Bode-like scholarly edition I’d like to make. What would it be?

Datasets of men’s, women’s, and unsigned writing from the 1790s? Filtering to fiction, or poetry? Or sermons?? Non-literary writing? Things that people might want to teach?

Circulating libraries? Maybe one specific library, to be feasible?

Reprints??  

A collocation formula like “ix,\[3\],68\[i.e. 60\]p. ; 4⁰” (the physical description of Smith’s T*he Emigrants* provided in the ESTC, ECCO, and ECCO-TCP) is no more transparent and obvious in meaning than the following markup:

\<listPrefixDef\>

\<prefixDef ident="tcp"

matchPattern="(\[0-9\\-\]+):(\[0-9IVX\]+)"

replacementPattern="https://data.historicaltexts.jisc.ac.uk/view?pubId=ecco-$1&index=ecco&pageId=ecco-$1-$20"/\>

\<prefixDef ident="char"

matchPattern="(.+)"

replacementPattern="https://raw.githubusercontent.com/textcreationpartnership/Texts/master/tcpchars.xml#$1"/\>

\</ListPrefixDef\>

Indeed, the collocation formula is less transparent than simple markup like the following:

\<titleStmt\>

\<title\>The emigrants, a poem, in two books. By Charlotte Smith\</title\>

\<author\>Smith, Charlotte Turner, 1749-1806.\</author\>

\</titleStmt\>

It may even compare unfavorably to relatively well-commented code, like the following:

| \# iterate through the directory ||
|  | for filename in listdir\_nohidden("./" + directory): |
| -----: | :----- |
|  |  |
|  | \# define the path to this file |
|  | path = "./" + directory + "/" + filename |
|  |  |
|  | \# strip the file's namespace |
|  | try: |
|  | xmlstring = stripNamespace(path) |
|  | except: |
|  | print "error stripping namespace of file %s" % (filename) |


What these comparisons intend to illuminate is *not* that collocation formulae ought to be simpler or more accessible. Rather, my point is that specialized encoding serves a practical purpose, and that it is a matter of training which determines what encodings will seem natural and useful.  

Vincent Carretta sees a lull in abolitionist activities during the late 1790s. Do I see anything like that?  

As these examples from Bode and Underwood show, it is increasingly rare for DH scholars to present their own research with sweeping claims to unmediated truth. Indeed, beyond Moretti and Jockers, it can be hard to find the bombastic claims with which DH has become associated.[^cf36] \[TODO-RESEARCH: cite 4-5 examples, one sentence summary / one sentence quote from each.\]  

“Proprietary mass-digitized collections such as Google Books, Early English Books Online, and The British Newspaper Archive (owned by Google, ProQuest, and findmypast, respectively) are increasingly used in humanities research. But their scope and scale---let alone the histories of transmission that produce them---can be very difficult to discern; indeed, **the commercial imperatives of these enterprises arguably depend on them presenting these collections as comprehensive**.” (Bode *World* 47)  

“It set generic limits to the material and would not include: engraved material; printed forms, such as licences, warrants, certificates, etc.; trade and visiting cards, tickets, invitations, currency (although advertisements were to be included); playbills and concert programmes; playing cards, games, and puzzles (Alston & Janetta, 1978: 16--17). More significantly, it constructed its scope along geographic and linguistic lines. It would include: 

1. All relevant items printed in the British Isles in any language; 

	
2. All relevant items printed in Colonial America, the United States 

	(1776--1800), and Canada in any language; 

	
3. All relevant items printed in territories governed by Britain during any 

	period of the eighteenth century in any language; 

	
4. All relevant items printed wholly or partly in English, or other British 

	vernaculars, in any part of the world. “ (Gregg 12)

[fig-databases-venn]: fig-databases-venn.png width=349px height=286px

[CSmith-in-ESTC-ECCO-TCP-Hathi-table]: CSmith-in-ESTC-ECCO-TCP-Hathi-table.png width=184px height=312px

[CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3]: CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3.png width=244px height=198px

[ScreenShot2021-01-10at12.20.20PM]: ScreenShot2021-01-10at12.20.20PM.png width=694px height=251px

[ScreenShot2021-01-10at12.22.09PM]: ScreenShot2021-01-10at12.22.09PM.png width=693px height=262px

[ScreenShot2021-01-10at12.24.37PM]: ScreenShot2021-01-10at12.24.37PM.png width=472px height=266px

[pastedGraphic]: pastedGraphic.png

[pastedGraphic-1]: pastedGraphic.png

[pastedGraphic-2]: pastedGraphic.png

[ScreenShot2020-12-08at1.40.54AM]: ScreenShot2020-12-08at1.40.54AM.png width=515px height=502px

[ScreenShot2020-12-08at1.42.30AM]: ScreenShot2020-12-08at1.42.30AM.png width=694px height=245px

[ScreenShot2020-12-08at1.44.47AM]: ScreenShot2020-12-08at1.44.47AM.png width=50px height=187px

[ScreenShot2020-12-08at1.47.18AM]: ScreenShot2020-12-08at1.47.18AM.png width=508px height=139px

[ScreenShot2020-12-20at4.35.21PM]: ScreenShot2020-12-20at4.35.21PM.png width=312px height=68px

[ScreenShot2020-12-20at4.37.10PM]: ScreenShot2020-12-20at4.37.10PM.png width=303px height=64px

[ScreenShot2020-12-20at4.44.12PM]: ScreenShot2020-12-20at4.44.12PM.png width=304px height=347px

[ScreenShot2020-12-27at2.41.37AM]: ScreenShot2020-12-27at2.41.37AM.png width=579px height=90px

[ScreenShot2020-12-27at8.06.45PM]: ScreenShot2020-12-27at8.06.45PM.png width=586px height=72px

[ScreenShot2020-12-27at8.06.33PM]: ScreenShot2020-12-27at8.06.33PM.png width=590px height=316px

[ScreenShot2020-12-27at9.54.05PM]: ScreenShot2020-12-27at9.54.05PM.png width=659px height=411px

[ScreenShot2020-12-30at2.57.30AM]: ScreenShot2020-12-30at2.57.30AM.png width=649px height=556px

[ScreenShot2020-12-31at2.36.37AM]: ScreenShot2020-12-31at2.36.37AM.png width=658px height=356px

[ScreenShot2020-12-31at2.39.58AM]: ScreenShot2020-12-31at2.39.58AM.png width=654px height=226px

[ScreenShot2020-12-31at2.46.27AM]: ScreenShot2020-12-31at2.46.27AM.png width=656px height=556px

[ScreenShot2021-01-01at3.35.54AM]: ScreenShot2021-01-01at3.35.54AM.png width=659px height=127px

[ScreenShot2021-01-01at8.58.52PM]: ScreenShot2021-01-01at8.58.52PM.png width=654px height=156px

[ScreenShot2021-01-01at8.55.50PM]: ScreenShot2021-01-01at8.55.50PM.png width=651px height=152px

[ScreenShot2021-01-01at9.03.09PM]: ScreenShot2021-01-01at9.03.09PM.png width=660px height=185px

[ScreenShot2021-01-03at1.33.01AM]: ScreenShot2021-01-03at1.33.01AM.png width=658px height=322px

[ScreenShot2021-01-03at1.35.28AM]: ScreenShot2021-01-03at1.35.28AM.png width=646px height=523px

[ScreenShot2021-01-03at1.35.54AM]: ScreenShot2021-01-03at1.35.54AM.png width=644px height=345px

[ScreenShot2021-01-03at1.37.34AM]: ScreenShot2021-01-03at1.37.34AM.png width=651px height=177px

[ScreenShot2021-01-03at1.37.42AM]: ScreenShot2021-01-03at1.37.42AM.png width=642px height=101px

[ScreenShot2021-01-04at1.18.43AM]: ScreenShot2021-01-04at1.18.43AM.png width=684px height=710px

[ScreenShot2021-01-04at1.21.19AM]: ScreenShot2021-01-04at1.21.19AM.png width=652px height=284px

[ScreenShot2021-01-04at1.21.25AM]: ScreenShot2021-01-04at1.21.25AM.png width=655px height=49px

[ScreenShot2021-01-04at1.28.07AM]: ScreenShot2021-01-04at1.28.07AM.png width=575px height=624px

[ScreenShot2021-01-13at7.59.25PM]: ScreenShot2021-01-13at7.59.25PM.png width=869px height=418px

[ScreenShot2021-01-16at6.22.28PM]: ScreenShot2021-01-16at6.22.28PM.png width=661px height=588px

[ScreenShot2021-01-16at8.35.01PM]: ScreenShot2021-01-16at8.35.01PM.png width=669px height=160px

[ScreenShot2021-01-16at8.42.18PM]: ScreenShot2021-01-16at8.42.18PM.png width=660px height=179px

[ScreenShot2021-01-16at8.44.05PM]: ScreenShot2021-01-16at8.44.05PM.png width=665px height=340px

[ScreenShot2021-01-16at9.05.10PM]: ScreenShot2021-01-16at9.05.10PM.png width=686px height=106px

[ScreenShot2021-01-17at5.08.36PM]: ScreenShot2021-01-17at5.08.36PM.png width=674px height=516px

[ScreenShot2021-01-17at5.26.36PM]: ScreenShot2021-01-17at5.26.36PM.png width=659px height=380px

[ScreenShot2021-01-17at5.37.06PM]: ScreenShot2021-01-17at5.37.06PM.png width=667px height=155px

[ScreenShot2021-01-17at5.38.49PM]: ScreenShot2021-01-17at5.38.49PM.png width=660px height=442px

[ScreenShot2021-01-17at5.41.17PM]: ScreenShot2021-01-17at5.41.17PM.png width=687px height=704px

[ScreenShot2021-01-17at5.44.19PM]: ScreenShot2021-01-17at5.44.19PM.png width=662px height=189px

[ScreenShot2021-01-17at6.00.01PM]: ScreenShot2021-01-17at6.00.01PM.png width=672px height=499px

[ScreenShot2021-01-24at7.21.33PM]: ScreenShot2021-01-24at7.21.33PM.png width=662px height=530px

[ScreenShot2021-01-24at7.33.30PM]: ScreenShot2021-01-24at7.33.30PM.png width=650px height=158px

[ScreenShot2021-01-24at7.36.34PM]: ScreenShot2021-01-24at7.36.34PM.png width=651px height=201px

[ScreenShot2021-01-24at7.46.36PM]: ScreenShot2021-01-24at7.46.36PM.png width=648px height=273px

[ScreenShot2021-01-24at7.47.55PM]: ScreenShot2021-01-24at7.47.55PM.png width=650px height=175px

[ScreenShot2021-01-24at7.51.00PM]: ScreenShot2021-01-24at7.51.00PM.png width=647px height=229px

[ScreenShot2021-01-28at1.13.39AM]: ScreenShot2021-01-28at1.13.39AM.png width=663px height=170px

[ScreenShot2021-01-28at2.26.55AM]: ScreenShot2021-01-28at2.26.55AM.png width=674px height=359px

[ScreenShot2020-12-31at2.47.39AM]: ScreenShot2020-12-31at2.47.39AM.png width=656px height=388px

[ScreenShot2020-12-31at2.48.57AM]: ScreenShot2020-12-31at2.48.57AM.png width=665px height=568px

[ScreenShot2020-12-31at2.49.14AM]: ScreenShot2020-12-31at2.49.14AM.png width=648px height=104px

[ScreenShot2021-01-24at8.20.29PM]: ScreenShot2021-01-24at8.20.29PM.png width=424px height=106px

[ScreenShot2021-01-25at1.03.31AM]: ScreenShot2021-01-25at1.03.31AM.png width=642px height=175px

[ScreenShot2021-01-25at1.19.56AM]: ScreenShot2021-01-25at1.19.56AM.png width=639px height=210px

[ScreenShot2021-01-25at1.26.13AM]: ScreenShot2021-01-25at1.26.13AM.png width=638px height=374px

[ScreenShot2021-01-25at1.29.16AM]: ScreenShot2021-01-25at1.29.16AM.png width=649px height=508px

[ScreenShot2021-01-28at1.04.56AM]: ScreenShot2021-01-28at1.04.56AM.png width=633px height=427px

[ScreenShot2021-01-28at1.05.17AM]: ScreenShot2021-01-28at1.05.17AM.png width=633px height=427px

[ScreenShot2021-01-28at1.07.32AM]: ScreenShot2021-01-28at1.07.32AM.png width=637px height=134px

[ScreenShot2020-12-27at9.39.56PM]: ScreenShot2020-12-27at9.39.56PM.png width=592px height=228px

[ScreenShot2020-12-31at2.52.20AM]: ScreenShot2020-12-31at2.52.20AM.png width=661px height=261px

[ScreenShot2021-01-01at3.37.26AM]: ScreenShot2021-01-01at3.37.26AM.png width=651px height=175px

[ScreenShot2021-01-01at9.01.06PM]: ScreenShot2021-01-01at9.01.06PM.png width=655px height=408px

[ScreenShot2021-01-17at5.28.00PM]: ScreenShot2021-01-17at5.28.00PM.png width=673px height=158px

[ScreenShot2021-01-17at5.31.00PM]: ScreenShot2021-01-17at5.31.00PM.png width=658px height=520px

[Hu4qt]: Hu4qt.png width=600px height=255px

[ScreenShot2020-12-27at2.33.01AM]: ScreenShot2020-12-27at2.33.01AM.png width=596px height=207px

[ScreenShot2020-12-27at2.33.35AM]: ScreenShot2020-12-27at2.33.35AM.png width=591px height=125px

[ScreenShot2020-12-27at2.33.51AM]: ScreenShot2020-12-27at2.33.51AM.png width=586px height=338px

[ScreenShot2020-12-27at2.37.28AM]: ScreenShot2020-12-27at2.37.28AM.png width=596px height=227px

[ScreenShot2020-12-27at2.38.21AM]: ScreenShot2020-12-27at2.38.21AM.png width=597px height=163px

[ScreenShot2020-12-27at8.35.42PM]: ScreenShot2020-12-27at8.35.42PM.png width=594px height=315px

[ScreenShot2020-12-27at9.38.54PM]: ScreenShot2020-12-27at9.38.54PM.png width=593px height=252px

[ScreenShot2020-12-27at9.42.05PM]: ScreenShot2020-12-27at9.42.05PM.png width=587px height=230px

[ScreenShot2021-01-01at8.52.55PM]: ScreenShot2021-01-01at8.52.55PM.png width=652px height=692px

[ScreenShot2021-01-01at3.44.43AM]: ScreenShot2021-01-01at3.44.43AM.png width=649px height=251px

[ScreenShot2021-01-01at8.50.51PM]: ScreenShot2021-01-01at8.50.51PM.png width=649px height=360px

[ScreenShot2020-12-27at2.39.58AM]: ScreenShot2020-12-27at2.39.58AM.png width=602px height=295px

[ScreenShot2020-12-27at2.49.10AM]: ScreenShot2020-12-27at2.49.10AM.png width=591px height=28px

[ScreenShot2020-12-27at2.48.53AM]: ScreenShot2020-12-27at2.48.53AM.png width=599px height=42px

[ScreenShot2020-12-27at2.49.16AM]: ScreenShot2020-12-27at2.49.16AM.png width=592px height=33px

[ScreenShot2020-12-27at2.49.05AM]: ScreenShot2020-12-27at2.49.05AM.png width=590px height=34px

[ScreenShot2020-12-27at7.50.54PM]: ScreenShot2020-12-27at7.50.54PM.png width=589px height=205px

[ScreenShot2020-12-27at8.11.55PM]: ScreenShot2020-12-27at8.11.55PM.png width=592px height=69px

[ScreenShot2020-12-27at8.19.58PM]: ScreenShot2020-12-27at8.19.58PM.png width=582px height=270px

[ScreenShot2021-01-26at12.50.20AM]: ScreenShot2021-01-26at12.50.20AM.png width=665px height=553px

[ScreenShot2021-01-28at2.21.02AM]: ScreenShot2021-01-28at2.21.02AM.png width=666px height=267px

[ScreenShot2021-01-28at2.45.49AM]: ScreenShot2021-01-28at2.45.49AM.png width=589px height=85px

[ScreenShot2021-01-28at3.29.06AM]: ScreenShot2021-01-28at3.29.06AM.png width=604px height=191px

[ScreenShot2021-01-29at7.06.54PM]: ScreenShot2021-01-29at7.06.54PM.png width=601px height=193px

[ScreenShot2021-01-29at7.43.04PM]: ScreenShot2021-01-29at7.43.04PM.png width=598px height=212px

[ScreenShot2021-01-29at8.14.05PM]: ScreenShot2021-01-29at8.14.05PM.png width=606px height=253px

[ScreenShot2021-01-29at8.19.59PM]: ScreenShot2021-01-29at8.19.59PM.png width=615px height=164px

[ScreenShot2021-01-29at9.44.33PM]: ScreenShot2021-01-29at9.44.33PM.png width=601px height=215px

[ScreenShot2021-01-28at2.49.22AM]: ScreenShot2021-01-28at2.49.22AM.png width=578px height=297px

[ScreenShot2020-11-20at11.58.31PM]: ScreenShot2020-11-20at11.58.31PM.png width=768px height=434px

[juxta-emigrants-p1]: juxta-emigrants-p1.png width=145px height=99px

[juxta-emigrants-histogram]: juxta-emigrants-histogram.png width=72px height=155px

[fig-ecco-emigrants-p1]: fig-ecco-emigrants-p1.png width=100px height=161px

[hathi-emigrants-p1]: hathi-emigrants-p1.png width=148px height=192px

[^cf1]: More specifically, these “authors” are “Great Britain, Parliament,” “Great Britain,” “Great Britain, Parliament, House of Commons,” “Great Britain, Lords Commissioners of Appeals in Prize Causes,” and King George III. After King George comes Thomas Paine and Hannah More, and then it’s “Great Britain, Parliament, House of Lords” and “Church of England.”

[^cf2]: Google Books and Project Gutenberg are not, of course, traditionally “scholarly” resources, but that is why they form an informative contrast with the other resources examined.

[^cf3]: This calculation is carried out by manually examining the metadata of the six corpora I have acquired.

[^cf4]: This calculation is carried out by a small, simple program I am writing, described in Appendix A. Because the program just simplifies a straightforward process of counting, it is only lightly theorized in the dissertation itself.

[^cf5]: I already know that ‘titles per year’ are distributed fascinatingly differently between ECCO, ECCO-TCP, ESTC, and HathiTrust

[^cf6]: This calculation is carried out by a larger, more complex program I am writing, applying topic modelling to the titles of works. Because it makes several major interpretive choices, it is theorized and discussed in detail when it is applied.

[^cf7]: ‘Too much’ and ‘too little’ are here, of course, defined from the point of those with cultural capital which they wish to maintain.

[^cf8]: Part of Guillory’s argument is that, although the rhetoric of the canon debates generally sought to re-value authors of any number of oppressed categories, often using the phrase “gender, race, and class” as a single unit, the work undertaken was in fact unable to address class, since class operates differently from gender and race.

[^cf9]: Appendix B (“Methodology”) contains many examples of these algorithmic procedures executed by the human researcher and the computational programs in concert. The act of writing a program is an iterative process of delegation.

[^cf10]: Indeed, Buurma notes, “There are good reasons, of course, that scholars and journalists like to begin with Busa: he was the first concordance-maker to automate all five stages of the process, in 1951,” and he intentionally foregrounded and publicized the innovative nature of his work. \\cite\{Buurma:2018wt\}

[^cf11]: In the interest of preserving this history of citation, the students were Mary Jackman and Helen S. Agoa, credited on the cover of the published Dryden index. (Miles herself attached her name only to the preface.) From the computer lab staff, Miles particularly thanked Shirley Rice, Odette Carothers, and Penny Gee.

[^cf12]: It may also be the case, of course, that even fields with a long history of graphical display would benefit from greater scrutiny of the evidence they use; see: the Data Dinosaur. But this is beyond the remit of what an English PhD can address.

[^cf13]: I cite Tufte and Cairo as the thinkers whose design philosophies best accord with my own current understanding of the work and craft of persuasive data visualization, but my actual practical training as a graphic designer is indebted to Judith Galas, Sonia Davis Gutiérrez, and Tom Hapgood.

[^cf14]: Tufte is careful not to blame the engineers for being better at engineering and systems analysis than they were at design: rather, this example shows that design is a skill that involves expertise; when designs matter, people with that expertise need to be involved.

[^cf15]: 1. show comparisons, contrasts, differences 2. show causality, mechanism, explanation, systemic structure (intervention relies on manipulable causality -- can't do anything with the information without causality) 3. show multiple variables (3 or more) -- the world is multivariate 4. \*completely integrate\* words, numbers, maps, graphics, etc, etc. Provide information at exact point of need 5. documentation must thoroughly describe evidence and its sources, provide complete measurement scales 6. presentations succeed based on their content. for better presentations, get better content.

[^cf16]: Kath Bode and Leah Price have both described at length how textual editing and anthologizing, respectively, are literary methods of sampling. See: Bode, Katherine. *A World of Fiction: Digital Collections and the Future of Literary History*. University of Michigan Press, 2018. And Price, Leah. *The Anthology and the Rise of the Novel: From Richardson to George Eliot*. Cambridge UP, 2000.

[^cf17]: Because Bode is examining only one database, she is able to present a single date on which her data collection ceased. I have not been able to accomplish this, but for any given resource, will identify the date of that resource’s “snapshot.” This approach means that my observations may be out of date from the moment I make them, though one of my findings in chapter 2 is that many databases are currently changing more slowly than one might expect.

[^cf18]: Although these events, of course, did not occur on January 1 or December 31, respectively, the entirety of 1789 and 1799 are both included in my study, out of sheer technological necessity.

[^cf19]: (Harper 2016; Jacsó 2008; Weiss 2016) (CITE Mike Sutton and Mark D. Griffiths)

[^cf20]: CITE http://languagelog.ldc.upenn.edu/nll/?p=1701

[^cf21]: I have heard it quipped more than once in digital humanities gatherings that you always think you’re going to get your texts from somewhere else, but Project Gutenberg is where you’ll actually get them.

[^cf22]: For example, it might be able to acquire a text document with all of the words of a novel, but sorted into alphabetic order: such a text file can be used for some analyses based on word-frequency, but cannot be read. Or, it might be possible to find collocations of where a given word appears, but with only a limited number of words of context on either side of the term in question. Or, scholars can run pre-written code provided by HathiTrust to carry out things like topic modelling on the full, intact texts of their chosen works, but without being able to inspect those texts or run their own code on them. All of these modes of analysis make research much more difficult to carry out, and nearly impossible to verify. In the study of contemporary copyrighted literature, however, even these very limited tools for corpus analysis are valuable.

[^cf23]: I have heard it quipped more than once in conferences sessions that you always *think* that you’re going to get your texts from OCR, but you always *do* get them from Project Gutenberg.

[^cf24]: I am particularly excited to explore “false advertising” in titles!

[^cf25]: Byron being, of course, the exception. Bode turns to William St. Clair to support this claim, and *The Reading Nation in the Romantic Period*. Cambridge: Cambridge University Press, 2014.

[^cf26]: Volumes 4 and 5 of *Letters of a Solitary Wanderer* are in fact part of the same bibliographic record as the first three volumes. The publication date for the combined five-volume work is listed as “1800-1802.”

[^cf27]: Several of HathiTrust’s records provide “mixed copies” like this, with some volumes scanned from one library’s holdings and other volumes scanned at another. If there is overlap, multiple scans will be provided for the duplicated holdings. Nonetheless, all of these scans are tied to a single unified MARC record, taken from only one of the holding library (with no indication of which library provided it).

[^cf28]: Later known as Primary Source Microfilm, an imprint of the Gale Group.

[^cf29]: Later discussion of Project Gutenberg will more fully explore the implications of this textual selection.

[^cf30]: As an example of the frustrating impacts of the need to brand services as universally useful: I attended a daylong university workshop about the new Gale Digital Scholars Lab in the company of a medievalist who arrived, discovered that no medieval materials were available in the Digital Scholars Lab, nor even the Early English Books Online collection, and spent the morning awkwardly listening to speakers effuse about the incredible scope of U of T’s subscriptions.

[^cf31]: “Because ESTC is a bibliographical database rather than a catalogue, strictly speaking, its records describe groups of copies,” such as editions, “rather than specific copies,” such as the Exeter Book (Tabor 369).

[^cf32]: “The first problem relates to the unit of classification. A clearly defined unit is necessary to ensure that a study of change over time is reliable and based on consistent terms. What is the unit that the ESTC uses? Scholars sometimes answer by using the terms “edition,” “issue,” or “title” interchangeably. But since the ESTC does not rely in a consistent manner on any of these terms for its unit of classification, one should refer instead only to the ESTC record, a unit created by the ESTC and having no meaning outside the ESTC.” (Karian 289)

[^cf33]: Technically, in the “##” sequence, the first “#” encodes that the work is a first edition (as opposed to a “2” for an “intervening” edition or a “3” for the “current” most recent edition), and the second “#” doesn’t encode anything. That position in the MARC record is undefined, with no possible meanings, and simply always contains a ‘blank’ #.

[^cf34]: One exception to this assumption has to do with treatment of the character ſ, which the TCP file modernizes to an s, but which HathiTrust renders as ſ. To avoid penalizing HathiTrust for “inaccuracy” when it is actually a more accurate reproduction of the page than my reference point, I amended every instance of ſ in HathiTrust to an s.

[^cf35]: Leaving the ſ characters unchanged in the HathiTrust document resulted in a .29 change from base (71% accuracy), so my normalization of ſ to s had a major impact on the comparison. I consider the .09 result more appropriate than the .29 because the normalized copy better reflects how an OCR file would be used.

[^cf36]: Even Moretti and Jockers do not go as far as the news media would like them to. \[TODO-RESEARCH: support claim that critiques of DH often respond implicitly to news coverage, not scholarship itself.\]