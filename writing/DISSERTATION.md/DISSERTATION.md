Title: DISSERTATION  
Author: Lawrence Evalyn

# ch 1 - intro #



## 1.1.  intro ##  

According to the English Short Title Catalogue (ESTC), the most popular English authors of the 1790s were Thomas Paine, Hannah More, John Wesley, and William Shakespeare. Of course this inflammatory claim immediately falls apart on further scrutiny. In fact, by the metric of ‘unique entries in the ESTC database,’ the most popular author of the decade is by far Great Britain, followed by Great Britain, Great Britain, Great Britain, and King George III.[^cf1] Paine, More, Wesley and Shakespeare are only able to rise to our notice if we intervene in the dataset to filter out all authors whose names contain the phrase “Great Britain”; otherwise, Shakespeare is outnumbered by the House of Lords and by the Church of England. These claims demonstrate that a poorly-formed question will produce a useless and stupid answer even (or perhaps especially) if computation is used to answer it. This dissertation is dedicated to the formulation of better questions. I am interested in the limits of the generalizations that we make, both in “distant reading” research and in non-digital scholarship[ , which still frequently relies on claims that a given work was “popular” because it went through a certain number of editions, or the author was paid a certain amount, and so on. These generalizations break down in part because “popular,” as a concept, is overdetermined: does it mean financially successful, or widely beloved, or important? Examinations of “popularity” also break down, at close scrutiny, because of the contentious relationship between concepts of “popular” and “literary”: important literature should have some claim to cultural relevance, but it shouldn’t be *too* popular or it becomes suspect. Nonetheless, \[TRANSITION\]]. I take as my starting point the contention that, in order to identify what is “popular” or “important,” we must also understand what is normal. At its core, my question is: given that it is not possible to read everything (or even most things), how do we, and how *should* we, determine what to read, preserve, study, and teach? This “question” is, of course, many questions: what we do is by no means what we *should* do; what we read is not necessarily what we study or teach. It is also an old, nearly an old-fashioned question. The current moment of self-reflection in the field of Digital Humanities, however, provides a timely reason to revisit it. Even literary scholars who do not carry out “Digital Humanities” research are impacted by the corpus-building choices of major digital resources, since all literary research is now mediated at some level by search algorithms and databases, even if this mediation is as small as looking up the holding libraries for physical copies of texts. It is therefore relevant to the field as a whole if, as I contend, corpus-building has become the new canon-building: an invisible and naturalized process of selecting texts for idiosyncratic and historically-specific reasons, and then treating those individual texts as ideal representatives of an imagined “whole” of literature.   

Despite the crucial importance of corpus-building to the interpretation of “distant reading” research, it is often extremely difficult to know what is in a corpus. Even large institutional resources used by many scholars provide little context for their choices of what to include or exclude. These hidden choices are particularly problematic when historical selection factors might have led to the creation of databases which re-create social inequalities. I focus specifically on writing printed in England between 1789 and 1799, to explore how works from this eleven-year “decade” have been selected as important, literary, or popular. For this period, the English Short Title Catalogue provides basic bibliographic data for nearly 52,000 titles, but the Eighteenth Century Collections Online Text Creation Partnership corpus of XML-encoded full texts includes fewer than 500 titles. This difference raises the question: why were the other 51,500 titles *not* considered worth the investment of scholarly effort? And with particular urgency: do the most invested-in resources underrepresent women? My experiments examine six major databases to answer these questions: The English Short Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the Eighteenth Century Collections Online Text Creation Partnership (ECCO-TCP), Google Books, Project Gutenberg,[^cf2] and HathiTrust. For each database, I download their holdings identified as printed in England 1789-1799.[^cf3] I identify how many titles the database attributes to each year. I calculate how many works are attributed to male, female, or unknown authors.[^cf4] These very simple pieces of information, when they differ widely between databases,[^cf5] provides the basis for an initial analysis of the assumptions and limitations of each database. I then examine the contents of each database more closely, to compare the inclusion of broad categories of writing like poetry, drama, prose fiction, and ephemera.[^cf6] Identifying these categories of writing within each corpus reveals a predictable preference for “literary” forms such as novels and poetry in the smaller databases. This preference for particular kinds of writing might explain changes in gender representation of smaller databases. If the novel is the domain of women, for example, a corpus can underrepresent women by underrepresenting novels. Or it could include a representative number of novels, but disproportionately include novels by men. My investigation allows me to identify the patterns of selection. To ground my analysis in specifics, I take Charlotte Smith, Mary Robinson, Hannah More, and Ann Radcliffe as case study authors. All four authors have long histories of contentious reception, rooted in debates about seriousness, popularity, and women’s writing. I revisit them to see how their careers might be interpreted through a new lens. I do so, in part, to challenge the contrast drawn between ‘popular’ and ‘serious’ writing, especially in the historical evaluation of women’s writing as literary.  

The problem of evaluating literature is not a new or a simple one. In the eighteenth century, the debate took the form of urgently needing to distinguish ‘trash’ from ‘treasure’. Michael Gamer, in *Romanticism and the Gothic: Genre, Reception, and Canon Formation*, highlights the role of the eighteenth-century reviewer as a crucial mediator between the writers and readers of books[ Reviewers sought to dictate the social assessment of individual literary works in order to enforce morality for society at large.\[ The emerging idea of the public sphere (cf Habermas) in the eighteenth century brought with it an urgent task of literary assessment. --- this is true but maybe I don’t have to say it! and then I don’t have to waste time talking about Habermas\] \[They basically say so themselves in their reviews---I’ll quote a juicy representative one.\] ]. Importantly, although the assessments take the form of reviews of individual works, Gamer also argues that the critics’ objections are in fact “a regulatory discourse -- carried out under the fiction of paternalistic advice to a given gothic writer, but functioning as an implicit threat to other readers and writers” that affiliation with the gothic comes with “cultural costs” (42). The gothic stands in as a proxy for any kind of “popular” reading that takes place “in the absence of formal education and training” (57), so a denunciation of a gothic work becomes a reaffirmation of class-based literary hierarchies. In other words, these reviews create and affirm the cultural capital of a category of ‘serious’ literature[ The really clever bit of Gamer’s argument is that he then unpacks how Romantics, especially Wordsworth, use just enough Gothic material to sell their books while also repeating these conventional attacks on the Gothic --- they get to have their cake and eat it too, pursuing both financial and cultural capital. But I don’t think that’s relevant here.]. Gamer is only concerned with the gothic and romanticism, but the overall regulatory function of literary reviewers as moral arbiters--- and the stock conventionality of their objections, which do not affect the actual production or consumption of the works attacked--- applies to most forms of writing in the period. For example, George Taylor sees the same dynamic in the theatre. In *The French Revolution and the London Stage,* he argues that, “\[c\]ritics might make sharp comparisons” between the many kinds of entertainments that were staged, “but little of the programme was dismissed \[by audiences\] as ‘trash', or ‘immoral', or irrelevant ‘fancy’” (3). Taylor sees the repetitive discourse of eighteenth century literary critics as proof of a larger social divide: “Disagreement as to what is trash and what is treasure suggests cultural crisis, when values are put under question by social stress or political conflict” (3). Gamer and Taylor both suggest that moral judgment of literature by its critics was driven by social friction, rather than by the aesthetic distinctions which they claimed as their motivation.  

In other words, Gamer and Taylor both affirm the key conclusion of John Guillory’s *Cultural Capital: The Problem of Literary Canon Formation*, that “in fact ‘aesthetic value’ is nothing more or other than cultural capital" (332). Guillory’s sociological history of literary canons is a well-established part of literary studies, which will take on new dimensions as I apply to to the current moment of digital databases. In the eighteenth century, he argues, the cultural capital of vernacular English literature is defined by its use within the school system to enable and restrict social mobility. English vernacular literature first begins to accumulate cultural capital in middle-class schools where it is “a substitute for the study of Greek and Latin, but with the same object of producing a linguistic sign of social distinction” (97) that would allow readers to improve and signify their social standing. The public re-assessment of literature described by Gamer and Taylor is, for Guillory, “the first crisis in the status of the vernacular canon, the problem of assimilating new vernacular genres such as the novel” (xi), which seem in danger of affording too much social mobility by offering too little literary distinction for social elites.[^cf7] The ‘solution’ is institutionalization, in which “the school becomes the exclusive agent for the dissemination of High Canonical works,” and therefore, he argues, “the prestige of literary works as cultural capital is assessed according to the limit of their dissemination, their relative exclusivity” (133). Under this system, ‘serious’ literature may not be identifiable linguistically, but it can still be identifiable by the difficulty of accessing it. This history of canonization has important implications for the field of literary study. As Guillory himself insists, if the aesthetic value of a text is determined by the social operations of class, it undermines the notion of literature itself as a category of writing distinguishable in aesthetic terms from non-literary writing[ Q for Gillespie: should I still research “the new formalist work to reclaim the literary qua the literary - and e.g. Steve Conner's - ‘nah’” ? What is useful/important to address in this work, given what I say here?]. Guillory’s book is motivated by the canon debates of the 1990s, which were driven by an urgent re-valuation of literature by women and people of colour.[^cf8] His response insists that it is untenable to conceive of the problem in terms of increasing the ‘representation’ of individual works or authors within existing systems. Instead, for Guillory problem lies in the institutionalization of literature itself. “If literary criticism is ever to conceptualize a new disciplinary domain,” he says, embedding his prescription in that “if,” “it will have to undertake first a much more thorough reflection on the historical category of literature; otherwise I suggest that new critical movements will continue to register their agendas symptomatically, by ritually overthrowing a continually resurgent literariness and literary canon” (265). In other words, assigning the cultural capital of “literature” to different works cannot change the underlying system.[ Do I need to say more about this? I feel like I’ve already spent a long time on Guillory.



Bourdieu: “‘\[T\]o deny evaluative dichotomies is to pass a morality off for a politics. The dominated in the artistic and the intellectual fields have always practiced that form of radical chic which consists in rehabilitating socially inferior cultures of the minor genres of legitimate culture. ... To denounce hierarchy does not get us / anywhere. **What must be changed are the conditions that make this hierarchy exist, both in reality and in minds. We must---I have never stopped repeating it---work** __to *universalize in reality the conditions of access*__ **to what the present offers us that is most universal.**’" (Qtd in Guillory 339-340)]  

Perhaps indicating that Guillory was correct, twenty years later, we are still debating the need for “literary criticism ... to conceptualize a new disciplinary domain” (Guillory 265), now in the context of computation. The reconceptualization of literary study itself is at the core of Franco Moretti’s coinage of ‘distant reading’: the problem for which “\[r\]eading ‘more’ seems hardly to be the solution” (“Conjectures” 55) is the problem of conceiving of *world* literature, rather than the “canonical fraction, which is not even one per cent of published literature” (55). His new methods are meant to enable literary studies to examine a new object. The field of distant reading has been moving away from Moretti himself. However, it is still shaped by the attempt to redefine the disciplinary domain of literary studies. In many cases, the new domain is no longer the “canon” but the “corpus,” a collection of texts which are studied *en masse* for macroanalytical insights. Katherine Bode, for example, in “The Equivalence of ‘Close’ and ‘Distant’ Reading,” argues that Moretti and Matthew Jockers replicate the approaches of New Criticism with their corpora, and calls for “a new scholarly object of analysis” (79) that directly examines historical and textual context of corpora as representations of “literary systems” (97). Lauren Klein, too, treats the textual corpus as the new object of literary analysis requiring curation, contextualization, and interpretation. Her critique argues that “it’s not a *coincidence* that distant reading does not deal well with gender, or with sexuality, or with race,” but also that these failings are not inevitable: “it’s not that distant reading *can’t* do this work,” she insists, “it’s that it’s yet to sufficiently do so” (n. pag.). Bode, too, despite her strong critique of distant reading as it has been practiced by Moretti and Jockers, does not blame distant reading itself. Distant-readers like Moretti and Jockers, she argues, “while claiming direct and objective access to ‘everything,’ ... represent and explore only a very limited proportion of the literary system, and do so in an abstract and ahistorical way” (78). Klein, like Bode, calls for “more corpora---more accessible corpora---that perform the work of recovery or resistance” to allow research “beyond quote ‘representative’ samples, which tend to reproduce the same inequities of representation that affect our cultural record as a whole” (n. pag.). This framing re-creates, at the cite of the corpus, the identical narratives of exclusion and representation which were previously located in critiques of the canon.  

The relocation of the debate from the canon to the corpus is not without grounds. As this dissertation will explore in depth, challenges to the technological accessibility of texts have created new hierarchies, and a new “great unread.” Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. For example, the English Short Title Catalogue records 51,965 titles printed in England between 1789 and 1799. The corpus most commonly used for DH work on eighteenth century literature, ECCO-TCP, includes only 466 titles for that same time period. What are the other 51,499 titles, why are they accessible in the ways they are, and what does it mean for digital eighteenth century studies that they are not included? Although the examination of databases prompts similar hypotheses of exclusion as in longstanding conversations about canons, digital databases do not simply replicate new canons. \[By the end of my dissertation, I will be able to state here what IS happening --- something structured by related logics of access and prestige, and related simplifications of historical complexity, and related *institutional* replication of privileged texts.. But very importantly different, too, since we don’t *read* databases.\] In a series of computational and non-computational research processes, I examine six databases of eighteenth century texts to learn about four eighteenth century authors, and I examine four eighteenth century authors to learn about eighteenth century databases. This dissertation, therefore, takes place within three scholarly conversations: the digital humanities, as an increasingly self-reflective set of practices; eighteenth century studies, and the challenges presented by the 1790s; and the frameworks of reparative reading within queer theory which seem to offer valuable resources for both. The remainder of this chapter will describe in more detail the relevant scholarship shaping my frameworks, and then introduce my chapters by introducing my four case study authors.



## 1.2.  frameworks ##  

The theoretical frameworks of this dissertation are drawn from the fields of feminist DH and queer DH, and from non-DH schools of thought which seem to offer valuable tools. My core motivating framework, as I conceptualize my work, is that of reparative reading. Eve Sedgwick’s “Paranoid Reading and Reparative Reading” persuasively describes in the dominance of paranoia in literary criticism, and attempts to sketch an alternative in what she terms reparative reading. A paranoid rhetoric of exposure and critique strikes me as the most obvious narrative to structure this dissertation’s investigation of the uneven institutional valuation of different writing. However, these obvious critiques also require rejecting many generations of sincere work by my fellow academics, without necessarily offering new discoveries of value to replace them. One experiment of this project, not yet complete, is to articulate an assessment of the limitations of contemporary digital resources which nonetheless allows those resources to be recuperated. My touchstones are two descriptions from Sedgwick’s original chapter:  

The desire of a reparative impulse... is additive and accretive. Its fear, a realistic one, is that the culture surrounding it is inadequate or inimical to its nurture; it wants to assemble and confer plenitude to an object that will then have resources to offer to an inchoate self. (149)

What we can best learn from such practices are, perhaps, the many ways selves and communities succeed in extracting sustenance from the objects of a culture - even of a culture whose avowed desire has often been not to sustain them. (150-151)

What Sedgwick describes, here, is a “desire,” not a methodology. I therefore understand “reparative reading” to refer, not to a precise set of practices, but to a position one might occupy in relation to a text. What I posit is also a desire: that my methods here can provide useful practices for others. The reparative position is a generous one, both in terms of giving of oneself to a text, and in terms of seeking a text’s strengths over its weaknesses. What I learn from Sedgwick, therefore, that *attention* is the first step toward *caring*, and that non-judgment can be more informative than rejection.  

I have mentioned moving away from critique as well as from paranoia: in rethinking the role of critique, I draw upon the work of Rita Felski, and the theories of “surface reading” described by Sharon Marcus, Stephen Best, and Heather Love. Felski, in her article “After Suspicion” and then further in her monograph *The Limits of Critique*, seeks to attend seriously to literary attachments, including our own attachments as critics. Felski’s approach to these attachments is essentially sociological, drawing heavily on Bruno Latour’s actor-network-theory, and thus involves almost no close reading. “Surface reading” positions itself as an alternative to “symptomatic reading”; rather than seeking to expose hidden truths concealed within texts, it attempts accurate descriptions that “make visible what is invisible only because it's too much on the surface of things” (Best 13). The analogues to reparative and paranoid reading are obvious, but not perfect: all paranoid reading is symptomatic, but not all symptomatic reading is paranoid[ unpack]. Reparative reading, as described by Sedgwick, is often still interested in ‘deep’ meanings of texts, in which striking textual features can be interpreted to locate additional meanings. Felski’s readings are often symptomatic in this way. In contrast, “surface reading,” as Heather Love describes, pursues “a turn away from the singularity and richness of individual texts” (374), seeking descriptions that are “complex and variegated, but not rich, warm, or deep” (378). Love’s disavowal of “richness” here is part of her attempt to move away from “the ethical charisma of the literary translator or messenger” (374) who characterizes the paranoid, critical figure that both Sedgwick and Felski also seek to escape.  

Love’s later article, “Close Reading and Thin Description,” provides a more precise articulation of the kind of close reading that she calls for, in which an “exhaustive, fine-grained attention to phenomena” (404) enables “taking up the position of the device; by turning oneself into a camera, one could---at least ideally---pay equal attention to every aspect of a scene that is available to the senses and record it faithfully” (407). Although Love is uninterested in “distant reading” as synonymous with Moretti (Love 411), this invocation of the mechanical implies, I argue, an obvious potential for computation. The actual *practice* of computational research requires a great deal of laborious, intimate encoding. The researcher must occupy a “mechanical” position of receiving inputs and responding to them consistently over time, whether entering details in a spreadsheet with a consistent taxonomy or running the same program over multiple datasets.[^cf9] Love says:

Good descriptions are in a sense rich, but not because they truck with imponderables like human experience or human nature. They are close, but they are not deep; rather than adding anything ‘extra’ to the description, they account for the real variety that is already there. (377)

A computational model is unlikely to “truck with imponderables,” but it *absolutely* *must* “account for the real variety that is already there” or else the code will simply fail to run. If you are forced to manually encode your assumptions into a system, you are forced to confront what they are. Even deleting or ignoring information is still a way of “accounting for” it in the coding process: some part of the program will have to say, in effect, ‘if I get an input that doesn’t match what I expect, discard it.’ Choosing to ignore contradictory or difficult information carries the assumption that this information does not ‘count,’ or does not matter to the question at hand. The choice faced by scholars is how to address our encoded assumptions. The encounter with variety does not in itself produce nuanced results: it is possible to selectively ignore any uncomfortable details. But it is also possible to do computation reflectively, asking not “how can I make this work the way I want?” but “where do my assumptions encounter resistance?” and turning one’s attention to the nature of the resistance. Integrating this reflection into the research process can allow a scholar to avoid both the pitfalls of “conquering” their material and of claiming an algorithmic grasp of “objective” truth.   

To bring these principles into the field of Digital Humanities by way of an example, I want to offer an alternative geneaology for the practice of distant reading itself. Rachel Buurma and Laura Heffernen provide a valuable history[ Ted Underwood’s “Genealogy of Distant Reading” presents a history of distant reading which is not for the most part centrally concerned with computers, and is therefore fundamentally distinct from concepts of “digital humanities” \\cite\{Underwood:2017uc\}. In Underwood’s history, distant reading is “a tradition continuous with earlier forms of macroscopic literary history, distinguished only by an increasingly experimental method, organized by samples and hypotheses that get defined before conclusions are drawn” (Underwood:2017uca p.29\}. Underwood “tease\[s\] out the elided social-scientific genealogy behind distant reading” \\cite\{Underwood:2017uca p.39\} to argue that the term “\[d\]istant reading was not coined to describe a radically new method. The first occurrence of the phrase, in \[Franco Moretti’s\] ‘Conjectures on World Literature,’ seems in fact to describe the familiar scholarly activity of aggregating and summarizing previous research” \\cite\{Underwood:2017uca p.9\}\[ what is your position on this? You go on to Buurma who does connect distant reading to computation - and you suggest how you will draw on similar ideas about collaboeation etc - but this underwood bit dangles\].

]of Josephine Miles as the first ‘distant reader’. Miles’ history, briefly, is as follows:

In the 1930s, as a graduate student at Berkeley, she completed her first distant reading project: an analysis of the adjectives favored by Romantic poets. In the 1940s, with the aid of a Guggenheim, she expanded this work into a large-scale study of the phrasal forms of the poetry of the 1640s, 1740s, and 1840s. In all of this distant reading work, Miles created her tabulations by hand, with pen and graph paper. She also directed possibly the first literary concordance to use machine methods. In the early 1950s, Miles became project director of an abandoned index-card-based Concordance to the Poetical Works of John Dryden. Partnering with the Electrical Engineering department at Berkeley, and contracting with their computer lab and its IBM tabulation machine, Miles used machine methods to complete the concordance. It was published in 1957, six years after she and several woman graduate students and woman punch-card operators began the work. It was thus begun around the time that Busa circulated early proof-of-concept drafts of his concordance to the complete works of St. Thomas Aquinas, and published 17 years before the first volumes of the 56-volume Index Thomasticus began to appear. (Buurma and Heffernan)

Buurma and Heffernan bring Miles’ history to our attention not simply because Miles predates  Roberto Busa, whose *Index Thomisticus* is often credited as the first large-scale computational literary study.[^cf10] Rather, they emphasize, Miles’ origin story for computational literary study “can stand as an example of how we might write a history of literary scholarship that does not center originality and individual accomplishment” (n. pag.). Unlike Busa, Miles not only gave authorship to the (female) graduate students who carried out much of the labour of creating the concordances, she also thanked and credited the (female) punch card operators who encoded the resulting data.[^cf11] Moreover, when talking of Penny Gee, one of the female staff members of the computer lab, Miles praises her as “‘very smart and good’ and---most importantly---a true collaborator, as opposed to those ‘IBM people from San Jose’ ... ‘I’ve never been able to connect with them,’ Miles explains, ‘though I did with Penny Gee. She really taught me’” (n. pag.). Of the positive qualities highlighted here, only one, “smart,” is traditionally valorized among literary critics: to be “good,” a “collaborator,” who can “connect” and “teach” --- these qualities are often seen as irrelevant to the singular authority of the figure of the critic, but they are core to a reparative practice. Miles’ work, too, struggled to find appreciation “among literary critics who viewed her datasets as merely preparatory to the true work of evaluation” (n. pag.).  

What’s crucial, to use computational reading reparatively, is to use it *reflectively*. The desirable kinds of computation which I describe above will not happen inevitably. Here I draw upon the rich body of work emerging in critical algorithm studies, which examines (and attempts to reform) the human elements of computational algorithms. Any methodology is, to a certain extent, an “algorithm,” in the loose definition of ‘a series of pre-defined steps to be carried out’. But computational algorithms differ from “algorithms” implemented by humans. Computational algorithms have two key vulnerabilities: first, their operations are less easily scrutinized; second, their results are more easily trusted. The second vulnerability --- the cultural aura of empirical trustworthiness which accrues to anything ‘computational’ --- is another flavour of the same vulnerability that Drucker describes with ‘data’ generally. Because the human agents who designed and trained any given algorithm appear to be absent from its operation, the algorithm appears able to discover truth directly. This is how Daily Wire reporter Ryan Saavedra was able to tweet with disdain that “Rep. Alexandria Ocasio-Cortez (D-NY) claims that algorithms, which are driven by math, are racist” (@RealSaavedra): anything “driven by math,” he assumes, must be incapable of human fallibilities like racism. But as Safiya Noble shows extensively in *Algorithms of Oppression*, algorithms by default reproduce, and can easily exaggerate, the assumptions and biases of the culture in which they are made (CITE). In other words, in a racist world, algorithms *are* racist --- and sexist, and duplicative of all other systemic inequities.

  

Critical algorithm studies is therefore a crucial background for my work --- but “critical” is literally in the name of of the field, and I still seek to be post-critical and reparative. As I encounter the limitations of the various information and tools through which I attempt to understand the 1790s, my goal is to do something other than facilely observe that they are limited. Instead, I want to identify the best ways to continue building on their foundations. In a digital humanities context, a focus on building connections can be mundanely practical: typing indexes from print works into spreadsheets, correcting errors within datasets, writing programs to process metadata: all of these maintain the functional usability of existing resources in new contexts. When this kind of extended, detail-oriented labour is combined with serious reflection on the histories and possible futures of these resources, I contend, they bring us to new knowledge. In this, maintaining and using digital resources is also a way to repair them --- and to produce reparative readings of their contents.



## 1.3.  methods ##  

This dissertation undertakes computational distant reading. At every possible point, however, the underlying methodology will be made visible, and its assumptions scrutinized. The bibliographic histories of my multiple corpora are explicit objects of inquiry. Much of the code underlying this project I have written myself. Some has been written at my request. In every case where the code is available to me, the program itself appears in Appendix A (“Codebase”), accompanied by a plain-language explanation of how it operates. Where I have used closed-source software, Appendix A contains an explanation of my best guess at its underlying process. My exact use of these tools --- sufficient for another to replicate my work --- is provided in Appendix B (“Methodology”). These details are explicated in full in the appendices in order not to over-burden the body of the dissertation, but they are by no means *confined* to the appendices. Computation is not a “black box” to be consulted for simple answers, but is inextricable from my reasoning and argument.  

My attention to the *sources* of digital knowledge creation comes, in part, from Johanna Drucker, and her distinction between “data” and “capta.” Drucker, in “Humanities Approaches to Graphical Display,” specifically addresses the digital humanities practice of creating, and then close-reading, data visualizations. She argues that the tools for visual representation which may be effective in the sciences cannot be simply and uncritically transposed to humanistic subject matter. When an experiment is presented as a ‘data visualization,’ she says, “the rendering of statistical information into graphical form gives it a simplicity and legibility that hides every aspect of the original interpretative framework” (8). In fields where the readers of such charts are also frequent creators of charts, and where norms exist to explicitly describe one’s interpretive frameworks in a methodology section, the simplicity and legibility of an individual chart may be a benefit which does not impede complex scrutiny of the information it presents.[^cf12] In a field like literature, however, the “graphical force” of something like a network graph or even a simple pie chart “conceals what the statistician knows very well --- that no ‘data’ preexist their parameterization” (8). Drucker problematizes the term “data,” the etymology of which presents it as a “given” which is stable and independent of observation. She proposes that humanities visualizations embrace, instead, the framework of “capta,” that which is “‘taken’ actively” (3), “fundamentally codependent, constituted relationally, between observer and observed phenomena” (50). Drucker’s assessment shapes my own prioritization of qualitative and reflective computational research. The term “capta” itself has not seen uptake in subsequent digital humanities scholarship, even in cases where scholars explicitly take Drucker’s warnings to heart. Accordingly, for clarity, this dissertation will continue to use the more usual term “data” to refer to the information gathered for analysis here. However, as I integrate and compare a wide variety of data from many disparate sources, a preliminary task of my analysis is always to determine, as precisely as possible, how the information was captured and quantified.   

Additionally, all of the figures presented in this dissertation are of my own design. My design praxis is informed by the work of Edward Tufte and Alberto Cairo, both of whom provide practical design advice in service of demystifying the visual rhetoric by which graphs present their arguments.[^cf13] Neither Tufte nor Cairo is a scholar of media studies; rather, they are professional practitioners of ‘data visualization’ who reflect critically on the assumptions of their work. Tufte’s work primarily strives to correct badly-designed data visualizations, and the dangerous decisions that bad design can lead people to. His most famous example is an analysis of the engineers’ report at NASA which led to the ill-fated launch of the Challenger space shuttle in 1986: as his extensive visual analysis argues, the engineers (untrained in graphic design) unintentionally obfuscated crucial information about the day’s launch conditions. The poorly-designed graphics these engineers produced made the launch appear low risk to their superiors; despite the engineers’ strong warnings, their verbal argument was disregarded in favor of their accidental graphical argument. As Tufte demonstrates, a few simple alterations of their graphic design would have made it obvious that the day’s unprecedentedly low weather was extremely dangerous, and potentially averted disaster \\cite\{Tufte:2001vw\}.[^cf14] Tufte’s six principles of design[^cf15] primarily seek to guide undertrained designers away from misleading themselves. Cairo, following on Tufte’s work from the perspective of an active journalist, more often turns his attention to successful designs which mislead their audiences intentionally. His forthcoming book, *How Charts Lie*, addresses the readers of infographics with insights into visual literacy \\cite\{Cairo:ikIksuMr\}. His preceding book, *The Truthful Art*, addresses the creators of good-faith infographics with insights into visual manipulation \\cite\{Cairo:2016uv\}. Cairo draws a distinction between “data visualization” and “infographics”: “an infographic tells the stories that its designer wants to explain, but a data visualization lets people build their own insights based on the evidence provided,” summarized more succinctly as “infographics to explain, data visualizations to explore” \\cite\{Cairo:2014tl\}. Using this terminology, my argument will proceed with infographics in the body of the dissertation as curated figures to support my argument, with fuller data visualizations available in Appendix C (“Data”) to allow further exploration. Following in both Tufte and Cairo’s footsteps, I conceive of the figures throughout this dissertation as rhetorical devices. In service of arguing honestly, therefore, my designs --- in the body of the dissertation and in Appendix C --- are accompanied by footnoted explanations of my design rationale.   

This dissertation understands archives, bibliographies, anthologies, and corpora to all be, variously, *models* of an imagined object of study. In the language of social science, these models might be described as ‘samples,’ which are intended to permit discoveries about an underlying ‘population’ by being ‘representative’ of that population’s features. Only the language and not the methods of social science need to be imported here, since it has long been ordinary practice in literary studies to select and examine representative texts for insights about larger movements[ cite Leah Price?]. A work like Ann Tracy’s bibliography *The Gothic Novel 1790-1830*, for example, clearly names the population of works which are of interest to her: all Gothic novels published between 1790 and 1830. But in providing detailed information on 208 texts --- mostly Gothic, mostly novels, mostly between 1790 and 1830 --- Tracy obviously does not claim to have presented all that might belong within this population. Instead, her book operates as a model of the underlying population, which can be queried for further insight into ‘the Gothic novel, 1790-1830’ only so long as one keeps the limits of the model in mind. Indeed, by presenting plot summaries and bibliographic data, rather than reproducing the novels in full, Tracy provides a model of a model. Importantly, a model is a tool for thinking, and not necessarily a truth-claim in itself: creating a model is a way of saying, ‘it might be helpful to think of X as Y,’ not an assertion that X is equivalent to Y. Willard McCarty[ need a citation for McCarty here] articulates this important feature of models by stressing that a model’s value is determined not by its exact correspondence with the object it models --- if it were possible to fully examine the underlying object, then no model would be necessary --- but by the *fruitfulness* of its simplifications. Even a deeply incorrect model can be fruitful if its divergence from observed phenomena rules out an incorrect theory. As I examine the many existing models of ‘English literature, 1789-1799,’ and create several more of my own, I articulate the underlying assumptions of each model, and assess the fruitfulness of the results.



## 1.4.  scope ##  

All of the computational work in this dissertation aims to identify, in as minute detail as possible, all works printed in England between January 1 1789 and December 31 1799. This eleven-year “decade” was a turbulent one across the Channel, encompassing the whole of the French Revolution, from the Estates General in 1789 to Napoleon’s coup in 1799.[^cf16] In England, these events caused strong and variously nationalist reactions in a country which had so recently lost its colonies in America and feared that a French invasion could come at any moment. This is the decade of *Common Sense*, it is the decade of *Lyrical Ballads*; it is the decade of Hannah More, it is the decade of Ann Radcliffe; it was the age of wisdom, it was the age of foolishness; it was the epoch of belief, it was the epoch of incredulity. Charles Dickens’ now-famous superlatives capture the tension often seen by scholars between ‘Enlightenment’ modes of writing and ‘Romantic’ or ‘Gothic’ modes.

Scholarship on 18thC works often takes the form of evaluating or assigning the cultural capital of individual works, or, perhaps, analyzing the strategies by which they accrue or fail to accrue that capital. The winners of the cultural capital game are the Romantics in poetry and Walter Scott in prose. For example, Simon Bainbridge examines the decade and its poetry through the lens of war to identify “the attempts made by several writers to fill the role of national bard prior to Scott” (3). Both poetry and the poet, in his conception, are pursuing a particular kind of cultural capital that allows them to rise above their own popularity. Richard Cronin’s *The Politics of Romantic Poetry* and Robert Miles’ \[WHICHEVER ONE IT IS\], too, seem to treat Scott’s \[intensely serious popular romances\] as the teleological end of the late eighteenth century birth development within the novel. These works follow a pattern established from the beginning with \[Kiely and Tompkins\], of treating the novel as synonymous with the realist novel, and treating Romantic and especially Gothic novels as aberrations in the history of the novel, a problem which needs to be explained away. E.J. Clery’s *The Rise of Supernatural Fiction* has examined at length the historical conditions by which supernatural plot elements began to make limited claims to literary seriousness throughout the eighteenth century. The “rise” she describes is not an increase in volume and prominence of supernatural stories, since her starting point in 1762 (the Cock Lane ghost) is a major national phenomenon with many imitators. Rather, supernatural fiction ‘rises’ when it acquires cultural legitimacy. Michael Gamer has more recently expanded on how this ‘rise’ fuelled Romanticism’s own rise. Gamer, like Bainbridge and Cronin, primarily examines Wordsworth and the ‘winners’ of the struggle for cultural capital: I, like Clery, am more interested in the ‘losers.’ Accordingly, I attend to much that is *not* literature, in order to better understand why it is not.  

To navigate the 1790s, I turn to an author whose careers and works usefully focalize my core questions of genre, publics, and the status of literature: Charlotte Smith. Smith was highly productive in multiple genres throughout the 1790s, and had a complex and contested literary legacy after the 1790s. As literary scholars re-assess ideas about literary seriousness, popularity, and women’s writing, our assessment of Smith has shifted as well. By examining their bibliographies with computational methods, I again ask how she might continue to look different if we look at her a different way. I particularly examine the extent to which digital resources have kept up with the re-evaluation of Smith as a central figure in British Romanticism.  

Charlotte Smith is selected as a writer who was productive in multiple genres, only some of which may end up represented in corpora. Charlotte Smith’s literary career began with the publication of her volume of poetry *Elegiac Sonnets*, in 1784.[ maybe these whole bibliography sections should just be tables?] This work is the one upon which much of Smith’s fame and prestige rested in the eighteenth century. A second edition of *Elegiac Sonnets* rapidly followed the first in the same year, with only slight amendments. The third and fourth editions of *Elegiac Sonnets* appeared in 1786, adding new poems. 1786 also saw the publication of Smith’s *The Romance of Real Life*, a translation of *Les Causes Célèbres,* her first foray into prose, which would occupy the major part of the next phase of her career. In 1788 she published her first original novel, *Emmeline, or the Orphan of the Castle*. 1789 begins this dissertation’s decade of interest, a period of intense productivity for Smith: she had at least one new publication almost every year from 1789-1799. In 1789, she published her second original novel, *Ethelinde, or the Recluse of the Lake*, and a fifth edition of *Elegiac Sonnets*. In 1791 she published *Celestina,* her third novel; in 1792, her fourth novel, *Desmond*[ add info about the “phases” of her novelistic career], and a sixth edition of *Elegiac Sonnets*. Although *Elegiac Sonnets* continued to be reprinted, reaching its tenth edition in 1812, after this edition no further poems were added. Instead, her new poetry appeared in their own independent publications, and no longer took the form of sonnets. In 1793 she published *The Emigrants*, a poem in two volumes, as well as *The Old Manor House*, her fifth novel. In 1794, her sixth and seventh novels, *The Wanderings of Warwick* and *The Banished Man*. In 1795 she published her eighth novel, *Montalbert*, and began writing in a new genre with *Rural Walks*. With *Rural Walks*, Smith’s dominant genre again changed: having gone from a poet to a novelist, she now primarily published in a form which does not have a contemporary name: morally instructive natural history for “young persons.” 1796 saw the sequel to *Rural Walks*, *Rambles Farther*, as well as the novel *Marchmont*, and the poem *A Narrative of the loss...* of several ships. 1797 saw the eighth edition[ when was the seventh???] of *Elegiac Sonnets*, unchanged since the sixth. 1798 saw the novel *The Young Philosopher*, and more natural history for children in *Minor Morals*. In 1799, Smith tried her hand at theatre with *What Is She?*, a comedy --- not a form she will revisit. After this dissertation’s decade of interest, Smith continued to write at a slightly less frenetic pace. In 1800 she published the first three volumes of *Letters of a Solitary Wanderer*, an epistolary anthology of narratives. In 1802 she published two additional volumes of *Letters of a Solitary Wanderer.* In 1804, she published *Conversations, Introducing Poetry*, for children. In 1806, Smith published *History of England*, another work for young persons, and Smith herself died, age 55. The next year saw the posthumous publication of the poem *Beachy Head* and the work for young persons, *The Natural History of Birds.*  

Smith’s personal life sometimes overshadows this career. As her works often make clear to her readers, after a briefly comfortable youth as the daughter of a well-off country gentleman who lived beyond his means, she was married at age sixteen to Benjamin Smith, “son of a prosperous London merchant and owner of Barbados sugar cane plantations. The marriage was contracted hastily to remove her from her paternal home, now dominated by her new wealthy stepmother. Looking back in bitterness nearly forty years later, Charlotte Smith described the event as her father's decision to sell her like a ‘legal prostitute, in my early youth, or what the law calls infancy’ (Smith to Sarah Rose, 15 June 1804)” (Roberts). Benjamin Smith was cruel and violently abusive. He was also so financially irresponsible that his wealthy father, Richard Smith, wanted to prevent Benjamin from inheriting. Charlotte Smith assisted Richard with business correspondence and impressed him as responsible and competent. In recognition of her husband’s unreliability, “she persuaded \[Richard\] to relieve his son of all his ties to the business and establish him as a gentleman farmer in Hampshire” in 1774 (Zimmerman). Richard Smith died in 1776. “In an attempt to provide for his daughter-in-law, Richard bequeathed the bulk of his property to her children. But he had drawn up his will without professional advice; legal wranglings over the inheritance worth nearly £36,000 soon arose and were not settled until almost forty years later. By 1783 Benjamin had already unlawfully squandered more than a third of this trust and, as a consequence, found himself first in deep debt and then in King's Bench Prison.” (Roberts). After the success of the *Elegiac Sonnets* allowed Smith to pay for her husband’s release from prison, Benjamin Smith fled to France to escape further creditors. Charlotte Smith moved between England and France over the next year and a half to negotiate his debts, and in 1785, the family was able to return to England. In 1787, after 22 years of marriage, Charlotte Smith legally separated from her husband, “an unusual step for a woman of her time” (Fry 7), and moved to a town near Chichester with her nine surviving children (of the twelve she had given birth to). However, despite this separation, Benjamin Smith retained a legal right to Charlotte Smith’s profits from her writing. Smith moved frequently after her separation, due to financial instability and declining health. “On 23 February 1806 Benjamin died in a debtors' prison and some money reverted to Charlotte Smith. By then she was far too ill to execute her favourite scheme, to settle on the shores of Lake Leman. On 28 October 1806 she died, only eight months after her husband, and seven years before Richard Smith's estate was finally settled.” (Blank)  

Smith’s posthumous critical reception has undergone multiple shifts in appreciation and obscurity. Duckling’s study of her presence in anthologies indicates that shortly after her death in 1806, Smith was widely eulogized and anthologized, remembered and emulated as an important British poet. As the nineteenth century went on, poetesses began to be anthologized separately from poets, in collections with ambitions that were commercial rather than intellectual; Smith, too, “lost intellectual ground” even as she continued to be sold (Duckling 2016). By the end of the nineteenth century, even these volumes marginalized Smith’s poetry, with prefatory material which dismissed them as trite and depressing, unenjoyable reading. In the early twentieth century, Smith began to be considered as a novelist, rather than a poet; this new field did not lead at first to a much better reputation for her. Florence Hilbish produced the first extensive study of Smith, considering her as both poet and novelist, in 1941, to unappreciative reviews: Ernest Bernbaum’s faint praise said that “‘much time and care have been devoted to it; whether deservedly, is perhaps questionable,” since “the subtle or intricate is absent from Charlotte Smith's writings” (138). Hilbish presents Smith’s emotional poetry as sincere rather than conventional, and her prose as more motivated by politics than commerce.

Duckling credits the feminist movement of the 1960s and 1970s with the beginning of Smith’s recovery (217): the renewed interest in women’s writing rediscovered her novels, and especially the radical political content which Hilbish had observed. At the same time, Bishop Hunt published a record of Smith’s influence on Wordsworth, as demonstrated by an almost overwhelming amount of physical evidence: Wordsworth owned copies of her works, which he annotated; he copied out some of her sonnets in his own hand; he paid her a personal visit; he edited some of her poetry for publication; he wrote explicitly of her influence in notes to his works. Hunt calls Smith “an important early influence on Wordsworth which has not been explored in any detail up to now” (85); his abstract somewhat snarkily asserts that “Wordsworth did not suddenly start writing sonnets in 1802 simply because he happened to read Milton’s.” However, Hunt has little praise for Smith herself: of one poem, he says, “Whatever the artistic value of such verses,” what matters is the underlying theme which Wordsworth would later express more masterfully (89). Smith continued to be treated separately as an interesting woman novelist, and a minor pre-Romantic poet, through the 1980s. Smith rose to greater prominence in both of these fields in the 1990s: with work by Stephen Curran, Roger Lonsdale, Jennifer Breen, Andrew Ashfield, and Jacqueline Labbe, “Smith became established not only as a prominent figure in the revised female canon, but also as a central figure in Romanticism” (Duckling 217).

Throughout this history, two aspects of Smith which have prompted frequent re-assessments are her personal life, and her work across genres. The first matter, the importance of a female author’s life as a woman to her importance as a figure worth remembering, is implicit in several phases of the rise and fall described above. Fry is not alone in concluding that “\[f\]ew writers have presented themselves in their works so fully as did Charlotte Smith” (3): Smith’s poetry lyricizes her personal experiences, her novels feature autobiographical stand-in characters, and “the often intensely personal pleading prefaces” (Behrendt 189) to her works explicitly ask for them to be read light of her ongoing struggles. Perhaps as a result, much scholarship on Smith takes the stance of *The Literary Encyclopedia* in defining her as a woman who wrote because of, and chiefly about, her personal distress. Antje Blank’s article there highlights Smith’s financial motive to write: “Smith turned to writing when a failing marriage and a costly lawsuit left her without resources to raise her large family” (Blank). “And so,” Blank says, Smith “churned out” her novels (and the many editions of *Elegiac Sonnets*, and her other poetry, and her educational writing) to support herself and her nine children (Blank). Even when Smith’s Elegiac Sonnets “won her the reputation as an author of serious verse,” this is important primarily because it “lent greater respectability to her ensuing productions in a less prestigious but more lucrative genre -- the novel” (Blank). At the same time, as Labbe argues in her article “Selling One's Sorrows: Charlotte Smith, Mary Robinson, and the Marketing of Poetry,” Smith cultivated a public persona as a paragon of victimhood and motherhood, suffering deeply but turning her suffering into marketable prose out of a duty to her children. In periods where this image of womanhood is valuable, Smith is more easily valued, as in the eighteenth and nineteenth century anthologies which saw Smith as a moral exemplar (Duckling 203-4). Or, in periods when women’s resistance to patriarchal oppression is of scholarly interest, the direct, personal nature of Smith’s writing is valuable in itself, as in early feminist scholarship.

A complicating factor to these evaluations of Smith is that, as Labbe’s edited volume *Charlotte Smith in British Romanticism* thoroughly demonstrates, Smith’s writing is neither as uniform nor as simplistically personal as autobiographical readings sometimes see it. Labbe contends that Smith-the-novelist and Smith-the-poet have been largely studied as separate entities, “and consequently we have been learning about two separate Smiths, each closely linked to the genre she writes in, neither closely linked to the other” (5). Labbe is not quite the first to attempt to unify Smith: Carol L. Fry’s 1996 monograph *Charlotte Smith* also addresses her poetry before moving on to the several phases of her novel-writing, including the children’s writing which made up much of Smith’s later career but does not appear in Labbe. Indeed, from the beginning, Hilbish’s 1941 monograph explicitly identifies Smith as “Poet and Novelist” in its title. However, Labbe is accurate regarding the somewhat different assessments of Smith current in the somewhat separate study of novels and of poetry in general: Labbe argues that as a novelist, Smith is now often praised for her innovative narrative techniques (implying a mode of writing that is intellectual and ‘distant’), whereas as a poet, she is praised for her innovative expressions of interiority (implying a mode of writing that is emotional and ‘close’). Labbe draws greater attention to important differences between Smith’s writing personae in different genres, and her edited collection “pulls together many Smiths” (2) to address these disjunctions. The volume not only addresses her novels and poetry, but also includes her plays, letters, and posthumous reception. Each of these Smiths, the volume contends, has something innovative and unexpected to reveal, important to the formation of British Romanticism. In Judith Phillips Stanton’s “Recovering Charlotte Smith's Letters,” for example, Smith’s letters, less studied, reveal a third kind of writer, different from both the novelist and the poet, who conceives of herself as a professional businesswoman of her craft. More Smiths are available in genres not included in this volume, such as Smith the naturalist and children’s author (touched on only lightly in Labbe’s volume), or Smith the political philosopher who drives Amy Garnai’s *Revolutionary Imaginings in the 1790s,* a highly political Smith who consciously participates in the “political public sphere” conceived by Habermas, despite Habermas’ insistence that women were excluded from this sphere (1)*.*[ \[Something about each Smith having her own peers...? Political Smith now becomes peers with Mary Robinson and Elizabeth Inchbald, whereas Poetic Smith is peers with Wordsworth & Coleridge, and Novelistic Smith with Radcliffe etc\]] From these distinctions, Labbe concludes that “Smith, significantly, composes herself anew according to genre” (2) --- and then asks, “Is it all to do with inherent qualities of genre, or is it more to do with the expectations we as readers bring to different genres?”[ Labbe’s immediate answer: “Genre, it seems, carries a greater force in constructing our preconceptions of identity than has been recognized, and Smith is a case in point, a case we can crack by studying closely Smith’s style and techniques across genres.” (Labbe 5)] (5). This question about genre is one of the initial questions to inspire this dissertation: to see it asked as a core question about Smith demonstrates Smith’s suitability as a figure whose career can shed light on important questions about the mediascape of the 1790s.  

A core object of study for this dissertation is the makeup and history of contemporary digital databases. Eighteenth century materials of various kinds have been collected in many digital archives, of very different scopes. I will draw materials from the English Short-Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the ECCO Text Creation Partnership corpus (ECCO-TCP), Google Books, Project Gutenberg and HathiTrust. My examination of these six databases will, of necessity, examine a ‘time capsule’ of their holdings at a particular moment; the sources of my data, and my procedures for working with them, are described in more detail in Appendix B (“Methodology”). The databases vary from each other in terms of two main qualities: their size, and their reputation. The reputation of any given digital resource is shaped largely, I argue, by its ability to signal ‘rigour’ in its collection practices. Several databases of different sizes have established reputations of seriousness, and, correspondingly, cultural capital within scholarly communities. The databases that I will examine at length form two groupings of three each, to explore two sets of related concepts. The first set consists of ESTC, ECCO, and ECCO-TCP, all of which follow the same rigorous collection practices at different scales. The second set consists of Google Books, HathiTrust, and Project Gutenberg, which follow very different collection practices while sharing a dubious scholarly reputation.  

The first three databases I examine will be no surprise to eighteenth century scholars: ESTC, ECCO, and ECCO-TCP. Gale’s Eighteenth Century Collections Online (ECCO), contains over 180,000 titles 1701-1800, of which 42,000 were printed in England between 1789 and 1799. ECCO is itself (mostly) a subset of the broader English Short Title Catalogue (ESTC), which contains more 460,000 texts 1473-1800, of which 51,965 were printed in England between 1789 and 1799 (indicating that nearly 10,000 titles in the decade appear in the ESTC but not ECCO). The ESTC does not provide access to texts themselves: instead, it is an authoritative bibliographic catalogue, available as a searchable database. It is ECCO which provides texts: ECCO’s 180,000 titles works are available as photographed facsimiles of the full text of each title. The facsimiles can be searched within ECCO’s online interface; these searches examine a plaintext version of the facsimile pages that was generated by Optical Character Recognition (OCR), but this OCR text is not made directly available. As a result, the facsimiles may be read individually by scholars, but cannot form the basis for computational corpus analysis. A subset of ECCO’s texts have been hand-prepared, as part of the Text Creation Partnership (TCP), to be easier to use in computational research. The resulting corpus of ECCO-TCP texts contains 2,231 titles, of which 466 were printed in England between 1789 and 1799. These titles are available as carefully-edited texts encoded according to the Text Encoding Initiative (TEI) standard, which not only provides an accurate version of the text’s words, but encodes substantial details regarding its context on the page. Most large-scale distant reading of eighteenth century literature relies on the ECCO-TCP corpus as its ‘model’ or ‘sample’ to represent the period[ footnote some examples]. Accordingly, one of the tasks of this dissertation is to examine the makeup of this corpus, and how it differs both from other corpora and from print culture in the period itself. These three digital collections --- ECCO, ESTC, and ECCO-TCP --- are the primary digital resources for the period, which form the basis of most digital research. However, they represent only one approach toward the collection and presentation of digital texts, to which there are two broad kinds of alternatives. These large but meticulous collections occupy a middle space between, on the one hand, highly selective thematic collections, such as The Shelley-Godwin Archive, of which there are many, and the giants of indiscriminate textual accumulation, such as Google Books, of which there are few.  

Smaller collections allow for more scholarly curation, but have corresponding limitations. Whereas the ‘main players’ of the the mega-archives can be easily enumerated, these specialized collections are numerous. Some will focus on particular kinds of texts, such as the Early Novels Database (2,041 novels 1700-1799) or Broadside Ballads Online (more than 30,000 broadside ballads). Others exhaustively index particular publications, such as *The Hampshire Chronicle* (1,950 references to fiction in issues from 1772-1829), the Index to the *Lady’s Magazine* (14,729 articles from 1770 to 1818), or the Novels Reviewed Database (1,836 reviews from *The Critical Review* and *The Monthly Review*, 1790-1820). Feminist scholarship in particular has seen the creation of resources like the Orlando Project, the Chawton House library Novels Online, Northeastern University’s Women Writers Online and UC Davis’s British Women Romantic Poets. The virtue of these collections is that they achieve even greater accuracy and comprehensiveness within their defined scope. The Shelley-Godwin Archive, for example, can reasonably aspire to digitize *every* known manuscript of Percy Bysshe Shelley, Mary Wollstonecraft Shelley, William Godwin, and Mary Wollstonecraft, and to provide these manuscripts in hand-encoded plaintext transcripts. However, as is inevitable, these specialized archives have the vices of their virtues: their specialized focus allows them to adapt precisely to their materials, and their idiosyncratic data structures can rarely be combined with other resources. The William Blake Archive, for example, benefits enormously from designing its archive around the unique images of each page of each copy of each of Blake’s works. But because this approach is so well-suited to Blake, it cannot be applied beyond Blake. Even if the archive’s resources were available for download, they could not be directly compared to materials from another source which does not record its information at such a minute level of detail. As a result, although a great deal of excellent digital scholarship is contained in specialized micro archives, I do not examine them further in this dissertation.  

Instead, I look at a set of larger archives of more contested “scholarly” status: Google Books, Project Gutenberg, and HathiTrust. Google Books may be the most infamous database of books. In a scholarly context, one hesitates even to designate this as an “archive,” particularly in the same breath as resources like ECCO: books of all kinds are scanned indiscriminately with only the bare minimum of roughly-accurate metadata collected about them. These rapidly-scanned books are prone to unpredictable errors, including inaccurate dates, misspellings, duplicate copies, and inaccurate subject classifications[^cf17] --- infamously, many books have “1899” assigned as their publication date because this date was used as a placeholder for “no date”.[^cf18] Nonetheless, Google Books is frequently used to study the prevalence of various “n-grams” (words or short phrases) over time, thanks to Google’s built-in tool. The tool is able to search books which are, for copyright restrictions, not available directly to readers, making it highly tempting for questions about contemporary language use.  

Also in the category of smaller and specialized archives is Project Gutenberg. Project Gutenberg makes no claims to scholarly reliability but nonetheless underlies a not-significant amount of scholarly work[^cf19] --- its cultural capital as a resource lags far behind its use and utility. Project Gutenberg is easily conceived of as a haphazard, ‘unscholarly’ source for materials, but unlike Google Books, Project Gutenberg actually does have selection criteria. Project Gutenberg will only collect public domain works which contemporary audiences might be interested in reading for pleasure. It narrows the field substantially to exclude works which have either ceased to be broadly interesting (as in the case of most forgotten fiction), or which were never particularly interesting (as in the case of almanacs and tax codes). Project Gutenberg includes 57,796 texts: far more than specialized scholarly archives like the Early Novels Database or the Shelley-Godwin Archive, but nonetheless an order of magnitude fewer than its more-voracious potential competitors. And, like smaller specialized scholarly archives, Project Gutenberg has tailored its holdings to make it easy for readers to read, and quite difficult for its collection to be applied to any other use. By tailoring the structure of the archive itself to its specific materials, these collections are able to thoughtfully achieve their aims --- but they also make it correspondingly difficult for users[ does this not depend on the user? unpack] to achieve their own, different aims.  

What makes Google Books of interest in the context of this dissertation is its relationship to HathiTrust, an increasingly popular resource for scholars. HathiTrust’s collection contains digitized content from “a variety of sources, including Google, the Internet Archive, Microsoft, and in-house member institution initiatives.” The “in-house member institutions” include one hundred and fifty-five universities, colleges, and consortia of universities. The aggregate scholarly authority of these institutions carries the weight of elevating HathiTrust above the Google Books scans which form the backbone of much of its contents: “The members ensure the reliability and efficiency of the digital library,” the website assures us, “by relying on community standards and best practices.” The texts themselves are stored in the database as facsimile page images and full-text OCR transcripts. In order to comply with copyright law, however, HathiTrust only provides large-scale downloads and OCR transcripts for texts which are in the public domain. Most scholars use HathiTrust to run experiments on OCR transcripts of copyrighted texts, which they can only access through computational workarounds that intentionally make it impossible for the scholar to see the full transcript itself.[^cf20] Through its collection, HathiTrust provides a hodgepodge of texts, of often unverifiable provenance and accuracy, selected largely by happenstance and convenience in a quest to contain all printed books. Through its tools, however, which provide a unique solution to real barriers for scholars of contemporary literature, and through its institutional affiliations, HathiTrust has acquired a cultural captal among scholars which Google Books still lacks.  

HathiTrust’s success in acquiring scholarly capital stands in interesting contrast with Project Gutenberg’s continued lack of cachet. Project Gutenberg is used in research with similar frequency to Google Books’ n-gram tool,[^cf21] but scholars often mention Project Gutenberg with a note of apology for not having found a better source. Its cultural capital as a resource lags far behind its actual use and utility, likely, I argue, because its organizing principles are the ‘unserious’ ones of popularity and pleasure. Project Gutenberg is easily conceived of as a haphazard source for materials, but unlike Google Books, Project Gutenberg actually does have selection criteria. Project Gutenberg will only collect public domain works which contemporary audiences might be interested in reading for pleasure. This criteria might not render Project Gutenberg more useful for scholarly work but, it nonetheless narrows its selection substantially. Project Gutenberg includes 57,796 texts: far more than specialized scholarly archives like the Early Novels Database or the Shelley-Godwin Archive, but an order of magnitude fewer than its more-voracious potential competitors. In taking Project Gutenberg seriously as a collection of texts, I seek to explore the extent to which its reputation as “unreliable” may or may not be deserved.  

As this brief survey of eighteenth-century digital archives shows, there is no ‘perfect’ corpus for large-scale study of eighteenth-century texts.  Moreover, I argue, the imperfect samples which each archive provides are shaped not only by historical factors of eighteenth-century print culture, but also by contemporary digital culture. Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. As this dissertation will argue, these questions of digital history have important resonance with literary questions about literary canon formation.



## 1.5.  dissertation map ##  

Chapter two examines Charlotte Smith and the ways that her writing is made accessible today. The specific experimentation undertaken in chapter two tests the basic assumptions and methods of my project. I identify what subset of Smith’s works each corpus contains, as a concrete example to compare their holdings overall. Smith’s *Elegiac Sonnets*, for example, are not included in the ECCO-TCP corpus (which is the one most often used for text mining research) --- only *Celestina* and *The Emigrants* are included. Why these two texts? And what text mining research based on ECCO-TCP might have found slightly different answers if Smith’s sonnets had been included? As a related test of comparison between databases, for each database which provides access to the actual text of Smith’s works, I compare the textual similarity of *Celestina* and *The Emigrants.* What editorial choices are being made? How *much* worse is the OCR text than the transcribed text? Another key concept I will explore through Smith is the role of reprints. HathiTrust, for example, includes multiple editions of *Elegiac Sonnets*. How reliable and effective are its distinctions between editions? How do the databases I examine handle multiple editions of a single work? I am particularly interested in how reprints can be incorporated into our understanding of what literature is “of” a particular decade: what does it mean to think of *Elegiac Sonnets*, initially printed in the 1780s, as “1790s literature”? Finally, having surveyed my six databases with the help of Smith, I discuss the multiple “Smiths” which emerge, and what it means to attempt to unify her disparate works.



### 1.5.1.  chapter 3: databases ###



In chapter three, I re-examine my core databases, but no longer with Smith as a focalizing lens. Instead, I undertake computational assessment and comparison of the databases’ contents. My research examines the authorship and subject matter (broadly construed) of all titles printed in England between 1789 and 1799 which are included in the English Short Title Catalogue (ESTC) database, the Eighteenth Century Collections Online (ECCO) database, the Eighteenth Century Collections Online Text Creation Partnership (ECCO-TCP) corpus, or the HathiTrust database. I calculate the proportion of the titles in each resource that are attributed to men, to women, or are left unsigned. Using the titles of these works and a topic modelling tool which has performed accurately when compared to a manual assessment of random samples, I also roughly identify the subject matter of each title, categorizing works into broad genres such as drama, poetry, Romance, History, or sermons. I am then able to compare these four resources to each other, and to existing scholarly work on the print production of the 1790s. For example, I am able to compare each resource’s holdings to the statistics on the English novel included in Garside, Raven and Schöwerling’s *Bibliographical Survey of Prose Fiction*. These comparisons illuminate the implicit principles of selection which have shaped the ECCO-TCP and HathiTrust digital collections, but also suggest conclusions about the period itself. I present multiple ways of visualizing the same underlying “data,” (that is, the print production of the 1790s itself) in order to analyze the implications of each way of seeing the decade’s literature. From what viewpoint, for example, can we see the hordes of female Radcliffean imitators which were so decried for dominating the print marketplace? And what other visions are possible?  

In my fourth chapter, I playfully attempt what might be considered a devil’s advocate method of textual selection: pure random sampling. Using a random number generator, I select ten arbitrary texts to close-read.  

A final brief conclusion to this dissertation offers an assessment of the role of digital textual collections in contemporary literary study.



# ch 2 - Charlotte Smith #



## intro ##



## Smith in databases ##  

For the purposes of this chapter, I examine Smith’s works which fall outside this dissertation’s decade of interest. As Table 1 shows, Smith’s publishing career began in 1784 and continued until her death in 1806; when I refer to Smith’s “full” output, I consider all 47 editions of her works published in her lifetime or in the year immediately following her death. Her 1790s output (that is, the editions published 1789-99) consists of 30 of those editions.  Her 1790s output (that is, the editions published 1789-99) consists of 30 of those editions. I have slightly expanded my chronological focus in part because some of the most interesting exclusions occur earlier and later in Smith’s publishing career, such as the first edition of her immensely influential Elegiac Sonnets (1784), which is listed in the ESTC but not available in facsimile anywhere, or the publications in the last years of her life, which are excluded from the chronological focus of most resources but can still appear in HathiTrust. Of particular interest is the fact that Beachy Head, which is now one of Smith’s most frequently anthologized and taught poems, does not appear in a single digital database.

  

![][CSmith-in-ESTC-ECCO-TCP-Hathi-table]

Table 1: All editions of Charlotte Smith’s works published in England during her lifetime or in the year immediately following her death, and their inclusion in the ESTC, ECCO, ECCO-TCP, and HathiTrust databases.  

Figure 1 shows how Smith’s presence in four major databases has the effect of winnowing down her full output arbitrarily. Even the largest collection, the 42 editions included in the ESTC, is not comprehensive: since the ESTC does not include any works published after 1800, it excludes volumes 4 and 5 of *Letters of a Solitary Wanderer* (1802), three works for children (*Conversations, Introducing Poetry*, 1804; *History of England*, 1806; and *Natural History of Birds*, 1807), and the posthumous publication that now forms a major part of Smith’s reputation as a poet, *Beachy Head* (1807). ECCO lacks these five editions for the same reason, and is also missing five others: the first and ninth editions of *Elegiac Sonnets* (1784 and 1800), the second edition of *The Banished Man* (1795), the first edition of *Minor Morals* (1798), and the second edition of *Rambles Farther* (1800[ Why these five?]).

HathiTrust contains 18 of Smith’s 47 editions, though these are not a simple subset of the ESTC and ECCO. Unlike the ESTC and ECCO, HathiTrust contains volumes 4 and 5 of *Letters of a Solitary Wanderer* (1802)[^cf22]. This is the only post-1800 work which appears in HathiTrust, however--- the others are also missing, including the important volume *Beachy Head* (1807). There is one work included in HathiTrust but not in ECCO, the second edition of *The Banished Man* (1795). Whereas ECCO does not include works unless there is a complete copy available, HathiTrust provides scans of volumes 2, 3, and 4, and simply implies through their numbering that there is a missing first volume --- perhaps in the optimism that a volume 1 will appear from another library’s holdings, to complete the set later.[^cf23] The remaining HathiTrust included titles appear in both the ESTC and ECCO, and a further 21 titles appear as facsimiles in ECCO but not in HathiTrust. At first blush it is somewhat surprising that HathiTrust has failed to include works which are, demonstrably, in known locations at institutional libraries, and in physically sound condition to be scanned--- but the scans making up HathiTrust bear no relation to the scans in ECCO. *The Young Philosopher* (1798), for example, appears in ECCO sourced from a British Library copy, but the HathiTrust images are “Google-digitized” from the New York Public Library. Google’s rapacious book-scanning, evidently, was not as thorough as ECCO’s sustained scholarly project.

The smallest subset of all of these texts is the ECCO-TCP holding of just two titles: the second edition of *Celestina* (1791), and the first edition of *The Emigrants* (1793). Both titles appear in all larger databases, including HathiTrust (though, as I will discuss, they arrive in HathiTrust from a different source). *The Emigrants* is included in ECCO-TCP as one file, based on the ECCO facsimile of an original from the Huntington Library. *Celestina* is included as four files, one for each of four volumes, based on the ECCO facsimile of an original from the British Library. Both works were first reproduced in the microfilm version produced 1982-2002 in by Research Publications,[^cf24] then digitized in 2003 (released on ECCO in June 2004), and finally published as TEI XML files in January 2007. The current files have been kept up to date with changes in TEI standards, and were created by converting TCP files to TEI P5 using tcp2tei.xsl. The bibliographic metadata for these works is the same between ESTC, ECCO, and ECCO-TCP records. In HathiTrust, however, the source text for *The Emigrants* is a University of California Library copy, rather than the British Library, scanned by Google Books, and presented with substantially less detailed bibliographic information. The ESTC, ECCO, and ECCO-TCP records for The Emigrants all provide the same physical description “ix,\[3\],68\[i.e. 60\]p. ; 4⁰” with the same note“\[n\]umbers 9-16 omitted in pagination; text is continuous.” HathiTrust, in contrast, gives the physical description “ix, 68 p. ; 26 cm,” which is both more and less information: a quarto volume could be a range of sizes, so HathiTrust provides new detail by giving a measurement in centimetres, but the data on page numbers is now misleading. Consulting the HathiTrust facsimile shows that it, too, omits the page numbers 9-16, going directly from page 8 to page 17 without a break in the poem. HathiTrust also omits information on the three unnumbered pages between the preface and the poem. Evidently, a human did consult the book, to identify a nine-page preface in roman numerals, and the page number on the last page, but they did not carry out a full collation.  

![][CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3]

Figure 1: An alluvial chart, showing the winnowing down of Smith’s works from database to database. Of the 47 editions printed in England between 1784 and 1807, 42 are included in the ESTC, and 5 do not appear in the ESTC because they were printed after 1800 and thus fall outside its purview. ECCO contains 37 of Smith’s 47 editions, all of which also appear in the ESTC. ECCO is missing the 5 editions not listed in the ESTC (since it, too, does not contain works past 1800), as well as another 5 works. HathiTrust contains 18 of Smith’s 47 editions, but unlike ECCO, these are not a simple subset of the ESTC. HathiTrust contains one of the 5 editions excluded from the ESTC, and one of the 5 editions included in ESTC but excluded from ECCO. The remaining 16 HathiTrust editions appear in both the ESTC and ECCO. ECCO-TCP includes only 2 of Smith’s 47 editions, both of which appear in every previous database. Graph generated using RAW Graphs (Mauri et al.).  

Only one of Charlotte Smith’s works is available in Project Gutenberg: *Emmeline, the Orphan of the Castle* (first published 1788).  

Searching the ESTC for records which both have “Toronto” in the library name and “Charlotte Turner” in the author name turns up two records: volume one of *Rural Walks* (1795) and *Minor Morals* (1798), both held at the Toronto public library. The Toronto Public Library catalogue has two distinct author identities for “Smith, Charlotte Turner, 1749-1806, author.” and for “Smith, Charlotte, 1749-1806,” and the special collections holdings only appear under the latter name (making them initially difficult to find). Under the “Smith, Charlotte” name, however, six titles printed during Smith’s appear: the two listed in ESTC, plus a complete two-volume copy of *Rural Walks* (1795), the first and second editions of *Rambles Farther* (1796 and 1800), and *Conversations Introducing Poetry* (1804). Of these, *Rural Walks* and both editions of *Rambles Farther* are listed in the ESTC but without records of the Toronto copies. All six titles are part of the Osborne Collection of Early Children's Books. \[This is interesting because it shows how scholarly disciplinary interpretations perpetuate themselves *infrastructurally*: as a Toronto-based scholar, the path is easier for me to study Smith-the-children’s-writer than other Smiths.\]  

The existence of a carefully hand-corrected transcription of *The Emigrants* in ECCO-TCP provides an opportunity to check the reliability of the OCR in both ECCO and HathiTrust. I will proceed from the assumption that the ECCO-TCP files are 100% accurate, and that any differences between the OCR and ECCO-TCP represents an OCR error.[^cf25] Before beginning the experiment, my hypothesis was that both ECCO and HathiTrust would differ from each other in where and how they are inaccurate, but would have similar accuracy overall. I suspected that they were likely around 50% accurate, plus or minus 10% --- I wouldn’t be surprised if they were worse, but would be quite surprised if their accuracy was 80% or higher.[ What level of accuracy do people usually want for OCR research?] Acquiring the plaintext files from all three sources required some hunting for some hidden options and some workarounds; rendering them suitable for comparison required some modifications of each file, described more fully in Appendix B. Although Gale Digital Scholar Labs prominently provided an “OCR Confidence” of 95%, the first glance at the document was not very promising. To my surprise, Juxa calculated a relatively low “change index” for each text compared to the TCP witness: ECCO had a .16 change from base (i.e., 84% accuracy), and my normalized HathiTrust document had only a .09 change from base (i.e., 91% accuracy).[^cf26] This surprised me, and suggests that skepticism of OCR in eighteenth century text mining may no longer be appropriate.  

How accurate does OCR need to be? This depends on how the OCR will then be used.  

To make these comparisons concrete, consider the first page of Smith’s dedication, as it is captured by OCR in ECCO and HathiTrust, and in the ECCO-TCP transcript:



TO WILLIAM COWPER, Es DEAR SIR, THERE is,- I hope, some propriety in my addrefing a Com- potion to you, which would,never perhaps have existed, had I not, amid the heavy prefure of many sorrows, derived infinite consolation from your Poetry, and some degree of animation and of confidencefrom your efieen. . 'he.following performance isfarfrom aspiring to be con- fidered as an imitation of your inimitable Poem, " THE " TASK;" I am perfeetly sensible, that it belongs not to a feeble andfemninine hand to draw the Bow of Ulyfes.,Theforce, clearness, and sublimity ofyour admirable Poem; the felicity, almost peculiar to your genius, of givingto the moJ familiar objegls dignity and eset, I could never hope to,a reach (ECCO)



T O WILLIAM com/PER, Ess. DEAR SIR, THERE is, I hope, ſome propriety in my addreſſing a Com- poſition to you, which would never perhaps have exiſted, had I not, amid the beavy preſſure of many ſorrows, derived infinite conſolation from your Poetry, and ſome degree of animation and of confidence from your ºfteem. The following performance is far from aſpiring to be con- ſidered as an imitation of your inimitable Poem, “ The “TAsk;” I am perfºy fººl, that it belongs not to a feeble and feminine band to draw the Bow of Ulyſſes. The force, clearneſ, andſublimity of your admirable Poem; the felicity, almoſt peculiar to your genius, of giving to the moſt familiar obječís dignity and effečf, I could never hope to 3. - Reach (HathiTrust)



TO WILLIAM COWPER, ESQ.

DEAR SIR,



THERE is, I hope, some propriety in my addressing a Com\|position

to you, which would never perhaps have existed, had

I not, amid the heavy pressure of many sorrows, derived

infinite consolation from your Poetry, and some degree of

animation and of confidence from your esteem.



The following performance is far from aspiring to be con\|sidered

as an imitation of your inimitable Poem, "THE

TASK;" I am perfectly sensible, that it belongs not to a

feeble and feminine hand to draw the Bow of Ulysses.



The force, clearness, and sublimity of your admirable Poem;

the felicity, almost peculiar to your genius, of giving to the

most familiar objects dignity and effect, I could never hope to (ECCO-TCP)



Figure 2 shows how Juxta highlights the words which vary between these three copies.

Both of the OCR copies contain errors in individual letters which render the whole word interpretable by a human but not by text mining software, as in the case of “beavy” for “heavy.” The ECCO copy struggles with the fact that ſ is not an available character, sometimes substituting an f, as in “prefure” for “preſſure.” Both leave out spaces between words, creating new tokens like “isfarfrom” and “andſublimity,” though HathiTrust is less prone to this error.

Other features of the OCR copies are accurate to the page image but would nonetheless interfere with text mining. The hyphenation of “Com- poſition,” for example, would prevent it from rendering as a single word, though here even the careful TCP copy would introduce the same problem, since the line break is encoded as “Com\|position.” Before the TCP copy could be used for text mining, the \| characters would likely need to be removed --- not too different from removing the hyphenation from the ECCO and Hathi copies. Most difficult to resolve is the fact that OCR naturally attempts to capture *all* text on the page, including the signature mark and catch word. In ECCO these appear as “,a reach” and in Hathi they are “3. - Reach” whereas TCP more appropriately leaves these out. Unlike the problems with hyphenated words, there is no way to correct for the inclusion of catchwords in a document, since there is no predictable way to identify them --- but keeping them in the document will cause any text-mining software to count these words twice.

The usual “text cleaning” procedures would further prepare these OCR texts for text mining by transforming all words to lowercase, removing all punctuation, and, in most cases, deleting all words which don’t match a predefined dictionary of valid words. A scholar working with the HathiTrust OCR would almost certainly add to this a step converting the ſ character to an s, as discussed above, in order to make the dictionary comparison feasible. The result of this ‘cleaning’ would likely look something like the following:



to william dear sir there is i hope some propriety in my a potion to you which would never perhaps have existed had i not amid the heavy of many sorrows derived infinite consolation from your poetry and some degree of animation and of your he following performance aspiring to be con as an imitation of your inimitable poem the task i am sensible that it belongs not to a feeble hand to draw the bow of clearness and sublimity admirable poem the felicity almost peculiar to your genius of the familiar dignity and i could never hope to a reach (ECCO, as it would likely appear after text “cleaning”)



 

william dear sir there is i hope some propriety in my addressing a position to you which would never perhaps have existed had i not amid the pressure of many sorrows derived infinite consolation from your poetry and some degree of animation and of confidence from your the following performance is far from aspiring to be considered as an imitation of your inimitable poem the task i am that it belongs not to a feeble and feminine band to draw the bow of ulysses. the force of your admirable poem the felicity almost peculiar to your genius of giving to the most familiar dignity and i could never hope to 3 reach (HathiTrust, as it would likely appear after text “cleaning”)





Strikingly, these ‘clean’ texts are now further from legible to human eyes, as OCR errors which a reader could mentally correct (such as “beavy” for “heavy” are now entirely removed.  

![][juxta-emigrants-p1]

Figure 2: Juxta’s “Heat Map” visualization of the “base” witness of the first page of *The Emigrants* (i.e., the ECCO-TCP version carefully prepared by scholars), highlighting words which differ in the two witnesses of the ECCO OCR and the normalized HathiTrust OCR. A darker highlight indicates that the word varies in more than one witness.  

![][juxta-emigrants-histogram]

Figure 3: A histogram, produced by Juxta, showing where the two ECCO and normalized HathiTrust witnesses show the most difference from the base ECCO-TCP copy. “Longer lines indicate areas of considerable difference, while shorter lines indicate greater similarity between documents.” (“A User Guide to Juxta Commons”)  

![][fig-ecco-emigrants-p1]

Figure 4: The facsimile of the first page of *The Emigrants* found in ECCO, which forms the basis of the ECCO OCR text.  

![][hathi-emigrants-p1]

Figure 5: The facsimile of the first page of *The Emigrants* found in HathiTrust, which forms the basis of the HathiTrust OCR text.  

“16 In his discussion of JSTOR's "intolerably corrupt" OCR text, Nicholson Baker suggests that the reason why the user is prevented from scrolling through this naked OCR output is that scholars "might, after a few days, be dis- turbed by the frequency and strangeness of its mistakes . . . and they might no longer be willing to put their trust in the scholarly integrity of the database."17 Baker's criticism of JSTOR, however, is based on an error rate (with editorial intervention) of just one typo in every two thousand characters.” (Spedding 439)

“The two OCR-captured texts average over 150 typos per 2,000 characters,22 a high enough error rate to render parts of the text completely unintelligible. It is not clear how typical this error rate is, and how much it declines with editorial intervention,23 but again the scale of the problem is clear.24 Consequently, the claim that OCR errors "may occasionally result in incorrect character capture, which may affect some \[ECCO\] full-text search results," seems wildly, even heroically, optimistic.” (Spedding 440)  

A collocation formula like “ix,\[3\],68\[i.e. 60\]p. ; 4⁰” (the physical description of Smith’s T*he Emigrants* provided in the ESTC, ECCO, and ECCO-TCP) is no more transparent and obvious in meaning than the following markup:

\<listPrefixDef\>

\<prefixDef ident="tcp"

matchPattern="(\[0-9\\-\]+):(\[0-9IVX\]+)"

replacementPattern="https://data.historicaltexts.jisc.ac.uk/view?pubId=ecco-$1&index=ecco&pageId=ecco-$1-$20"/\>

\<prefixDef ident="char"

matchPattern="(.+)"

replacementPattern="https://raw.githubusercontent.com/textcreationpartnership/Texts/master/tcpchars.xml#$1"/\>

\</ListPrefixDef\>

Indeed, the collocation formula is less transparent than simple markup like the following:

\<titleStmt\>

\<title\>The emigrants, a poem, in two books. By Charlotte Smith\</title\>

\<author\>Smith, Charlotte Turner, 1749-1806.\</author\>

\</titleStmt\>

It may even compare unfavorably to relatively well-commented code, like the following:

| \# iterate through the directory ||
|  | for filename in listdir\_nohidden("./" + directory): |
| -----: | :----- |
|  |  |
|  | \# define the path to this file |
|  | path = "./" + directory + "/" + filename |
|  |  |
|  | \# strip the file's namespace |
|  | try: |
|  | xmlstring = stripNamespace(path) |
|  | except: |
|  | print "error stripping namespace of file %s" % (filename) |


What these comparisons intend to illuminate is *not* that collocation formulae ought to be simpler or more accessible. Rather, my point is that specialized encoding serves a practical purpose, and that it is a matter of training which determines what encodings will seem natural and useful.



## database histories ##  

In the next section I will close-read the implicit models underlying each database, to examine how each enforces a particular concept of “literature” and “a text.” However, before these models can make sense, we must understand the history of how they were built. I contend that each database is best understood as a negotiation between the noncommercial values of textual reproduction and \[capitalism\]. Each database has the goal of making valuable information available. After the 1990s, they are particularly influenced by the utopian ideal that digital reproduction at last made textual reproduction free. Each had to contend, however, with the fact that before a text can be reproduced digitally it must be *created* digitally, and that even if the material costs are entirely eliminated (which, of course, they are not) textual creation continues to have costs in labour.



### 2.1.1.  ESTC timeline history ###



The history of the English Short-Title Catalog is long, as befits its enormous scope. “The English Short-Title Catalog (ESTC) is a vast database designed to include a bibliographic record, with holdings, of every surviving copy of letterpress produced in Great Britain or any of its dependencies, in any language, worldwide, from 1473-1800” (CBSR)[ not sure how best to cite this - <http://estc.ucr.edu/index.html#>]. Today, “The English Short-Title Catalogue is the most comprehensive record of what has appeared in print in Britain and the English-speaking world for all branches of human experience from the last quarter of the fifteenth century to the start of the nineteenth. More specialized studies exist for fields and eras within that span, but no other resource matches ESTC’s dependability over such a broad range” (Vander Meulen 265).

It began as the Eighteenth Century Short Title Catalogue in the 1970s, operating in a similar line as the original Pollard and Redgrave Short-Title Catalogue for 1476--1640, which first appeared in 1926, and Donald Wing’s catalogue for 1641--1700, which appeared in 1951. These catalogues established the ambitious simplicity of the ESTC: to accurately describe every edition of every printed work in English or from the United Kingdom. After the completion of Wing’s STC, “\[e\]xploratory studies, poorly funded and inadequate though they were” (Korshin 209) throughout the 1950s and 60s pursued the feasibility of systematically accounting for the much larger body of printed work produced in the eighteenth century. The Eighteenth Century Short Title Catalogue began properly in 1976, at a conference jointly sponsored by the British Library and the American Society for Eighteenth Century Studies (Crump 106). Here, “bibliographers and librarians attempted both to arrive at a consensus of the size of the task and the methodology that would have to be adopted to achieve a union catalogue. However, until the works were catalogued, it would not be possible to answer basic questions (such as the potential number of extant items) which would predetermine working methods. The very fact that they found it difficult to agree for want of sound and accepted figures indicated the need for ESTC.” (Crump 105). A pilot project began at the British Library in 1977, under the direction of Robin Alston (Crump 105).

Unlike earlier Short-Title Catalogues, which appeared as lengthly print publications, the Eighteenth-Century Short Title Catalogue was conceived as digital from the beginning --- a decision which, as Karian notes, “exhibited considerable foresight” (283) in the 1970s. As a result, “ESTC records existed in digital form long before many humanists saw computer technology as central to their work” (Karian 283).  Robin Alston and Mervyn Jannetta developed their own cataloguing rules, distinct from the Library of Congress MARC and UK MARC standards (Korshin 211). Once these standards were established, the British Library began to re-catalogue its own holdings, and in 1979 libraries in the United States, Germany, and Australia undertook to supplement them. In these international collaborations, “Where ESTC records already existed, these were adopted as the \[new\] record and only those works not held in the ESTC base file were catalogued again” (Crump 105). “One implication of the publication history of short-title catalogues is that they have been deemed functional and valuable even before they were complete. (That estimation is crucial, for their full completion is for all practical purposes impossible.) Judging that even a preliminary form of the records was useful to scholars, the planners of ESTC determined to conduct its development ‘in full public view’ and to make the incomplete file available ‘warts and all’[ I can probably say a LOT more about all of this --- this is my answer to the “moving target” problem; move it to the introdction] (in the words of Henry Snyder and Michael Crump, responding to criticism by Peter Blayney)” (Vander Meulen 270). Accordingly, the in-progress database “was soon available online, from 1980 via the British Library BLAISE \[British LibraryAutomated Information SErvice\] system and from 1981 in the US Research Libraries Group RLIN \[Research Libraries Information Network\] system” (Norman). Each of these databases was worked on locally by researchers, and then updated and reconciled with each other weekly.

To supplement these databases, accessible almost exclusively to librarians with specialized training in operating them and primarily used by the scholars compiling the file, the ESTC intended to publish editions at particular milestones of completeness, intended for the use of non-librarian scholars. Their “first step, a fiche catalogue of \[the British Library’s\] holdings, together with indexes, generated by the computer” (Crump 105) was published in a microform “snapshot” in 1983, but other milestones did not occur according to schedule. The “joint Anglo-American interim publication of the ESTC file ” (Korshin 212) which was expected to follow on microform in 1984 (Korshin 212) did not appear. Alston attributed the delays partly to the immensity of the task, and partly to the impact of short-term cost-cutting decisions, like the reduction of early-stage proofreading or of in-person examination of books, which dramatically increased the labour of verifying the resulting database record. Although he consistently warned “how easily strategic decisions based exclusively on cost usually lead to greater, not less, eventual costs” (Alston), the ESTC each year seemed to be facing a new budget struggle, and important maintenance labour was several times deferred. This created something like a paradox for the ESTC: funding bodies wanted to commit less money to a project which was behind schedule, but the project would remain behind schedule unless it was funded to complete the work required. 

Nonetheless, work continued, and in 1985, the online databases in RLIN and BLAISE were upgraded to allow dynamic updates to a single shared file (Crump 106), which for the first time allowed continuous access to a shared record, rather than the constant exchange and messy merging of individual partially-overlapping records. “Until the file was dynamically available online on RLIN in 1985 batch processing was a weekly nightmare” (Alston). At this time, it was hoped that the new RLIN file would “result in a more complete and coherent ‘first edition’ of ESTC” to be published in 1989 (Crump 106), though this deadline, too, was not met. In the mean time “the ESTC file \[was\] available to scholars on both BLAISE-LINE and on RLIN.” (Crump 106). To facilitate its use, the ESTC distributed “\[a\] simplified manual for searching the file on-line” (Crump 106). Crump took the opportunity of the update to rhapsodize on the database’s potential usefulness for other scholars: “No longer is the scholar limited in access to the data by the fixity of the printed page” (106). This valuable resource was not without cost. Although the manual on how to formulate search queries was free, use of the ESTC itself was notably not. Institutions or individuals paid to subscribe to the ESTC itself, paid per query for searches to be run, paid per minute for being connected to the database, and often paid for access to the computers they must use in their own libraries. Tabor says “the ongoing expense of consulting ESTC was the cyber-equivalent of the hefty up-front payment needed to acquire its printed predecessors, STC and Wing” (367).

“In 1987, with the agreement of the Bibliographical Society and the Modern Language Association of America, the International Committee approved the extension of the database to cover the period from the beginning of printing in the British Isles (ca. 1472) to 1700. The file changed its name to the 'English Short Title Catalogue', thereby keeping its well-known acronym. The USA team began cataloguing pre-1701 material in 1989, joined in the mid-1990s by the British Library team, and the resulting records were made available in the RLIN file from 1994.” (Norman). “In 1992, IESTC approved a further extension of the file to include serial publications. The USA team began work in 1994 on the cataloguing of serials within the scope of ESTC” (Norman). 

Concurrently with the development of the ESTC, Wing’s seventeenth-century STC was undergoing redevelopment into a second edition, overseen by Katharine Pantzer. The second edition of Wing’s STC was published in two volumes in 1976 and 1986, followed by a set of exhaustive indexes in 1991. This second edition “represented a vast development of the original” (Vander Meulen 268), incorporating thousands of new entries, expanding the titles, and adding explanatory notes and headnotes. Its completion in 1991 also marked the end of the ability of its publisher and sponsor, the Bibliographical Society, to support it (Vander Meulen 269). “Accordingly, in 1999 the Society made an agreement with ESTC whereby the latter... would assume official responsibility for receiving new STC data” (Vander Meulen 270). The ESTC continued to research new entries and improve existing ones, releasing a second edition of the file on CD-ROM in 1998 and a third edition in 2003 (Norman).

In 2006, almost thirty years after the commencement of the project, the ESTC underwent another major shift: the database was made publicly available to be searched for free online. This inspired more rhapsodizing, this time from Tabor: “The freeing of ESTC ... now places in one location, for the consultation of anyone with internet access, the fullest and most up-to-date bibliographical account of ‘English’ printing” (367). At the same time, the ESTC began a project “to provide full title and imprint transcriptions for the eighteenth-century records” (Tabor 370). Vander Meulen says that “The history of ESTC is in fact the record of steady developments. Some have been conspicuous---for instance, the physical progression from a printed prototype to microfiche, CD, online access via the vendors Blaise Line and RLIN, and universal online availability through the British Library.” (Vander Meulen 270) Many more have been less visible, in constant improvements to the accuracy and detail of the records. In 2011, the Center for Bibliographical Studies and Research at the University of California Riverside was awarded a planning grant from the Andrew W. Mellon Foundation to “redesign the ESTC as a 21st century research tool” (“Planning Grant”)[ add to bibliography], which was followed in 2013 by a larger two-year grant to execute software improvements to the ESTC.



### 2.1.2.  ECCO timeline history ###  

To understand the history of the ESTC and ECCO, we actually need to begin with another resource: Early English Books Online, or EEBO.



“EEBO’s relationship with the original STC and Wing is straightforward and clear; EEBO’s relationship with electronic ESTC, on the other hand, is less well-known.20 A series of agreements made between ESTC and University Microfilms/ProQuest between 1989 and 1997 allowed EEBO to draw directly on ESTC’s existing bibliographical data. Consequently, / every search run on EEBO (with some exceptions) relies, in a fundamental sense, on bibliographical information originally supplied by ESTC -- but not in the form that one might expect. First, EEBO heavily edited ESTC’s data for its own purposes: certain categories of data were removed (e.g. collations, Stationers’ Register entrances), some information was amended (e.g. subject headings), and some was added (e.g. microfilm- specific details). Second, there is no formal mechanism for synchronising the data between the two resources. Occasionally, snapshots of data are sent by EEBO to ESTC but there is no guarantee that a correction or revision made to an ESTC entry will be replicated in the corresponding EEBO entry or vice versa: neither ESTC nor EEBO will necessarily know when the other has made a correction. As both resources continue to amend and expand their bibliographical data for their own purposes, there is an increasing likelihood of significant discrepancy between the two resources. Finally, although EEBO continues to microfilm and digitise, there is no absolute one-to-one correspondence between the pre-1701 entries in ESTC and the materials on EEBO; there are -- and will always be -- items on ESTC not available on EEBO.” (Gadd 685-6)



“unlike scholarly facsimile editions, the selection process for microfilming was often arbitrary. Copies were selected primarily by reference to the copies listed in STC and Wing, with particular preference for certain major collections; they were not selected because they were considered representative of a particular edition. By bringing together the bibliographical record for an edition and (usually but not always) only a single witness of that edition,22 EEBO is obviously aiming to provide a useful scholarly mechanism in terms of searching but by doing so are implying -- albeit not deliberately -- that the record and the copy *are* *one and the same thing*. It would be better, perhaps, if EEBO represented itself as a library of copies, rather than a catalogue of ‘titles’.” (Gadd 687)



“ in digitising the microfilms in their original forms, EEBO decided (presumably for commercial reasons rather than purely scholarly ones) against sanitising the images. Openings are retained rather than broken into single pages; images are not cropped; rulers, place-holders, and descriptive notes are left in place; blank leaves are not removed. While Kichuk’s concern about ‘remedia- tion’ is a real one, it is difficult to use EEBO for any length of time without being reminded that these are reproductions of actual objects.” (Gadd 688)  

ECCO, as a distinct resource, is a historic latecomer compared to EEBO. The initial microfilms were created in \[???\] by \[???\]. In 2003, Thomson Gale began making digital copies of the Eighteenth Century Collection microfilms available to subscribers online. The digital images were made from the microfilm masters, which were 400 dpi, and thus higher resolution than the microfilm copies. 



“the move from microfilms to the Internet has meant easier searching, easier physical access, easier manipulation of the images, clearer images and (for the UK at least) cheaper institutional access” (Gadd 685).



“However, the path to digitization was not ideal: these documents were imaged in the late 1970s, transformed into microfilm during the 1980s, and the microfilms digitized in the 1990s. Because of the state of reproductive technologies during the late 20th century, as well as the circuitous path to digitization (through microfilm), the image quality is very poor and bitonal, with no greyscale / images available. Furthermore, the original documents themselves, printed with premodern technologies, pose problems even for human readers of their pages, but much more so for optical character recognition (OCR) engines. For example, printed characters were not perfectly situated on a baseline, blackletter fonts were used, ink bled through the paper, and the typeface was broken and overworn.” (Christy et al. 1-2)

Reference “deep fried memes” --- xkcd comic: <https://xkcd.com/1683/>



The microfilms were scanned at 300 dpi (Spedding 440).



“while ESTC may be based on two thousand public and private libraries worldwide, the Eighteenth Century microfilm series is based on books from only a tiny fraction of that number - almost certainly less than twenty libraries, and rarely anywhere other than the British Library, the Bodleian, Harvard, and the Hunt” (Spedding 440)



### 2.1.3.  TCP timeline history ###  

“The Text Creation Partnership started, in 1999, as a collaboration between the university libraries of Michigan and Oxford, the Council on Library and Information Resources, and the publisher of Early English Books Online, Proquest. The aim was to create high quality ‘standardized, digitally-encoded electronic text editions’ starting with 25,000 titles from Early English Books Online.” (Gregg n. pag.)

“The Text Creation Partnership was conceived in 1999 between the University of Michigan Library, Bodleian Libraries at the University of Oxford, ProQuest, and the Council on Library and Information Resources as an innovative way for libraries around the world to:

* pool their resources in order to create full-text resources few could afford individually

* create texts to a common standard suitable for search, display, navigation, and reuse

* collaborate with commercial providers, rather than constantly bargaining and competing with them” (TCP “Welcome”)

Those four named organizers --- Michigan, Oxford, ProQuest, and CLIR --- are the tip of the iceberg for institutional buy-in. At “Our scholarly partners,” TCP lists *two hundred* institutions which were involved in at least one of EEBO-TCP Phase I, EEBO-TCP Phase II, ECCO-TCP, Evans-TCP, many of which were involved in multiple of these projects. “This list does not include higher-education institutions based in the UK, all of which are counted as EEBO Phase 2 partners thanks to consortial funding” (TCP “Our scholarly partners”). The “FAQ” page still shows traces of the enormity of this undertaking. One question reveals that each partner library committed $60,000 to the venture, an amount which funds the keying and conversion of only around 250-300 books --- a large number, but a drop in the bucket compared to the approximately 73,000 books which the TCP as a whole has encoded.

A ten-person working group developed an encoding standard. The University of Michigan Library staff developed instructions to use this encoding standard. And then the text creation began. “Texts were selected each month at Michigan, page-images were supplied by ProQuest, marked-up transcriptions were submitted by the vendors, and quality control and editing undertaken at U-M Library and soon also at Bodleian Libraries in Oxford and subsidiary sites at the National Library of Wales, Aberystwyth, and at the University of Toronto.” (TCP “About”)

  

“EEBO-TCP met its goal of producing 25,000 books in 2009 (thereafter known as “EEBO-TCP Phase 1”), and then undertook work on a second phase to convert the first edition of each remaining unique monographic work in EEBO---another 40,000 or so books, for a total of around 70,000, if all hopes were realized.” (TCP “About”)

“In 2005, the TCP executive board and staff sought to expand the TCP model to other databases of historical books, namely, Gale Cengage’s Eighteenth-Century Collections Online (ECCO) and Newsbank Readex’s Evans Early American Imprints (Evans-TCP). These projects never received quite the support attracted by EEBO-TCP, and in the end produced only about 8,000 texts, compared to the 60,000 produced by the latter, with another few thousand on the way.” (TCP “About”)

“In 2005 the project expanded to include Gale-Cengage’s Eighteenth-Century Collections Online (as well as Evans Early American Imprints by Newsbank). However, while the EEBO-TCP project flourishes (with around 40,000 texts transcribed so far), the work on ECCO-TCP stagnated at around 2,000 texts. As well as the main partner institutions of Michigan and Oxford that oess to the eighteenth-century TCP texts, so I’ve listed them below, with a few comments.” (Gregg n. pag.)

“ECCO natively supports OCR-based full-text searching of this corpus. This is significant because it meant that unlike EEBO-TCP (which produced searchable text where there was previously none at all), ECCO-TCP could only hope to produce *more accurate* text (and more reusable text) than what was already available. The larger size of ECCO (because of the great increase in printing and greatly enhanced chances of survival of printed works in the 18th century) also made it a different proposition: nothing so ambitious as EEBO-TCP coverage was feasible for ECCO-TCP. ... Because of these greater challenges facing ECCO-TCP, it is perhaps better described as a proof of concept than as a completed project. With the support of more than 35 libraries, the TCP keyed, encoded, edited, and released 2,473 ECCO-TCP texts. A further tranche of 628 texts was keyed and encoded but never fully proofed or edited. The texts in this group remain useful for many purposes, however, and bring the total of ECCO-TCP texts to over 3,000.” (TCP “ECCO”)  

“Begun in 2009, Phase II both shrank and expanded the scope of EEBO TCP.  Selection became more discriminating and focused more on English-language (and Welsh- and Gaelic-language) texts to the exclusion of French and Latin titles, and also set aside the serials (periodicals) as a fit project for another time. But within the constraints of English-language monographic titles, it aspired to something approaching comprehensive treatment: EEBO Phase II planned to convert each and every unique work in Early English Books Online (usually the first edition), or an estimated total of around 45,000 books on top of the 25,000 completed in Phase I. This was an ambitious, and always risky, goal. As it happened, enough institutions joined Phase II to fund the completion of about 40,000 titles, of which about 35,000 have been released to date, the remainder slowly working their way through the production pipeline. (TCP “EEBO”)

“As of 2019, the total number of books available in Phase II came to 34,963, with a further release of several thousand additional titles tentatively scheduled for later in the year.  Short of an infusion of new funding, or the adoption of a new production model, this should bring the active work of the TCP to at least an interim conclusion.” (TCP “EEBO”)  

Two questions in the FAQ, “Why would I buy something that is achievable only if others do the same?” and “Why would I buy something that is going to become freely available?” taken together imply the speculative and ambitious nature of the original project. In the official answers provided to these evidently frequently asked questions, there is a sense that the project posed a prisoner’s dilemma: every individual institution’s “best” move, from a game theory perspective, was to contribute nothing to the project and then reap the benefits of everyone else’s work, but if every institution did so, then no one would benefit. A close reading of both responses illuminates an interesting tension in values:

Why would I buy something that is achievable only if others do the same?

Mere calculation may have disinclined some libraries from joining. TCP partnership was always less a purchase than an (admittedly risky) investment, since all of Michigan’s projections for the TCP corpus depended on a certain optimistic assumptions about how many other institutions would join.  Some libraries may have joined out of faith in Michigan’s track record, or because of a long-standing connection with the University or its staff. Some out of an idealistic belief in the collaborative model that TCP represented or in the public value of the product it promised. Some perhaps out of a cost-benefit risk estimate. For all the partner libraries, however, TCP membership was in effect a commitment to fellow libraries to share the burden and reward of this work. Partner libraries contributed to the cost of producing tens of thousands of painstakingly produced electronic editions of early English works. Each new library that joined made it possible for the project to key books that we otherwise would not, improving the corpus for everyone.

Why would I buy something that is going to become freely available?

This question too has no obvious answer that will please everyone, and indeed this question may have influenced some potential partners to refrain from joining. The structure of the TCP, with its provisions for exclusive access for a time, followed by public release, was something of a balancing act, designed to encourage membership by both those who were unwilling to wait ten years or so for access to the texts on behalf of their students and faculty, and those who believed in the creation of an unrestricted public resource and were willing on altruistic grounds to contribute to it.  Regardless of their motives for joining, the success of the EEBO-TCP depended on the support of partner institutions. The partnership fee directly funded the conversion of new books,  and greatly affected the rate at which the work was carried out. By joining up,  a library not only gained immediate access to the texts,  and not only contributed to making a larger, more comprehensive corpus for everyone, but also measurably affected the pace, and advanced the completion date, of the project--and thereby advanced the date at which  the texts would be released to the public.

The answers weakly attempt to provide the game-theory argument, but also carry the sense that the true answer, which they would like to give, has nothing to do with game theory and rationality, and everything to do with transcendent values of scholarship itself. In the minutes to the 2001 meeting of their executive board, they open with: “the project should determine more clearly if it is ‘a partnership or a product.’ Comment was offered that it is a ‘partnership to create a product’” (TCP Executive Board, 2001)



The TCP is thus an intervention into \[stuff about money.\] They describe the project as “a public-private partnership, led by libraries” (TCP, “About”) and emphasize the “librarian’s attitude toward content” which prioritizes the widest possible access and use. This “librarian’s attitude” is most evident in the (eventual) availability of all of the transcriptions in the public domain, despite the fact that the images they are based on remain privately restricted by the companies which own them. Their description of the “partnership,” however, continues to show signs of the strain in value systems when commercial and noncommercial goals are intertwined: “Through our partnership with private vendors, we had access to a huge trove of images from which to transcribe. In return, these companies were supplied with a full-text index to their images ---work which would have otherwise been difficult or expensive to produce.” In other words, through purchasing a service (access to images), the academic institution received that service. These institutions carried out an enormous feat of labour at their own expense, using the service they purchased. Then, “in return,” they provided the results of their labour for free to the company, for the company to then further profit from the improvements to their service. Most telling, here, is the word “otherwise.” The suggestion here is that, without the TCP, the companies themselves would not have been willing to undertake the encoding (so desired by the users of their service) because it would be difficult and expensive. However, the TCP certainly did not make the task any less difficult or expensive. Instead, academic institutions absorbed the difficulty and expense on those companies’ behalf. I do not say that they were wrong to do so: on the contrary, the “librarian’s attitude” mirrors my own attitude, and it is surely to everyone’s benefit for a wonderful thing to exist even if that wonderful thing is not profitable. Rather, I highlight this rhetorical moment in the TCP’s self description to suggest that \[it takes two to collaborate.\] One of the three key aims of the TCP identified on the homepage is to “collaborate with commercial providers, rather than constantly bargaining and competing with them” (TCP “Welcome”). However, the TCP seems instead to have simply come up with a *better* bargain, one which creatively offers scholarly labour as a bargaining chip.  

“Selection of works to transcribe for EEBO Phase 1 was initially based on named authors mentioned in the New Cambridge Bibliography of English Literature.  Though this tended to bias selection a bit toward canonical, or at least attributed, works, anonymous works may also have been selected at this stage if their titles appeared in the bibliography. The New Cambridge Bibliography of English Literature was chosen as a guideline because it included foundational works as well as less canonical titles related to a wide variety of fields, not just literary studies. In any case, this initial reliance on the New Cambridge soon gave way to a series of deliberate attempts to cast a wider net, for example by selecting works exemplifying a particular theme (food, drugs, piracy, witchcraft), or fitting a particular format (broadsides, pamphlets, etc.)  The intention was to supplement methodical selection with more or less random selection based on arbitrary criteria in order to expand the generic diversity of the corpus. Requests for particular works by faculty at partner institutions were also taken into consideration and, if feasible, placed at the head of the queue. A user willing and able to make a case for a given work almost always prevailed over other considerations.” (TCP “EEBO”)  

Project Gutenberg began in 1971 with one individual, Michael Hart, who did not begin with a specific project vision in mind. From the beginning, then, Project Gutenberg was not goal-oriented in the same way as the other resources under discussion. By this I mean that Project Gutenberg orients itself toward goals of a fundamentally different kind than the goals which structure other textual archives, not that it has no goal. Project Gutenberg is, in general, subject to being dismissed as unserious or lacking rigorous standards, but I argue that these dismissals come from a failure to recognize and respect the real goals, seriousness, and standards which drive the project. In the case of the project’s founding, that goal was not, as in the case of the other databases under discussion, to provide a particular kind of access to a particular kind of texts. Instead, the goal of Project Gutenberg was born from a moment of happenstance and nepotism by which Hart, a student at the time, was donated $100,000,000 of computer time on the Xerox Sigma V mainframe at the Materials Research Lab at the University of Illinois. As Hart described it, he “decided there was nothing he could do, in the way of "normal computing," that would repay the huge value of the computer time he had been given ... so he had to create $100,000,000 worth of value in some other manner” (“History and Philosophy”). Rather presciently for 1971, Hart concluded that the greatest value computing would offer was the storage, searching, and retrieval of other materials. He therefore typed up and distributed the Declaration of Independence. This became the first text of what would eventually become Project Gutenberg. It might even be considered the first ebook (according to Lebert 2008). Project Gutenberg was certainly “the first information provider on the internet and is the oldest digital library” (Lebert).

“During the fist twenty years, Michael Hart himself keyed in the first hundred books, with the occasional help of others from time to time.” (Lebert)

“when we started, the files had to be very small as a normal 300 page book took one meg of space which no one in 1971 could be expected to have (in general). So doing the U.S. Declaration of Independence (only 5K) seemed the best place to start. This was followed by the Bill of Rights --- then the whole US Constitution, as space was getting large (at least by the standards of 1973). Then came the Bible, as individual books of the Bible were not that large, then Shakespeare (a play at a time), and then into general work in the areas of light and heavy literature and references.” (Hart “History and Philosophy”) “That edition of Shakespeare was never released, due to copyright changes. If Shakespeare's works belong to the public domain, the comments and notes may be copyrighted, depending on the publication date. But other editions belonging to the public domain were posted a few years later.” (Lebert)

“When the internet became popular, in the mid-1990s, the project got a boost and an international dimension. Michael still typed and scanned in books, but now coordinated the work of dozens and then hundreds of volunteers in many countries.” (Lebert)

“In August 1989, Project Gutenberg completed its 10th book, The King James Bible, that was first published in 1611, with the standard text dated 1769. In 1990, there were 250,000 internet users, and the standard was 360 K disks. In January 1991, Michael typed in Alice's Adventures in Wonderland, by Lewis Carroll (published in 1865). In July 1991, he typed in Peter Pan, by James M. Barrie (published in 1904).” (Lebert)

“By the time Project Gutenberg got famous, the standard was 360K disks, so we did books such as Alice in Wonderland or Peter Pan because they could fit on one disk. Now 1.44 is the standard disk and ZIP is the standard compression; the practical filesize is about three million characters, more than long enough for the average book.” (Hart “History and Philosophy”)

“Project Gutenberg gradually got into its stride, with the digitization of one book per month in 1991, two books per month in 1992, four books per month in 1993 and eight books per month in 1994. In January 1994, Project Gutenberg celebrated its 100th book by releasing The Complete Works of William Shakespeare.” (Lebert)

”The number of electronic books rose from 1,000 (in August 1997) to 5,000 (in April 2002), 10,000 (in October 2003), 15,000 (in January 2005), 20,000 (in December 2006) and 25,000 (in April 2008). ... The steady growth went on, with an average of 8 books per month in 1994, 16 books per month in 1995, and 32 books per month in 1996.” (Lebert)

“A fast growth thanks to Distributed Proofreaders, a website launched in October 2000 by Charles Franks to share the proofreading of books between many volunteers. Volunteers choose one of the books listed on the site and proofread a given page. They don't have any quota to fulfill, but it is recommended they do a page per day if possible. It doesn't seem much, but with hundreds of volunteers it really adds up.” (Lebert)

“If 32 years were necessary to digitize the first 10,000 books, between July 1971 and October 2003, 3 years and 2 months were necessary to digitize the following 10,000 books, between October 2003 and December 2006.” (Lebert)  





“There are three portions of the Project Gutenberg Library, basically be described as:

Light Literature; such as Alice in Wonderland, Through the Looking-Glass, Peter Pan, Aesop's Fables, etc.

Heavy Literature; such as the Bible or other religious documents, Shakespeare, Moby Dick, Paradise Lost, etc.

References; such as Roget's Thesaurus, almanacs, and a set of encyclopedia, dictionaries, etc.

The Light Literature Collection is designed to get persons to the computer in the first place, whether the person may be a pre-schooler or a great-grandparent. We love it when we hear about kids or grandparents taking each other to an etexts to Peter Pan when they come back from watching HOOK at the movies, or when they read Alice in Wonderland after seeing it on TV. We have also been told that nearly every Star Trek movie has quoted current Project Gutenberg etext releases (from Moby Dick in The Wrath of Khan; a Peter Pan quote finishing up the most recent, etc.) not to mention a reference to Through the Looking-Glass in JFK. This was a primary concern when we chose the books for our libraries.

We want people to be able to look up quotations they heard in conversation, movies, music, other books, easily with a library containing all these quotations in an easy to find etext format.” (Hart “History and Philosophy”)  

The founding logic of Project Gutenberg resonates strikingly with Bordieu’s call to “*universalize in reality the conditions of access*” (qtd in Guillory 340, emphasis original) to literature.

The first Project Gutenberg texts are almost a parody of important texts: The Declaration of Independence, The King James Bible. These are the texts assumed to be urgently desired by “99% of the general public” (Hart “History and Philosophy”). They are then followed, however, by a work which has rarely been central to the institutional hierarchies of cultural capital: Alice in Wonderland. As Hart describes his choices of what texts to transcribe next, he seems to be describing a version of what Guillory hoped for, “another kind of game” in which texts can compete for cultural capital, a game “with less dire consequences for the losers, an *aesthetic* game” (Guillory 340, emphasis original).



“Project Gutenberg selects etexts targeted a bit on the "bang for the buck" philosophy ... we choose etexts we hope extremely large portions of the audience will want and use frequently. We are constantly asked to prepare etext from out of print editions of esoteric materials, but this does not provide for usage by the audience we have targeted, 99% of the general public.” (Hart “History and Philosophy”)



### 2.1.4.  Hathi timeline history ###  

To review all of these events, \<$n#table:databases-timeline\> shows the milestones of all of these databases in chronological order.  

1918	Pollard first proposes a “short-title handlist”

1926	Pollard and Redgrave Short-Title Catalogue for 1476--1640

1938	Eugene B. Power founds University Microfilms

1945	Wing starts collecting his STC, 1641--1700

1951	Donald Wing’s catalogue for 1641--1700, first edition

1971	First text in what would be Project Gutenberg. Over the next twenty years, Michael Hart personally keyed the first hundred books.

1972	Beginning of second ed of Wing STC, 1641--1700

1976	Proposal for Eighteenth Century Short Title Catalogue, British Library and the American Society for Eighteenth Century Studies

1976	Second edition, vol 1, of Wing’s STC

1976	Beginning of second ed of Pollard & Redgrave STC, 1475-1640

1977	ESTC pilot begun at British Library, directed by Robin Alston

1979	ESTC: Libraries from USA, Germany, and Australia began contributing to ESTC

1980	ESTC database available via British Library BLAISE \[British LibraryAutomated Information SErvice\]

1981	Research Publications, Inc begins microfilming books

1981	ESTC database available via US Research Libraries Group RLIN \[Research Libraries Information Network\] system

1983	ESTC catalogue of BL holdings and indexes published in microform

1983	*Eighteenth Century Collection* microfilm produced by Research Publications, Inc

1985	ESTC online databases in RLIN and BLAISE upgraded to allow dynamic updates to a single shared file

1986	Second edition, vol 2, of Wing’s STC

1987	ESTC expanded scope to add all print prior to 1700, changing its name to the English Short Title Catalogue. Information from Wing and STC is added to ESTC.

1989	Project Gutenberg completes its tenth book, the King James Bible

1991	End of second edition of Pollard & Redgrave STC, 1475-1640

1991?	Exhaustive index to Wing’s STC --- after which Bibliographical Society no longer supported Wing

1992	ESTC expanded scope to add serials

1994	ESTC made pre-1700 records available

1994	Project Gutenberg completes its 100^th^ book, the Complete Works of William Shakespeare

“By the late 1990s, several thousand reels had been published in two series: ‘Early English Books, 1475--1640’ and ‘Early English Books, 1641--1700’.” (Gadd)

1997	Project Gutenberg publishes its 1000^th^ book, La Divina

Commedia di Dante, in Italian

1998	ESTC second edition released on CD-ROM

1998	Conclusion of second ed of Wing STC

1998	Beginnings of EEBO: University Microfilms (now ProQuest) began to make available digitised copies of its microfilms across the Internet to subscribing institutions

199	9	ESTC assumed official responsibility for receiving new Wing STC data

1999	TCP began

2000	Project Gutenberg: Charles Franks launches Distributed Proofreaders

2003	ESTC third edition released on CD-ROM

2003	Beginning of ECCO: Thomson Gale (now Gale Cengage Learning) made digital copies of Eighteenth Century Collection microfilms available to subscribers online

2005	TCP begins encoding ECCO texts

2006	ESTC made available to search free online; ESTC begins transcribing full title and imprints

“As of 26 April 2007 the number of microfilm reels of The Eighteenth Century that had been released was 16,625; the total number of titles on these reels is 189,569 (information provided by Katri Russick, Thomson Gale, Australia and New Zealand, in a private email). This number increased to at least 17,828 microfilm reels - the number received and catalogued by Monash University - by 1 November 2010.” (Spedding 450)

2008	Project Gutenberg publishes its 25,000^th^ book, English Book Collectors, by William Younger Fletcher

2009	EEBO-TCP Phase I complete: produced 25,000 books; beginning of Phase II

2015	EEBO-TCP Phase I books released to the general public

2021	EECO-TCP Phase II books released to the general public



Table 1: A chronological history of major events in the development of \[LIST THE DATABASES\].



  

“ProQuest is to be commended for its attitude to the wider scholarly community.27 EEBO is a commercial product but nonetheless there is an encouraging and genuine wish to engage with its users. This ranges from the active monitoring and rapid responding to queries submitted via its ‘Webmaster’ form to informal and formal consultations with students, scholars, and other users. EEBO representatives appear at -- and often sponsor -- academic events. Content is frequently corrected, updated, expanded and enhanced (such as the new ‘EEBO Introduction Series’); the searching mechanism continues to be improved; the project to produce full-text transcriptions (the Text Creation Partnership) is an academic venture, not a commercial one. Unlike Jackson’s microfilm photographer, lurking in his lair with his livid lights and chemical smells, the present providers of EEBO seem to be rather more interested in -- and responsive to -- contemporary scholarship.” (Gadd 688)  

“It is perhaps the inconsistency of the OCR readings, obvious from this rough assessment, that makes Gale bashful about the restricted files. The editors of the ESTC confess to their public resource being a construction site in a way that the proprietors of private, commercial websites like ECCO prefer not to. But few really mind these days, especially if improvements are seen to be ongoing. Gale should be more relaxed about the incompleteness of their work, though perhaps not quite so relaxed as they are with the Burney Collection.” (Bullard 756)  

“Viewing the field of eighteenth-century digital humanities as a single prospect, it is the contrast between publicly funded, open-access sites, and privately owned, subscription- access resources that is most striking. Each side of the divide has much to learn from the other. Publicly funded academic projects must acquire the pragmatism and ambitiousness of scale that commercial developers have always shown. Commercial developers must adapt themselves more generously to the principles of scholarly openness and accuracy. They might also imitate the inventiveness of the open sector, its adaptability to the demands raised by different kinds of primary media. Both sides recognize the desirability of making their resources interoperable across the divide, and the business of interconnectivity will preoccupy all kinds of digital humanist in the coming decade. Another set of players likely to step further forward in future years is the university presses.” (Bullard 756)  

The “microfilm series is not a random - and therefore randomly representatve--- selection of items from ESTC. Texts have been selected for filming on the basis of criteria that are rarely mentioned, but which include ease of access for filming (initially, items at the British Library) and the desire to avoid duplication of texts. That is, by the desire to get the biggest bang for Gale’s buck.” (Spedding 441)



“There may also be commercial considerations at work. Alt not conducted a systematic search for items from the British L Case" (its collection of erotic material), it seems that little of tha the Eighteenth Century microfilm series, and the material that h has only been quite recently added.37 Consequently, much of this ing from ECCO. The reason for this may be that much Private Ca as late as 1989, not represented on ESTC,38 but it may also be be Private Case was microfilmed by Adam Matthew Publications in under the title Sex and Sexuality 1 640-1 940 . That is, the eighteent terial in the Private Case may have been withheld from the Eigh microfilm series (and consequently ECCO) to ensure the profitab Sexuality. Similar, and similarly hidden, criteria seem to affect o such as EEBO and Goo” (Spedding 441)  

What is *in* the TCP? Well, when active transcription was taking place, “users (especially those affiliated with partner libraries) were welcome to request works from EEBO that had not yet been keyed, and that their requests would go to the top of the queue” (TCP “FAQ”). So --- the TCP contains whatever individual works happened to interest particular scholars.



The TCP, unlike the ESTC and ECCO, intentionally avoids including multiple editions of a given work. This decision was a pragmatic one motivated by “limited funding” and a sense of scarcity: “Simply put, for every book that we chose to convert, a different book does not get converted: duplication, even partial duplication, has its costs” (TCP “FAQ”). Since the TCP never envisioned itself as a fully complete collection, the priority in textual selection “was always to capture as many different works and as great a variety of text as we could, usually focusing on the first edition of each work”(TCP “FAQ”). To a certain extent, this lack of duplication can be useful for text-mining: it places all texts on an equal playing field, rather than double-dipping on some works. However, they “have keyed additional editions where there is sufficient justification for doing so, and a user has made a case for it,” so the corpus cannot be assumed to contain *no* duplicated works (TCP “FAQ”).



## database models ##



#### 2.2.1.1.  edition & ideal copy ####



One reason that it can be informative to close-read the data structures of a resource like the ESTC is that a resource’s categories of knowledge are driven by the *uses* to which it expects that knowledge to be put. Examining the implicit assumptions that will make a given organization of knowledge seem logical, we can work backwards to the purpose of mission of the initial knowledge creation. Thus Tabor describes the data structure and the mission of the ESTC in a single statement: “ESTC’s most basic bibliographical function is to provide, for each edition, a description of the ideal copy, meaning the most complete and correct manifestation of that edition as the printer and publisher intended it” (369). Korshin further elaborates the use envisioned for this information: “the ESTC's cataloguing rules have been devised in such a way that a scholar anywhere in the world can tell, from the ESTC entry, whether the copy of the book in his or her library is the same or different from the one listed in ESTC” (211). Both “edition” and “ideal copy” are terms defined around the interests of a specialist audience of bibliographers, which bear inexact but important relationships to the formulation of an ESTC record.

An “edition” is a group of copies of a work which are understood to be interchangeable with each other (Tabor 369),[^cf27] though in practice different levels of granularity are applied in distinguishing between editions. The ESTC sometimes has separate entries for groups within an edition “when certain separately planned marketing units can be identified within the edition, such as reissues, imprint variants, and large versus regular-paper copies” (Tabor 369). Karian describes that “\[s\]ometimes the ESTC contains additional records if there are multiple *states* of an edition (a different state results from cancels or minor changes to the setting of type)” (289). Or, in “the later eighteenth century, when reprints from standing type became more common, ESTC cataloguers have occasionally granularized down to the level of individual impressions” (Tabor 369). As a result, Karian argues persuasively that ESTC records should not be treated as synonymous with “editions,” “issues,” or “titles,” since the same definitions of those boundaries may not be applied consistently. The specific question he poses is “What is the unit that the ESTC uses?” (289), and important question, to which the answer cannot really be “editions,” despite the best attempts of the ESTC bibliographers. Instead, he says “one should refer instead only to the ESTC record, a unit created by the ESTC and having no meaning outside the ESTC” (Karian 289).[^cf28]

The “ideal copy,” too, represents an interpretation. Because the ESTC is essentially a movel based on limited samples of an imagined lost prior whole --- “the most complete and correct manifestation of that edition as the printer and publisher intended it,” as Tabor termed it (369) --- a new sample can change the model. As Tabor describes, “\[a\]s additional reports of copies arrive, it may be that the ideal description must change in response. For instance, the existence of a half-title may only emerge on the evidence of the seventh copy reported. A half-title would then be added to the description of the ideal copy, and the six previously matched physical copies will receive notes recording that they are imperfect in this respect” (370). The ideal copy, like the database itself, thus represents a moving target.

  

So, how do these ideas of the edition and the ideal copy shape the data structures employed in the building of the ESTC? Consulting an individual ESTC record in the online database, as we can see in Figure 6, reveals a lot of information all pointing ‘outside’ of the ESTC itself. It begins with six details which will be present for every title: the “System Number” and “Citation Number” uniquely identifying the record; the author; the title; the publication information; and a physical description. It then displays any uncategorized “notes,” which in the case of *The Emigrants* (1793) consist of two additions to the physical description. \[Add other examples of “general notes”?\] The entry then points ‘outward’ to two “Surrogates”: the microfilm, and the electronic reproduction of the microfilm which is collected in ECCO. A very brief description is made of the work’s content --- its subject is “English poetry --- 18^th^ century” and its genre/form is “Poems” --- which is the only information provided about the *work* rather than the *book*.[ There’s a lot more to be examined re: these subject headings, especially if I do topic modelling for contents.



“Because the ESTC was formed from three different projects over many decades, the existence of subject headings varies. For books that are from the periods 1475-1640 (from the original Short-Title Catalogue created by Pollard and Redgrave) and 1641-1700 (from Donald Wing’s short-title catalogue), subject headings exist. But subject headings were not initially created for the eighteenth-century records in the ESTC, and so subject headings rarely occur for the items in ECCO.” (Karian, “Guide” 3)



How many of my records have subject headings? What is the ESTC’s ontology?] The remainder of the record is an extensive list of libraries which hold copies of the edition, divided into the three geographic regions of “British Isles,” “North America,” and “Other,” followed by a direct link to the ECCO copy referenced above.

This, however, is only how the ESTC *displays* its contents. Clicking another tab makes visible the MARC tags in which the data itself is stored. The MARC tags encode information at a slightly more refined level of detail. For example, the publication location in the standard view is listed as “Publisher/year” and displayed as the string “London : printed for T. Cadell, in the Strand, 1793.” A human can parse that string, but as the MARC version of the same information reveals, it is made up of three points of information that have been combined. The MARC data is listed as “260,” which is the MARC standard code for “Publication, Distribution, etc.” The line itself is displayed as “\|a London : \|b printed for T. Cadell, in the Strand, \|c 1793” --- indicating three separate pieces of information in the subfields “a - Place of publication, distribution, etc.”, “b - Name of publisher, distributor, etc.”, and “c - Date of publication, distribution, etc.” The separation of these points of information in the underlying MARC data is what allows the online database to conduct searches based on publisher, publication location, and date of publication. Even this is a reformatting of the underlying MARC code, which would read “##$a London :$b printed for T. Cadell, in the Strand,$c 1793” --- with the two “#” symbols at the beginning encoding that this is the first edition.[^cf29] It is, of course, only sensible for the ESTC to reformat its MARC code for display: MARC stands for MAchine Readable Catalogue, and machines and humans have very different needs as readers. However, what this exploration reveals is that \[???[ Is it that the categories of information are made less explicit as they are translated for humans, relying on the expert reader’s interpretive skill?]\].  

Figure 6: A screencap of the ESTC record for Charlotte Smith’s *The Emigrants* (1793).  

There are several different ways to search ESTC records. The “Search” button takes a user to the “Basic Search” function, from which there are also links to “Advanced Search,” “Browse,” and “Browse Libraries List” (which takes the user to the identical page as “Browse” but with “Library name” pre-selected as the index to browse). Once you have found a work of interest, however, several new forms fo searching become available, implied in the hyperlink formatting: almost any field in the entry can be clicked to reach other matching ESTC entries.



### 2.2.2.  ECCO model ###



An edition being “included” in ECCO looks different from its inclusion in the ESTC --- whereas the ESTC lists just one record for each multivolume work, ECCO lists each volume separately, with links to the other volumes available in the “full citation” for the volume.



“Such consideration for users is sadly rather less visible with EEBO’s eighteenth-century equivalent ECCO. Unlike EEBO, ECCO presents users with a single, cropped page. In so doing, it has taken the opportunity to remove every blank page that ever appeared in an eighteenth-century book (pace its claim to provide ‘digital images of every page of 150,000 books published during the 18th Century’).” (Gadd 688)



“Unlike EEBO, ECCO includes an underlying text-transcription of its entire collection, which users can search but cannot access in any other way. According to ECCO’s online guide, the full-text transcriptions are generated using computerised ‘optical character recognition’ of the digitised images of \[start page 689\] each page, with ‘proprietary software created by the vendor to improve OCR accuracy, including the ‘correction’ of old English f/s ligatures and other spelling and character variants’; in addition, there are elaborate quality control systems:

For every digitized page of data, eight specific items are sampled for accuracy and correctness. Each page is visually scanned for glaring errors or omissions. Every 20th page is read in its entirety. (‘About Eighteenth Century Collections Online’)

With this in mind, it comes as somewhat of a surprise to discover that, according to ECCO, the word ‘fuck’ or versions thereof appear over 28,000 times in print in the eighteenth century. Finally, unlike EEBO, feedback from users does not seem to be much valued: the technical support e-mail address provided on its help pages no longer seems to be valid.” (Gadd 688-9)



“In various different ways, ECCO is less open with its users than is usual in academia. It likes to tidy away noisy information. The microfilms on which it is based reproduce images of the openings of books, but ECCO chops each opening into two single-page images and dispenses with all the detail (the rulers placed against type, the blank pages, and the indicators of physical structure) that is so precious to bibliographers. Since 2009, ECCO documents have been supported by MARC descriptions, drawn (presumably) from the ESTC and supplemented by Gale’s own ‘Subject’ entries. Users can search by subject, and yet the MARC files are not directly accessible for checking by readers using ordinary institutional subscriptions. Similarly, there is still no accurate way to identify by class- or shelf-mark the actual copy of a book that ECCO reproduces. An indication of the source collection is always given, but the larger libraries that the original microfilm photographers tended to favor often keep multiple copies of editions, and it is seldom possible to discover which copy has been used without a personal visit to the archive. This omission has several consequences: readers are further distanced from experience of the original material object; local features (such as MS annotations on the original) are inadequately documented; and the widespread phenomenon of stop-press variants within editions of hand-press era books is forgotten. Most frustratingly of all, ECCO does not allow users access to the scanned optical character recognition (OCR) documents that its full-text searches run on.

This secretiveness inevitably arouses suspicions about the accuracy of the scans.” (Bullard 755)



A glowing 2004 review of ECCO in the “Database & Disc Reviews” section of Library Journal says “The Advanced Search is so powerful it gave me sensory overload.” (LaGuardia 124)



### 2.2.3.  TCP model ###



Although the ECCO-TCP now seems obviously built for text-mining distant reading, in fact it is largely organized around searching and consulting individual works.





### 2.2.4.  Hathi model ###  

The structuring principle of Project Gutenberg is its missions to make books available for pleasure reading. I argue that its core concept, analogous to the “edition” in the ESTC, or the “book” in ECCO and HathiTrust, is the “story.” Many of the priorities of Project Gutenberg which seem incompatible with scholarly approaches to textual history are explained by thinking of Project Gutenberg as being structured around “stories” rather than “books.”



“At the end of 1993, Project Gutenberg's eTexts were organized into three main sections: a) "Light Literature", such as Alice's Adventures in Wonderland, Peter Pan or Aesop's Fables; b) "Heavy Literature", such as the Bible, Shakespeare's works or Moby Dick; c) "Reference Literature", such as Roget's Thesaurus, and a set of encyclopaedias and dictionaries. This organization in three sections was abandoned later for a more detailed classification.” (Lebert)  

The article ”Quantitative patterns of stylistic influence in the evolution of literature” uses Project Gutenberg --- do mathematicians not know what a good source of literature is, or do they know better than us?



Cite Hammond’s book re: comparing modernists to bestsellers --- he can’t always find bestsellers, it depends on whether bestsellers were enjoyed enough for someone to bother to type them up  

Google Books prioritizes low-quality information over *no* information. The algorithmic extraction of publication dates from title pages, for example, can never be perfect. But algorithms give their predictions with certainty estimates: if accuracy was a higher priority, Google Books could calibrate the algorithm to simply provide no answer when none of the possibilities cross a given certainty threshold.



Per <http://languagelog.ldc.upenn.edu/nll/?p=1701> , they actually OVERWRITE metadata provided by partners with their algorithmic information!! They could very easily *not*.  

Like literary canons, these corpora --- especially smaller ones, like the Eighteenth Century Collections Online Text Creation Partnership --- are vulnerable to a critique of their selection methods on the grounds of representation. However, unlike the various changing literary canons of the past, digital corpora tend to conceal which particular titles have been selected as representative. I argue that Charlotte Smith’s inclusion in these resources lags behind a scholarly consensus which sees her as increasingly important and canonical in the period. Her partial inclusion in ECCO-TCP seems particularly likely to lead to ill-supported conclusions by researchers who might easily assume that their text-mining research is taking her works into consideration. However, since none of her sonnets are included, nor any of the politically radical novels which made up a substantial portion of her latter career, nor any of her natural history, some of her most important contributions to the literature of the period are not able to impact studies in which they would be relevant. In particular, a study of women’s writing through the lens of the ECCO-TCP would emphasize the most conventional and expected women’s writing from Smith, with four volumes of one of her more straightforward marriage plot novels.

Exploring the technical affordances of the copies of Smith’s works available in each database also shows why the distorted impression of Smith’s works reflected in the ECCO-TCP’s corpus is likely to persist and continue to be reproduced: without the foundation of a reliable but transformable text (in the form of a human-corrected transcription, rather than a page image or machine OCR), there is a nearly insurmountable technical barrier before any individual project. Even to assess the accuracy of the OCR texts in ECCO and HathiTrust, I must rely on ECCO-TCP. Guillory has already argued persuasively that representation in literary canons is a matter of selection, not of exclusion, that the default state for a given text is not to be included. For Guillory, this serves as a proof that sexism and racism are rarely the direct cause of a particular text lacking canonical status; the role of social oppression in limiting textual representation occurs before scholars make their choices, when classes of people are systematically excluded from the means of textual production in the first place, limiting what we may select from. In the case of digital corpora, also, I see that the rhetoric of “exclusion” is not accurate, and directs attention away from the more complex systems at play. Although I critique the failure of ECCO-TCP to include important and relevant works by Charlotte Smith, it does not seem that she has been excluded out of a prejudice against women’s writing. Most likely, The Emigrants and Celestina were chosen because copies were conveniently accessible to a particular scholar involved in the creation of the ECCO-TCP, perhaps even directly related to a research question which would motivate them through the mind-numbing process of retyping long volumes of prose. Once these works had entered ECCO-TCP, they will naturally be re-used for text mining research which implicitly trusts the original selection. In this way, representation in digital corpora is a matter of infrastructure.  

We are on the cusp of eighteenth century OCR meeting the standards of twenty-first century OCR. What texts should be OCR’d, by whom, and what should be done with those text files?



# Raw Writing #  

“One implication of the publication history of short-title catalogues is that they have been deemed functional and valuable even before they were complete. (That estimation is crucial, for their full completion is for all practical purposes impossible.) Judging that even a preliminary form of the records was useful to scholars, the planners of ESTC determined to conduct its development ‘in full public view’ and to make the incomplete file available ‘warts and all’ (in the words of Henry Snyder and Michael Crump, responding to criticism by Peter Blayney)” (Vander Meulen 270).  





  

“The Project staff found that many eighteenth-century books in hundreds of libraries around the world have never been / catalogued at all, or are described in a group heading,” especially for single-sheet items (Korshin 210-211). “Panizzis ‘Rules' lead to confusing entries and filing for anonymous entries or for items with corporate authorship. For these, and many other related reasons, Alston and Jannetta decided to write their own cataloguing rules, allowing their entries eventually to be converted into machine-readable form, but differing slightly from the standards for machine-readable cataloguing devised in this country (Library of Congress MARC) and in the United Kingdom (UK MARC). Modern machine-readable cataloguing has been devised to deal with cataloguing new books and serials; the ESTC's cataloguing rules have been devised in such a way that a scholar anywhere in the world can tell, from the ESTC entry, whether the copy of the book in his or her library is the same or different from the one listed in ESTC.” (Korshin 211)



“Because the ESTC was formed from three different projects over many decades, the existence of subject headings varies. For books that are from the periods 1475-1640 (from the original Short-Title Catalogue created by Pollard and Redgrave) and 1641-1700 (from Donald Wing’s short-title catalogue), subject headings exist. But subject headings were not initially created for the eighteenth-century records in the ESTC, and so subject headings rarely occur for the items in ECCO.” (Karian, “Guide” 3)





“In ESTC the matching process hinges on five points of identity: the title, as far as it is given by ESTC; the edition statement; the imprint, again as far as it is given by ESTC; the pagination; and the format” (Tabor 370).  

Using the database:

“the needs of the specialist audience that has formed ESTC’s main constituency in the past and may reasonably be expected to continue as such: the explorers in the field who need detailed maps. In my task of mediating between the file and its users, I find that these people approach early books with questions regarding one or both of two broad topics: intellectual content and physical characteristics, the latter including the location of copies,” with most interested in “the physicality of books.” (Tabor 369)

“When everything is working right, as it does in ESTC more often than not, the bibliographical description is completely accurate as far as it attempts to go, and variations in the matched copies can be reliably determined by the presence or absence of copy-specific notes. If anything happens to disturb the links between the description and its attached holdings, including errors committed in the course of creating the description, the record gets broken and starts telling lies about physical copies or, worse, about the ideal copy” (Tabor 370)



“the ESTC is one of the best resources to identify relevant printed materials, determine their publishing histories, and find out where they can be examined” (Karian 283)  

The ESTC is not actually only one database: “The STAR file[ is this still true?], maintained at the ESTC editorial office in Riverside, California, has been functioning for years as a repository for revisions that are transmitted in periodic updates to the publicly accessible file. ... In consequence, though invisible and inaccessible to most of ESTC’s users, STAR contains the most up-to-date version of the file.” (Tabor 373) There are two kinds of information, in particular, which the STAR file contains and which is concealed from ESTC users: the true ‘verified’ status of individual edition matches, and edition-specific notes on known errors.

A “match” in the ESTC is a known copy of a book held in a library which serves as a representative of a particular edition. A match can be encoded as “verified” when an ESTC staff member consults the book to confirm that it corresponds to the edition in question, or “unverified” when this has not happened. But the STAR copy contains two additional verification statuses: when a contributor such as a librarian or a scholar submits a match, it is coded as a “web match.” “Because most outside contributors have received no training in the matching process from ESTC, it would be fair to say that these matches occupy a level of certainty somewhere between the ‘verified’ and ‘unverified’ levels” (Tabor 374). Automated uploads of various outside databases can also generate new matches, encoded as “catalogue matches;” these “have a comparatively low level of reliability” (Tabor 374). However, in the transition from STAR to ESTC, “The standard holdings display smoothes over these nuances; here both catalogue and web matches are translated to ‘verified’. This forces the file to make categorical statements of certainty even in obvious cases of considerable doubt” (374).

\[STAR annotations and DFONOTEs\]  

Transcription errors



Broken records

“A broken record occurs when information added to a record has not been checked against the copies already listed under that record” (Karian 285)



Inaccurate dates

‘The ESTC sometimes records the date as questionable, and sometimes records the date within a range. But when one does a large-scale search for records---for example, everything from 1720---there is no easy way to screen out items not definitively dated to 1720” (Karian 291)



“only records bibliographic information about surviving books ... After a careful study of book advertisements, inventories and bibliographies, it seems to this investigator that, excluding jobbing and newspaper printing, as much as 10 per cent of the printed record from 1701--1800 has not been incorporated into the ESTC. In other words, for up to 10 per cent of the editions printed in the eighteenth century, not a single copy is known to survive.” (Suarez 40)  

As Suarez notes, the ESTC is a unique resource for pre-19thC works: “Regrettably, although this volume of The Cambridge history of the book in Britain ends in 1830, it is not possible to perform a similarly comprehensive analysis for the first decades of the nineteenth century because we have no equivalent bibliographical control for this period” (Suarez 40).  

“To use a metaphor, some people prefer to explore the world through books of photographs with occasional schematic maps. ESTC, on the other hand, provides the equivalent of a detailed topographic map, but no pictures. Such technical tools have limited appeal, even to some specialists; but if you want to thoroughly learn the lie of the land, you will need one, and the more complete and accurate the better.” (Tabor)



  

“An increasingly common trend, I am sorry to report, is that more and more people do not want ESTC at all --- they want ECCO or EEBO. The younger generation of scholars in particular, lured by full-text images and ransacking the Web for illustrations for their books and articles, are using these utilities as de facto bibliographic databases. They find that the stripped-down records and simplified indexes are good enough for their purposes. To a minority of them, the fact that other works, editions, and copies exist outside the Web is irrelevant, and perhaps even irritating.” (Tabor 368)  

**Comedies vs tragedies performed**: the ratio of comedies to tragedies performed was an astonishing 14 to 1 in Paris (Theatre, Opera, and Audience in Revolutionary Paris: Analysis and Repertory by Emmet Kennedy, Marie-Laurence Netter, James P. McGregor, and Mark V. Olsen)

**Suarez numbers**

Some Statistics on the Number of Surviving Printed Titles for Great Britain and Dependencies from the Beginnings of Print in England to the year 1800, by Alain Veylit.   

Other work which has used the methodology of sampling includes 



Suarez: “Lacking the resources to conduct a detailed analysis of the entire ESTC from 1701 to 1800, I have resorted to sampling. Electing to examine all eighteenth-century records that appear in years ending in three -- 1703, 1713, 1723 and so on -- I have sought to avoid a number of cohort effects, most especially the cumulation of indeterminate records into years ending in ‘0’ or ‘1’ and, to a lesser degree, ‘5’.” (41)  

Which of these archives are the most "reliable", and which the most "distorted"? (obvs interrogate this framework)

* What’s *in* all these, anyway?
    * What does ECCO-TCP leave out compared to ECCO? Compared to ESTC? (Can I come up with adjustment factors?)
    * How do digital vs physical holdings compare?  

What is a "normal" footprint in the print culture of this decade? (i.e., what are the boundaries a work has to surpass to be unusually popular or unusually unpopular?)  

The problem of textual selection--- the paired difficulty and importance of deciding which few books one will actually read--- is an urgently meaningful problem in the “real world,” outside the realms of academia. A common solution is one which will likely alarm and distress most scholars who have dedicated themselves to thinking through canonical selection, or even the construction of a syllabus: a solution which could perhaps be called ‘radical impatience.’ Consider, for example, the following advice in a blog post, “100 Ways to Live Better,” item number 6, the first entry in the category “Mind”:

There are more great podcasts than you’ll ever have the time to listen to. If it sucks after 10 minutes, skip half an hour ahead. Still boring? Delete and move on. Obviously, do the same for books.

There is much to find distressing here. The operant metric that another’s ideas “suck” if they are “boring” in the first ten or thirty minutes. The fact that “obviously” the same metrics apply for books as for podcasts. Indeed, even the fact that podcasts so strongly come *before* books. Each word in the first sentence is itself a link to a specific podcast recommended by the writer, but “books” are a monolith and a footnote.



Later we see another metric for textual selection:

Should you watch that movie / play that game / read that book? Use the ratio:

(\[# who rated it 5/5\] + \[# who rated it 1/5\]) / \[# who rated it 3/5\].

This doesn’t apply to everything, but it applies to many things, including media. There are too many options out there to waste time on mediocrity, and everything great will be divisive.

Paired with the first piece of advice, this would suggest a process of seeking out divisive and controversial works, determining within ten minutes whether one is strongly in sympathy or strongly opposed, and continuing only with works which evoke strong sympathy.

Of the seven mentions of the word “book,” two are in fact the word “Facebook.” Two are the pieces of advice quoted above. Two are suggestions to find “good audiobooks, and/or a dog” to form a habit of going on walks, or “a good app or guidebook” to practice meditation. And the last is the advice that “If you’ve been waiting for months for someone to create an event and invite you, whether it’s a book discussion or a BDSM orgy, just throw one yourself,” in which “a book discussion” functions rhetorically as an extreme example of an unusual and improbable kind of social event, paired against its assumed opposite extreme.  

I created a “content set” called ECCO-1798 in Gale Digital Scholar Labs, by searching for all works in ECCO published “between” 1798 and 1798. This located 4,158 records. I downloaded the metadata for these 4,158 records as a CSV. I then used random.org’s “Random Integer Generator” to generate ten integers from 1 to 4,158 (inclusive), resulting in the following numbers: 1792, 2365, 159, 3511, 919, 170, 2136, 2259, 190, and 2242. I looked up the ten works appearing in those rows of the spreadsheet, without altering the order of the records from Gale’s default. (It is unclear to me what sorting method was used to organize them in the document.) I used the Gale Content Numbers of these ten works to create a new “content set” of just these ten titles.  

159 - Poetry; original and selected	Monograph	Monograph	Literature and Language I.	\[1796-98\]	Gale	London, United Kingdom		British Library	null	GALE\|CW0115892706	

170 - Sir, You are desired to meet the committee for improving the navigation of the River Thames, and for preventing encroachments on the said river, on board the navigation barge, at Staines, on Saturday, the 7th day of July 1798, at eight o'clock in the morning, and proceed from thence down the river at nine precisely, ...	Monograph	Monograph	Social Sciences II.	\[1798\]	Gale, a Cengage Company	Oxford, United Kingdom	Great Britain. Commissioners Appointed for Improving and Completing the Navigation of the Rivers Thames and Isis	Bodleian Library, University of Oxford	null	GALE\|CB0130118850	

190 - Thoughts concerning the proper principles of finance, that ought to be adopted at present, and in future, in support of the British government. Addressed To The Freeholders And Mercantile Interest Of Leeds, Wakefield, Halifax, Huddersfield, Bradford, Doncaster, Hull, And The Other Towns In The County Of York. By a freeholder of Yorkshire	Monograph	Monograph	Social Sciences I.	1798	Gale	Lawrence, KS, United States	James Cochrane	Spencer Research Library, University of Kansas	null	GALE\|CW0107793549	

919 - National blessings considered and improved, in a sermon, preached on Thursday, November 29, 1798. By Alex. Black, Minister, Musselburgh	Monograph	Monograph	Religion and Philosophy I.	1798	Gale	London, United Kingdom	Alexander Black	British Library	null	GALE\|CW0123387895	

1792 - False impressions: A comedy in five acts. Performed at the Theatre Royal, Covent Garden. By Richard Cumberland, Esq.	Monograph	Monograph	Fine Arts II.	1798	Gale, a Cengage Company	London, United Kingdom	Richard Cumberland	British Library	Eighteenth Century Collections Online	GALE\|CB0129794221	

2136 - The surprizing adventures, of Jack Oakum, & Tom Splicewell, two sailors, who went a pirating on the Kings' highway. How that the first \[prize\] they took gave information of their course, and being pursued by a whole squadron, Tom Spicewell was taken and condemned to be hanged \[:\] but by means of his beloved friend Jack Oakum, who interested with his Majesty, he was pardoned.. Also a copy of Jack's polite letter to the King, on the above occasion. To which is added, The merry revenge; \[our,\] Joe's stomach in June	Monograph	Monograph	Literature and Language I.	1798	Gale	Oxford, United Kingdom		Bodleian Library, University of Oxford	null	GALE\|CW0116560788

2242 - The American preceptor; being a new selection of lessons for reading and speaking. Designed for the use of schools. By Caleb Bingham, A.M. author of The Columbian orator, Child's companion, &c. \[One line of quotation\] Published according to act of Congress	Monograph	Monograph	Literature and Language II.	1798	Gale, a Cengage Company	Boston, MA, United States	Caleb Bingham	Boston Public Library	null	GALE\|CB0130828924	

2259 - The letters of Junius	Monograph	Monograph	Literature and Language I.	1798	Gale	Cambridge, MA, United States	Junius	Houghton Library, Harvard University	null	GALE\|CW0110410930	

2365 - Gil Blas corrigé; ou histoire de Gil Blas de Santillane. Par M. Le Sage. Dont on a retranché les expressions & passages contraires à la décence, ... & à laquelle on a ajouté un recueil de traits brillans, des plus célèbres poëtes françois. Par J. N. Osmond. ...	Monograph	Monograph	Literature and Language I.	1798	Gale	Oxford, United Kingdom	Alain René Le Sage	Bodleian Library, University of Oxford	Eighteenth Century Collections Online	GALE\|CW0116687637

3511 - A Plain narrative of facts respecting the trial of James Coigley; Including his letter to an Irish gentleman, in London, and A. Young's letter to G. Lloyd	Monograph	Monograph	Law II.	1798	Gale, a Cengage Company	Cambridge, United Kingdom		University of Cambridge Library	null	GALE\|CB0132172434	  

1053 - Le juge à paix, et officier de paroisse, pour la province de Quebec. Extrait de Richard Burn, chancellier du diocèse de Charlisle, & un des juges à paix de Sa Majesté, pour les comtés de Westmorland & Cumberland. Traduit par Jos. F. Perrault	M.DCC.LXXXIX.\[1789\]	London, United Kingdom	Richard Burn	

2464 - An apology for professing the religion of nature, in the eighteenth century of the Christian aera; addressed to the Right Reverend Dr. Watson, Lord Bishop of Landaff	MDCCLXXXIX. \[1789\]	Oxford, United Kingdom	David Williams

1211 - Some account of the discovery, made by the late Mr. John Dollond, F. R. S. which led to the grand improvement of refracting telescopes, in Order to Correct some Misrepresentations, in Foreign Publications, of that Discovery: with an attempt to account for the mistake in an experiment made by Sir Isaac Newton; on which Experiment, the Improvement of the Refracting Telescope Entirely Depended. By Peter Dollond, Member of the American Philosophical Society at Philadelphia	 M.DCC.LXXXIX. \[1789\]	London, United Kingdom Peter Dollond	

1086 - Reflections on the contentions and disorder of the corporation of Cambridge	M.DCC.LXXXIX. \[1789\]	Oxford, United Kingdom

51 - Nécessité de supprimer et d'éteindre les ordres religieux en France, prouvée par l'histoire philosophique du monachisme, ...	1789 London, United Kingdom

938 - Emma Dorvill. By a lady	MDCCLXXXIX. \[1789\]	London, United Kingdom

629 - Virtue and peace forever inseparable. A discourse at the interment of Capt. John Howard, of Hampton, June 17th, 1789. By Joseph Huntington, D.D. \[Three lines from Young\] A few thoughts appear in the copy which, for the sake of brevity, were omitted in preaching M.DCC.LXXXIX. \[1789\]	Washington, DC, United States	Joseph Huntington

1524 - A practical treatise on the gonorrhoea, and on the superior efficacy of the cure by injection. By Peter Clare, surgeon \[1789\] Boston, MA, United States	Peter Clare	

702 - Analyse raisonnée de la sagesse de Charron. premiere partie	M.DCC.LXXXIX. \[1789\]	Manchester, United Kingdom	Jean-Pierre-Louis de Luchet

1732 - Two discourses. I. On wisdom attainable by meditation of the vanity of human life ... II. Men more influenced by example than precept; ... Preached ... March 8, 1789, by the Reverend Samuel Hopkinson, ...	\[1789\]	London, United Kingdom	Samuel Hopkinson  

I could end by describing what Bode-like scholarly edition I’d like to make. What would it be?

Datasets of men’s, women’s, and unsigned writing from the 1790s? Filtering to fiction, or poetry? Or sermons?? Non-literary writing? Things that people might want to teach?

Circulating libraries? Maybe one specific library, to be feasible?

Reprints??  

My own sliver of the ESTC was generously provided to me by the British Library in January 2017. It contains all items matching the query I specified, “(Words= alldocuments and W-year= 1789-\>1799 and W-Country of publica= enk),” which requests all documents published between 1789 and 1799 (inclusive) with a place of publication encoded as “England.” Running this search on the ESTC website at the time returned 52,001 records. The tools used to create the file, according to the librarian with whom I corresponded, returned 51,965 records, 36 records having gone missing; however, the file itself contains only 51,860, another 105 mysteriously lost. These 141 missing records are currently an unsolved mystery. My records come from the British Library’s ESTC database, rather than the STAR file. The corpus itself consists of a csv file[^cf30] with fifteen columns of information. The columns are: “Type of resource” (“Monograph” or “Serial”); “ESTC citation number”; “Name” (e.g., of an author, editor or illustrator); “Dates associated with name” (generally, the years they lived); “Type of name” (“meeting/conference,” “organization,” or “person”); “Role” (e.g., “author,” “cartographer,” or “bookseller”), “All names”, “Title”, “Variant titles”, “Place of publication”, “Publisher”, “Date of publication” (a single year), “Date of publication (not standardised)” (e.g., a year in roman numerals, or a date which includes a month or day), and “Publication date range” (for serials). In other words, it includes the very basic information of author, title, publisher, and year, in a complex structure which belies the apparent simplicity of these “basics.” Some of the ESTC records included in this corpus do not necessarily match my selection criteria (England, 1789-99), which is inevitably true of every corpus collected, and which I discuss in more detail in SECTION, Data Cleaning.  

My first source of ECCO metadata consisted of MARC records, kindly provided by University of Toronto libraries (my thanks to Leslie Barnes!). I requested information for all works published 1789-99 in the UK (so, including Ireland and Scotland, but excluding America.)

My ECCO metadata presented particular challenges. I had access to MARC records, which stands for MAchine Readable Catalogue. At several points, I read this data with my feeble non-machine eyes in order to guide my data processing. Using MarcEdit, I converted these MARC records to csv files which could, in OpenRefine, be read, manipulated, and merged like my other corpora. Since I was not able to simply convert “all the MARC headings that exist” using MarcEdit, I used all numbers 1 to 999 and \[will\] delete empty columns.

ECCO encodes much of its data in “unassigned” columns, rather than the standardized LOC categories.

<https://www.itsmarc.com/crs/mergedprojects/helpauth/helpauth/tag_list.htm>

Library of Congress MARC info







I also have ECCO metadata records, now, from the Gale Digital Scholars Lab. Due to their restrictions on how much data may be downloaded at once, I have created 11 files, each one holding the metadata for all of ECCO’s works in each year. ECCO-1789.csv, for example, is based on the following search: “LIMITS: Archive (Eighteenth Century Collections Online) And Publication Date (1789 - 1789).” These files initially contain works from a wide range of publication locations, not just England, since the Digital Scholars Lab does not provide any way to filter items by publication location.  

For each author, I went to http://estc.bl.uk/, determined the authoritative format of that author’s name within the database (e.g., “Smith, Charlotte Turner, 1749-1806”) and got the list of all titles attributed to that author. Paging through each entry, I recorded the date, edition, city, and publisher for every title which met my conditions of being printed in England 1789-99, and recorded the “permalink” URL to the work’s full ESTC record.  

I started by testing just *The Emigrants*. To get the text without the XML markup, I viewed it at <https://quod.lib.umich.edu/e/ecco/004801766.0001.000?rgn=main;view=fulltext> and simply copy-pasted the page into a plaintext file, manually deleting the header and footer website text so that the file only contained the poem. I saved it as emigrants-TCO-OCR.txt.

I attempted to download the HathiTrust edition by going to the “text-only view of this item” at <https://babel.hathitrust.org/cgi/ssd?id=uc1.31175035214942;page=ssd;view=plaintext;seq=15;num=ix>, where I discovered that I only had “one page at a time access to this item,” even though it was correctly identified under “Rights” as a public domain work. Authenticating through the University of Toronto gave me access to the full work. This page came with the warning “Use of this online version is subject to all U.S. copyright laws. Please do not save or redistribute this file,” which I disregarded under Fair Use to save a personal research copy of the text. Again the easiest method was to copy-paste the page into a plaintext file and delete unnecessary headers. I deleted “Book Text - Front Cover” and everything above, and “End of Section 9” and below, and saved the file as emigrants-Hathi-OCR.txt.

To download the ECCO OCR file, I went to the new Gale Digital Scholar Labs portal (since ECCO itself does not make the OCR available). The “basic search” very annoyingly attempted to autocomplete my search for “the emigrants” to unrelated terms, like “henry the eighth” and “female emigrants”. The “advanced search” did the same, changing “the emigrants” to “therefore” and “mall of the emirates.” Eventually, I was able to find the desired text by searching “the emigrants” as document title and “charlotte smith” as author. Gale Digital Scholar Labs prominently assigns an “OCR Confidence” of 95% to the record. The viewer did not give me 95% confidence. This method of accessing the text was the first to offer a download of the OCR. I downloaded it with their tool, examine the file, and deleted the disclaimer about OCR which appeared at the top, then renamed the file emigrants-ECCO-OCR.txt to match the other files.

At this point I was ready to load the files into Juxta. I signed in to Juxta Commons (at <http://juxtacommons.org/home/index>) to use the web interface, and uploaded all three files as “sources.” I prepared all three as “witnesses,” then created a “comparison set” of “Emigrants OCRs” with them and collated. It took a long time to collate, and then I discovered that, unfortunately, whichever file appears first (in this case, the ECCO file) is taken as the “base” to compare the others to, and I couldn’t determine how the base could be changed. I deleted the ECCO and Hathi witnesses from the set, then dragged them back in to it, to place them below the TCP witness. This caused unexpected server errors and failed, so I deleted the whole set, then created a new one with just the TCP witness and tried again. This worked, but the ECCO witness was immediately listed about the TCP witness, presumably due to alphabetization. I created a new witness from my TCP source, naming it beginning with A, and added that to the comparison set, then collated. To my surprise, Juxa calculated a relatively low “change index” for each text compared to the TCP witness: ECCO had a .16 change from base (i.e., 84% accuracy), and HathiTrust a .29 change from base (i.e., 71% accuracy).

Since some of HathiTrust’s inaccuracy may come from the fact that it uses the ſ character whereas TCP modernizes this to an s, I then ran a second experiment. Using “find and replace” in TextWrangler, I changed all 1278 instances of ſ in the HathiTrust file to s, and saved this as a new file, emigrants-Hathi-OCR-regularized.txt. I uploaded this to Juxta as a new source and witness for the same set. In doing so, it became clear that Juxta takes whatever the newest witness is as the “base,” so I had to delete/recreate my TCP witness again to make it the base. Once I got this working, the new HathiTrust copy had only a .09 difference from the TCP copy --- for 91% accuracy!  

The “gender” package in R is able to draw on a range of historical sources for gender information, but only one is applicable for this project. Most are US-based or only contain information beginning in the nineteenth century (or both). \[Or, SHOULD I use ipums...? Will US names differ a lot?\]

If no value is specified, then for the "ssa" method it will use the period 1932 to 2012; acceptable years for the SSA method range from 1880 to 2012, but for years before 1930 the IPUMS method is probably more accurate. For the "ipums" method the default range is the period 1789 to 1930, which is also the range of acceptable years. For the "napp" method the default range is the period 1758 to 1910, which is also the range of acceptable years. 



"The "napp" method uses census microdata from Canada, Great Britain, Denmark, Iceland, Norway, and Sweden from 1801 to 1910 created by the North Atlantic Population Project.” (Mullen, Blevins, and Schmidt)  “For the "napp" method the default range is **the period 1758 to 1910**, which is also the range of acceptable years.”(Mullen, Blevins, and Schmidt)

“The North Atlantic Population Project (NAPP) is a machine-readable database of the complete censuses of Canada (1881), Denmark (1787, 1801), **Great Britain (1851, 1861, Scotland 1871, 1881, 1891, 1901, 1911)**, Norway (1801, 1865, 1900, 1910), Sweden (1880, 1890, 1900, 1910), the United States (1850, 1880) and **Iceland (1703, 1729, 1801**, 1901, 1910).” (NAPP)

So actually, NAPP data has nothing relevant to *both* my time and place. But this may be the same US data that’s in the IPUMS method, so it’s still worth comparing the two.



“The "ipums" method looks up names from the U.S. Census data in the Integrated Public Use Microdata Series.”(Mullen, Blevins, and Schmidt) “For the "ipums" method the default range is **the period 1789 to 1930**, which is also the range of acceptable years.”(Mullen, Blevins, and Schmidt)  

To get a handle on how the gender package in R actually worked, and to assess how well it would meet my needs for this project, I began my making a sample csv of 41 arbitrary titles from ECCO’s 1789 holdings. The choice was driven by simplicity for a proof of concept: my ESTC and full-ECCO files were too large to open on my computer in spreadsheet software, and writing a program to extract a random sample was more work than it was worth, so I chose a file that I knew I could open in Numbers, the sample of all 1789 ECCO titles which I had recently downloaded through Gale’s Digital Scholar Lab. I selected some rows from the top of the list, skimming the names as I went to make sure I had selected enough titles that they would include two by women, and then copied those rows to a new spreadsheet (ECCO-1789-sample.csv). After some false starts with RStudio, I was still struggling to read the file into the program. Since all I wanted to know was whether, once I got it running, the gender package would tell me results worth working with, I decided to do this first test in whatever way would get a result, even if it required a great deal of non-scalable manual labor.

I copied the authors column into a plaintext file, and manually created a comma-separated list of just the first name of each other, to match the data format which the gender package required. (I took the first word before a space in each name, to reflect how a future first-name-grabber algorithm would work, even when this meant choosing things which were clearly not first names --- one of my core questions is how the gender package will handle those kinds of exceptions.) The resulting list of names was saved as ECCO-1789-sample-firstnames.csv. Trying to paste in this list for the names finally helped me understand what is meant by a “character vector” in R. It’s basically an array of strings. You have to make the character vector before you can run gender() on it. Sample code:

names = c("john", "madison") # creates a character vector called “names” by ‘combining (c-ing) two strings

gender(names, method = "demo", years = 1985) # guesses gender for both of those names

Once I had gotten something to produce guesses for more than one name, I wanted to try using a method other than “demo.” This sent me down a spiral of trying to install genderdata, and trying to install devtools to install genderdata. Finally I got everything installed and able to run!

If a command is run requesting a name-year-method combination for which the package has no data, it simply returns an empty “tibble.” The following three sets of results illustrate the limits and possibilities of napp and ipums data:



\> **gender(ECCOnames, method = "ipums", years = 1789) - earliest year possible with ipums**

\# A tibble: 25 x 6

name      proportion\_male proportion\_female gender year\_min year\_max

\<chr\>               \<dbl\>             \<dbl\> \<chr\>     \<dbl\>    \<dbl\>

1 Andrew              1                 0     male       1789     1789

2 Ann                 0                 1     female     1789     1789

3 Benjamin            1                 0     male       1789     1789

4 Charles             1                 0     male       1789     1789

5 Charles             1                 0     male       1789     1789

6 Charles             1                 0     male       1789     1789

7 Charlotte           0                 1     female     1789     1789

8 Edward              1                 0     male       1789     1789

9 Francis             0.464             0.536 female     1789     1789

10 James               1                 0     male       1789     1789

\# ... with 15 more rows



\> **gender(ECCOnames, method = "napp", years = 1769) - earliest year that knows about “John”**

\# A tibble: 7 x 6

name  proportion\_male proportion\_female gender year\_min year\_max

\<chr\>           \<dbl\>             \<dbl\> \<chr\>     \<dbl\>    \<dbl\>

1 John                1                 0 male       1769     1769

2 John                1                 0 male       1769     1769

3 John                1                 0 male       1769     1769

4 John                1                 0 male       1769     1769

5 John                1                 0 male       1769     1769

6 John                1                 0 male       1769     1769

7 John                1                 0 male       1769     1769



\> **gender(ECCOnames, method = "napp", years = 1789) - more direct comparison to ipums**

\# A tibble: 9 x 6

name      proportion\_male proportion\_female gender year\_min year\_max

\<chr\>               \<dbl\>             \<dbl\> \<chr\>     \<dbl\>    \<dbl\>

1 Charlotte               0                 1 female     1789     1789

2 John                    1                 0 male       1789     1789

3 John                    1                 0 male       1789     1789

4 John                    1                 0 male       1789     1789

5 John                    1                 0 male       1789     1789

6 John                    1                 0 male       1789     1789

7 John                    1                 0 male       1789     1789

8 John                    1                 0 male       1789     1789

9 Samuel                  1                 0 male       1789     1789



Neither is very stunningly thorough, especially since the year is supposed to reflect the *birth* year of the person, and I can only barely get information about the year they published something. Somewhat to my surprise, the ipums method --- which I originally planned to reject in favor of napp, because ipums is US only --- has much more data available, and doesn’t seem to reflect any US-specific oddities. So, I might try to use both ipums and napp, but if that is difficult, just using ipums seems appropriate. I’m not entirely sure, yet, whether using ipums will be faster than just manually assigning genders: my next question, I think, is determining how many unique first names there are, and then trying to guess how many of them ipums would be able to sort out for me. It managed 25/41 = 61% of the random ECCO sample, which is a substantial number, especially since it will by definition include the most common names. Expanding the acceptable years slightly also seems to help it know a lot more about the names, while still according with my own estimates. (ipums can learn “Augustus” in 1799, for example.)  

Table 1 shows data that I compiled by hand in Numbers. The first three columns are based on my synthesis of scholarship on Charlotte Smith. As I consulted a range of work on Smith, I updated this information to reflect the most complete and accurate information possible. My editorial decisions included, for example, the exclusion of *D’Arcy* from consideration, since it was never published in England. I introduced standardized titles for the two volumes of *Elegiac Sonnets*, retroactively naming the initial publication “volume 1” to distinguish it from the second volume which would appear 13 years later, so that each title has its own edition count. The next four columns represent the results of my queries in the ESTC, ECCO, ECCO-TCP, and HathiTrust databases. I searched each database with several queries to locate Smith’s works, beginning (where possible) by finding all works categorized under her authorship, and then searching individual titles of works. This figure shows the simplified results from a more detailed spreadsheet, which also includes links to the records themselves where they exist, and notes on how the records are encoded (e.g., multiple volumes or all as one volume.) Simplifying inclusion down to a boolean yes/no involved some editorial decisions. If only *part* of a work was included (as in HathiTrust’s record for the first edition of *Celestina*, which only includes volumes 3 and 4), I recorded that as a “yes.” If a work was only included in its Dublin edition (as in HathiTrust’s record for *Desmond*), I recorded that as a “no.” These searches were conducted in February 2020.  

Figure 1 is based on the data recorded in Table 1, which was pasted into the RAW Graphs visualization tool (Mauri et al.) and processed using their default settings for an alluvial diagram. Using an alluvial diagram required imagining the databases as sequential “stages” through which all books flow. Accordingly, I added a “source” for this flow of books by adding a column labeling every edition as falling into the category “Works printed in England, 1784-1807.” I could have chosen to place the following “stages” in any order; to assist in visualizing Smith’s representation in these databases as a process of winnowing down, I chose to place them in order from largest selection to smallest. The scale of each “strand” of the diagram is scaled in width based on the number of editions it represents, as per RAW Graphs’ default settings. The colours, fonts, and width of the graph are also simple defaults.

[CSmith-in-ESTC-ECCO-TCP-Hathi-table]: CSmith-in-ESTC-ECCO-TCP-Hathi-table.png width=184px height=312px

[CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3]: CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3.png width=244px height=198px

[juxta-emigrants-p1]: juxta-emigrants-p1.png width=145px height=99px

[juxta-emigrants-histogram]: juxta-emigrants-histogram.png width=72px height=155px

[fig-ecco-emigrants-p1]: fig-ecco-emigrants-p1.png width=100px height=161px

[hathi-emigrants-p1]: hathi-emigrants-p1.png width=148px height=192px

[^cf1]: More specifically, these “authors” are “Great Britain, Parliament,” “Great Britain,” “Great Britain, Parliament, House of Commons,” “Great Britain, Lords Commissioners of Appeals in Prize Causes,” and King George III. After King George comes Thomas Paine and Hannah More, and then it’s “Great Britain, Parliament, House of Lords” and “Church of England.”

[^cf2]: Google Books and Project Gutenberg are not, of course, traditionally “scholarly” resources, but that is why they form an informative contrast with the other resources examined.

[^cf3]: This calculation is carried out by manually examining the metadata of the six corpora I have acquired.

[^cf4]: This calculation is carried out by a small, simple program I am writing, described in Appendix A. Because the program just simplifies a straightforward process of counting, it is only lightly theorized in the dissertation itself.

[^cf5]: I already know that ‘titles per year’ are distributed fascinatingly differently between ECCO, ECCO-TCP, ESTC, and HathiTrust

[^cf6]: This calculation is carried out by a larger, more complex program I am writing, applying topic modelling to the titles of works. Because it makes several major interpretive choices, it is theorized and discussed in detail when it is applied.

[^cf7]: ‘Too much’ and ‘too little’ are here, of course, defined from the point of those with cultural capital which they wish to maintain.

[^cf8]: Part of Guillory’s argument is that, although the rhetoric of the canon debates generally sought to re-value authors of any number of oppressed categories, often using the phrase “gender, race, and class” as a single unit, the work undertaken was in fact unable to address class, since class operates differently from gender and race.

[^cf9]: Appendix B (“Methodology”) contains many examples of these algorithmic procedures executed by the human researcher and the computational programs in concert. The act of writing a program is an iterative process of delegation.

[^cf10]: Indeed, Buurma notes, “There are good reasons, of course, that scholars and journalists like to begin with Busa: he was the first concordance-maker to automate all five stages of the process, in 1951,” and he intentionally foregrounded and publicized the innovative nature of his work. \\cite\{Buurma:2018wt\}

[^cf11]: In the interest of preserving this history of citation, the students were Mary Jackman and Helen S. Agoa, credited on the cover of the published Dryden index. (Miles herself attached her name only to the preface.) From the computer lab staff, Miles particularly thanked Shirley Rice, Odette Carothers, and Penny Gee.

[^cf12]: It may also be the case, of course, that even fields with a long history of graphical display would benefit from greater scrutiny of the evidence they use; see: the Data Dinosaur. But this is beyond the remit of what an English PhD can address.

[^cf13]: I cite Tufte and Cairo as the thinkers whose design philosophies best accord with my own current understanding of the work and craft of persuasive data visualization, but my actual practical training as a graphic designer is indebted to Judith Galas, Sonia Davis Gutiérrez, and Tom Hapgood.

[^cf14]: Tufte is careful not to blame the engineers for being better at engineering and systems analysis than they were at design: rather, this example shows that design is a skill that involves expertise; when designs matter, people with that expertise need to be involved.

[^cf15]: 1. show comparisons, contrasts, differences 2. show causality, mechanism, explanation, systemic structure (intervention relies on manipulable causality -- can't do anything with the information without causality) 3. show multiple variables (3 or more) -- the world is multivariate 4. \*completely integrate\* words, numbers, maps, graphics, etc, etc. Provide information at exact point of need 5. documentation must thoroughly describe evidence and its sources, provide complete measurement scales 6. presentations succeed based on their content. for better presentations, get better content.

[^cf16]: Although these events, of course, did not occur on January 1 or December 31, respectively, the entirety of 1789 and 1799 are both included in my study, out of sheer technological necessity.

[^cf17]: (Harper 2016; Jacsó 2008; Weiss 2016) (CITE Mike Sutton and Mark D. Griffiths)

[^cf18]: CITE http://languagelog.ldc.upenn.edu/nll/?p=1701

[^cf19]: I have heard it quipped more than once in digital humanities gatherings that you always think you’re going to get your texts from somewhere else, but Project Gutenberg is where you’ll actually get them.

[^cf20]: For example, it might be able to acquire a text document with all of the words of a novel, but sorted into alphabetic order: such a text file can be used for some analyses based on word-frequency, but cannot be read. Or, it might be possible to find collocations of where a given word appears, but with only a limited number of words of context on either side of the term in question. Or, scholars can run pre-written code provided by HathiTrust to carry out things like topic modelling on the full, intact texts of their chosen works, but without being able to inspect those texts or run their own code on them. All of these modes of analysis make research much more difficult to carry out, and nearly impossible to verify. In the study of contemporary copyrighted literature, however, even these very limited tools for corpus analysis are valuable.

[^cf21]: I have heard it quipped more than once in conferences sessions that you always *think* that you’re going to get your texts from OCR, but you always *do* get them from Project Gutenberg.

[^cf22]: Volumes 4 and 5 of *Letters of a Solitary Wanderer* are in fact part of the same bibliographic record as the first three volumes. The publication date for the combined five-volume work is listed as “1800-1802.”

[^cf23]: Several of HathiTrust’s records provide “mixed copies” like this, with some volumes scanned from one library’s holdings and other volumes scanned at another. If there is overlap, multiple scans will be provided for the duplicated holdings. Nonetheless, all of these scans are tied to a single unified MARC record, taken from only one of the holding library (with no indication of which library provided it).

[^cf24]: Later known as Primary Source Microfilm, an imprint of the Gale Group.

[^cf25]: One exception to this assumption has to do with treatment of the character ſ, which the TCP file modernizes to an s, but which HathiTrust renders as ſ. To avoid penalizing HathiTrust for “inaccuracy” when it is actually a more accurate reproduction of the page than my reference point, I amended every instance of ſ in HathiTrust to an s.

[^cf26]: Leaving the ſ characters unchanged in the HathiTrust document resulted in a .29 change from base (71% accuracy), so my normalization of ſ to s had a major impact on the comparison. I consider the .09 result more appropriate than the .29 because the normalized copy better reflects how an OCR file would be used.

[^cf27]: “Because ESTC is a bibliographical database rather than a catalogue, strictly speaking, its records describe groups of copies,” such as editions, “rather than specific copies,” such as the Exeter Book (Tabor 369).

[^cf28]: “The first problem relates to the unit of classification. A clearly defined unit is necessary to ensure that a study of change over time is reliable and based on consistent terms. What is the unit that the ESTC uses? Scholars sometimes answer by using the terms “edition,” “issue,” or “title” interchangeably. But since the ESTC does not rely in a consistent manner on any of these terms for its unit of classification, one should refer instead only to the ESTC record, a unit created by the ESTC and having no meaning outside the ESTC.” (Karian 289)

[^cf29]: Technically, in the “##” sequence, the first “#” encodes that the work is a first edition (as opposed to a “2” for an “intervening” edition or a “3” for the “current” most recent edition), and the second “#” doesn’t encode anything. That position in the MARC record is undefined, with no possible meanings, and simply always contains a ‘blank’ #.

[^cf30]: explain what a csv is