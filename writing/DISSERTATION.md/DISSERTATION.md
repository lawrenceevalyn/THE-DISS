Title: DISSERTATION  
Author: Lawrence Evalyn

# ch 1 - intro #



## 1.1.  intro ##  

According to the English Short Title Catalogue (ESTC), the most popular English authors of the 1790s were Thomas Paine, Hannah More, John Wesley, and William Shakespeare. Of course this claim immediately falls apart on further scrutiny. In fact, by the metric of ‘unique entries in the ESTC database,’ the most popular author of the decade is by far Great Britain, followed by Great Britain, Great Britain, Great Britain, and King George III.[^cf1] Paine, More, Wesley and Shakespeare are only able to rise to our notice if we intervene in the dataset to filter out all authors whose names contain the phrase “Great Britain”; otherwise, Shakespeare is outnumbered by the House of Lords and by the Church of England[ TK: “More specifically, these “authors” are “Great Britain, Parliament,” “Great Britain,” “Great Britain, Parliament, House of Commons,” “Great Britain, Lords Commissioners of Appeals in Prize Causes,” and King George III. After King George comes Thomas Paine and Hannah More, and then it’s “Great Britain, Parliament, House of Lords” and “Church of England.”]. And a single paragraph cannot contain all of the reasons that the quantity of unique entries in a database would not correlate with any useful definition of popularity -- although later parts of this dissertation will undertake to enumerate them at greater length. These claims demonstrate that a poorly formed question will produce a useless and stupid answer even (or perhaps especially) if computation is used to answer it. This dissertation is dedicated to the formulation of better questions. I am interested in the limits of the generalizations that we make, both in “distant reading” research and in non-digital scholarship[ , which still frequently relies on claims that a given work was “popular” because it went through a certain number of editions, or the author was paid a certain amount, and so on. These generalizations break down in part because “popular,” as a concept, is overdetermined: does it mean financially successful, or widely beloved, or important? Examinations of “popularity” also break down, at close scrutiny, because of the contentious relationship between concepts of “popular” and “literary”: important literature should have some claim to cultural relevance, but it shouldn’t be *too* popular or it becomes suspect. Nonetheless, \[TRANSITION\]]. I take as my starting point the contention that, in order to identify what is “popular” or “important,” we must also understand what is normal. At its core, my question is: given that it is not possible to read everything (or even most things), how do we, and how *should* we, determine what to read, preserve, study, and teach? This “question” is, of course, many questions: what we do is by no means what we *should* do; what we read is not necessarily what we study or teach. It is also an old, nearly an old-fashioned question. The current moment of self-reflection in the field of Digital Humanities, however, provides a timely reason to revisit it. Even literary scholars who do not carry out “Digital Humanities” research are impacted by the corpus-building choices of major digital resources, since all literary research is now mediated at some level by search algorithms and databases, even if this mediation is as small as looking up the holding libraries for physical copies of texts. It is therefore relevant to the field as a whole if, as I contend, corpus-building has become the new canon-building: an invisible and naturalized process of selecting texts for idiosyncratic and historically-specific reasons, and then treating those individual texts as ideal representatives of an imagined “whole” of literature.   

Despite the crucial importance of corpus-building to the interpretation of “distant reading” research, it is often extremely difficult to know what is in a corpus. Even large institutional resources used by many scholars provide little context for their choices of what to include or exclude. These hidden choices are particularly problematic when historical selection factors might have led to the creation of databases which re-create social inequalities. I focus specifically on writing printed in England between 1789 and 1799, to explore how works from this eleven-year “decade” have been selected as important, literary, or popular. For this period, the English Short Title Catalogue provides basic bibliographic data for nearly 52,000 titles, but the Eighteenth Century Collections Online Text Creation Partnership corpus of XML-encoded full texts includes fewer than 500 titles. This difference raises the question: why were the other 51,500 titles *not* considered worth the investment of scholarly effort? And with particular urgency: do the most invested-in resources underrepresent women? My experiments examine six major databases to answer these questions: The English Short Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the Eighteenth Century Collections Online Text Creation Partnership (ECCO-TCP), Google Books, Project Gutenberg,[^cf2] and HathiTrust. For each database, I download their holdings identified as printed in England 1789-1799.[^cf3] I identify how many titles the database attributes to each year. I calculate how many works are attributed to male, female, or unknown authors.[^cf4] These very simple pieces of information, when they differ widely between databases,[^cf5] provides the basis for an initial analysis of the assumptions and limitations of each database. I then examine the contents of each database more closely, to compare the inclusion of broad categories of writing like poetry, drama, prose fiction, and ephemera.[^cf6] Identifying these categories of writing within each corpus reveals a predictable preference for “literary” forms such as novels and poetry in the smaller databases. This preference for particular kinds of writing might explain changes in gender representation of smaller databases. If the novel is the domain of women, for example, a corpus can underrepresent women by underrepresenting novels. Or it could include a representative number of novels, but disproportionately include novels by men. My investigation allows me to identify the patterns of selection.[ TK: “What would be your preliminary hypothesis? I guess one assumption might be that market-driven choices are made in light of existing demand i.e. the established teaching / research canon, in which case there would be an inherent conservatism in selection of texts.”] To ground my analysis in specifics, I take Charlotte Smith, Mary Robinson, Hannah More, and Ann Radcliffe as case study authors. All four authors have long histories of contentious reception, rooted in debates about seriousness, popularity, and women’s writing. I revisit them to see how their careers might be interpreted through a new lens. I do so, in part, to challenge the contrast drawn between ‘popular’ and ‘serious’ writing, especially in the historical evaluation of women’s writing as literary.  

The problem of evaluating literature is not a new or a simple one. In the eighteenth century, the debate took the form of urgently needing to distinguish ‘trash’ from ‘treasure’. Michael Gamer, in *Romanticism and the Gothic: Genre, Reception, and Canon Formation*, highlights the role of the eighteenth-century reviewer as a crucial mediator between the writers and readers of books[ Reviewers sought to dictate the social assessment of individual literary works in order to enforce morality for society at large.\[ The emerging idea of the public sphere (cf Habermas) in the eighteenth century brought with it an urgent task of literary assessment. --- this is true but maybe I don’t have to say it! and then I don’t have to waste time talking about Habermas\] \[They basically say so themselves in their reviews---I’ll quote a juicy representative one.\] ]. Importantly, although the assessments take the form of reviews of individual works, Gamer also argues that the critics’ objections are in fact “a regulatory discourse -- carried out under the fiction of paternalistic advice to a given gothic writer, but functioning as an implicit threat to other readers and writers” that affiliation with the gothic comes with “cultural costs” (42). The gothic stands in as a proxy for any kind of “popular” reading that takes place “in the absence of formal education and training” (57), so a denunciation of a gothic work becomes a reaffirmation of class-based literary hierarchies. In other words, these reviews create and affirm the cultural capital of a category of ‘serious’ literature[ The really clever bit of Gamer’s argument is that he then unpacks how Romantics, especially Wordsworth, use just enough Gothic material to sell their books while also repeating these conventional attacks on the Gothic --- they get to have their cake and eat it too, pursuing both financial and cultural capital. But I don’t think that’s relevant here.]. Gamer is only concerned with the gothic and romanticism, but the overall regulatory function of literary reviewers as moral arbiters--- and the stock conventionality of their objections, which do not affect the actual production or consumption of the works attacked--- applies to most forms of writing in the period. For example, George Taylor sees the same dynamic in the theatre. In *The French Revolution and the London Stage,* he argues that, “\[c\]ritics might make sharp comparisons” between the many kinds of entertainments that were staged, “but little of the programme was dismissed \[by audiences\] as ‘trash', or ‘immoral', or irrelevant ‘fancy’” (3). Taylor sees the repetitive discourse of eighteenth-century literary critics as proof of a larger social divide: “Disagreement as to what is trash and what is treasure suggests cultural crisis, when values are put under question by social stress or political conflict” (3). Gamer and Taylor both suggest that moral judgment of literature by its critics was driven by social friction, rather than by the aesthetic distinctions which they claimed as their motivation.  

In other words, Gamer and Taylor both affirm the key conclusion of John Guillory’s *Cultural Capital: The Problem of Literary Canon Formation*, that “in fact ‘aesthetic value’ is nothing more or other than cultural capital" (332). Guillory’s sociological history of literary canons is a well established part of literary studies, which will take on new dimensions as I apply to to the current moment of digital databases. In the eighteenth century, he argues, the cultural capital of vernacular English literature is defined by its use within the school system to enable and restrict social mobility. English vernacular literature first begins to accumulate cultural capital in middle-class schools where it is “a substitute for the study of Greek and Latin, but with the same object of producing a linguistic sign of social distinction” (97) that would allow readers to improve and signify their social standing. The public re-assessment of literature described by Gamer and Taylor is, for Guillory, “the first crisis in the status of the vernacular canon, the problem of assimilating new vernacular genres such as the novel” (xi), which seem in danger of affording too much social mobility by offering too little literary distinction for social elites.[^cf7] The ‘solution’ is institutionalization, in which “the school becomes the exclusive agent for the dissemination of High Canonical works,” and therefore, he argues, “the prestige of literary works as cultural capital is assessed according to the limit of their dissemination, their relative exclusivity” (133). Under this system, ‘serious’ literature may not be identifiable linguistically, but it can still be identifiable by the difficulty of accessing it. This history of canonization has important implications for the field of literary study. As Guillory himself insists, if the aesthetic value of a text is determined by the social operations of class, it undermines the notion of literature itself as a category of writing distinguishable in aesthetic terms from non-literary writing[ Q for Gillespie: should I still research “the new formalist work to reclaim the literary qua the literary - and e.g. Steve Conner's - ‘nah’” ? What is useful/important to address in this work, given what I say here?]. Guillory’s book is motivated by the canon debates of the 1990s, which were driven by an urgent re-valuation of literature by women and people of colour.[^cf8] His response insists that it is untenable to conceive of the problem in terms of increasing the ‘representation’ of individual works or authors within existing systems. Instead, for Guillory problem lies in the institutionalization of literature itself. “If literary criticism is ever to conceptualize a new disciplinary domain,” he says, embedding his prescription in that “if,” “it will have to undertake first a much more thorough reflection on the historical category of literature; otherwise I suggest that new critical movements will continue to register their agendas symptomatically, by ritually overthrowing a continually resurgent literariness and literary canon” (265). In other words, assigning the cultural capital of “literature” to different works cannot change the underlying system.[ Do I need to say more about this? I feel like I’ve already spent a long time on Guillory.



Bourdieu: “‘\[T\]o deny evaluative dichotomies is to pass a morality off for a politics. The dominated in the artistic and the intellectual fields have always practiced that form of radical chic which consists in rehabilitating socially inferior cultures of the minor genres of legitimate culture. ... To denounce hierarchy does not get us / anywhere. **What must be changed are the conditions that make this hierarchy exist, both in reality and in minds. We must---I have never stopped repeating it---work** __to *universalize in reality the conditions of access*__ **to what the present offers us that is most universal.**’" (Qtd in Guillory 339-340)]  

Perhaps indicating that Guillory was correct, twenty years later, we are still debating the need for “literary criticism ... to conceptualize a new disciplinary domain” (Guillory 265), now in the context of computation. The reconceptualization of literary study itself is at the core of Franco Moretti’s coinage of ‘distant reading’: the problem for which “\[r\]eading ‘more’ seems hardly to be the solution” (“Conjectures” 55) is the problem of conceiving of *world* literature, rather than the “canonical fraction, which is not even one per cent of published literature” (55). His new methods are meant to enable literary studies to examine a new object. The field of distant reading has been moving away from Moretti himself. However, it is still shaped by the attempt to redefine the disciplinary domain of literary studies. In many cases, the new domain is no longer the “canon” but the “corpus,” a collection of texts which are studied *en masse* for macroanalytical insights. Katherine Bode, for example, in “The Equivalence of ‘Close’ and ‘Distant’ Reading,” argues that Moretti and Matthew Jockers replicate the approaches of New Criticism with their corpora, and calls for “a new scholarly object of analysis” (79) that directly examines historical and textual context of corpora as representations of “literary systems” (97). Lauren Klein, too, treats the textual corpus as the new object of literary analysis requiring curation, contextualization, and interpretation. Her critique argues that “it’s not a *coincidence* that distant reading does not deal well with gender, or with sexuality, or with race,” but also that these failings are not inevitable: “it’s not that distant reading *can’t* do this work,” she insists, “it’s that it’s yet to sufficiently do so” (n. pag.). Bode, too, despite her strong critique of distant reading as it has been practiced by Moretti and Jockers, does not blame distant reading itself. Distant readers like Moretti and Jockers, she argues, “while claiming direct and objective access to ‘everything,’ ... represent and explore only a very limited proportion of the literary system, and do so in an abstract and ahistorical way” (78). Klein, like Bode, calls for “more corpora---more accessible corpora---that perform the work of recovery or resistance” to allow research “beyond quote ‘representative’ samples, which tend to reproduce the same inequities of representation that affect our cultural record as a whole” (n. pag.). This framing re-creates, at the site of the corpus, the identical narratives of exclusion and representation which were previously located in critiques of the canon.  

The relocation of the debate from the canon to the corpus is not without grounds. As this dissertation will explore in depth, challenges to the technological accessibility of texts have created new hierarchies, and a new “great unread.” Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. For example, the English Short Title Catalogue records 51,965 titles printed in England between 1789 and 1799. The corpus most commonly used for DH work on eighteenth-century literature, ECCO-TCP, includes only 466 titles for that same time period. What are the other 51,499 titles, why are they accessible in the ways they are, and what does it mean for digital eighteenth-century studies that they are not included? Although the examination of databases prompts similar hypotheses of exclusion as in longstanding conversations about canons, digital databases do not simply replicate new canons. \[By the end of my dissertation, I will be able to state here what IS happening --- something structured by related logics of access and prestige, and related simplifications of historical complexity, and related *institutional* replication of privileged texts.. But very importantly different, too, since we don’t *read* databases.\] In a series of computational and non-computational research processes, I examine six databases of eighteenth-century texts to learn about four eighteenth-century authors, and I examine four eighteenth-century authors to learn about eighteenth-century databases. This dissertation, therefore, takes place within three scholarly conversations: the digital humanities, as an increasingly self-reflective set of practices; eighteenth-century studies, and the challenges presented by the 1790s; and the frameworks of reparative reading within queer theory which seem to offer valuable resources for both. The remainder of this chapter will describe in more detail the relevant scholarship shaping my frameworks, and then introduce my chapters by introducing my four case study authors.



## 1.2.  frameworks ##



### 1.2.1.  overview ###



My work takes a critical algorithm studies approach to digital databases of eighteenth-century literature, examining the structural assumptions of the most-used resources (including some that scholars don’t like to admit to using). I close read the database structures, file formats, and historical documentation for the English Short Title Catalogue, Eighteenth Century Collections Online, the Text Creation Partnership, HathiTrust, Project Gutenberg, and Google Books, to examine how each resource’s algorithmic definition of a “book” (and the information that might matter about a book) is shaped by the material, historical conditions of each organization’s development. My initial research question was, by Eve Kosofsky Sedgwick’s definition, a classically paranoid approach: I sought to expose the under-representation of women’s writing underlying apparently “neutral” digital infrastructures. This question carried the combined urgency and futility of paranoid critique: urgent, because an unfair database would expose an unfair society; and futile, since the research could only be motivated by the conviction that its answer was already known. My paper will touch briefly on some specifics of this research and my findings, as the basis for a broader discussion of critical algorithm studies, and the project of imagining reparative algorithm studies.

One of the current problems of critical algorithm studies is how difficult it is to move from critique to action: it seems that no matter how carefully we dissect the flaws of oppressive computational systems, we cannot opt out of them. Excellent work by scholars like Wendy Hui Kyong Chun and Safiya Noble, for example, meticulously historicizes computational systems, and there is real value to the denaturalization of the systems they thus reveal. But this work relies on the paranoid logic of exposure, and I am interested in other attitudes. In my examination of digital infrastructures for eighteenth-century studies, I take a brief detour through Marxist thinking (via Bourdieu and John Guillory) to diagnose a deep tension between capitalist and anticapitalist value systems as the likely cause of the flaws in these systems today. I then am to move beyond the obvious paranoid critiques prompted by this observation. I confess that, at this stage, this is the point at which my thinking remains speculative--- but I feel certain that the right direction lies in queer strategies of creative reappropriation, subversion, and resistance.  

The theoretical frameworks of this dissertation are drawn from the fields of feminist DH and queer DH, and from non-DH schools of thought which seem to offer valuable tools. My core motivating framework, as I conceptualize my work, is that of reparative reading. Eve Sedgwick’s “Paranoid Reading and Reparative Reading” persuasively describes in the dominance of paranoia in literary criticism, and attempts to sketch an alternative in what she terms reparative reading. A paranoid rhetoric of exposure and critique strikes me as the most obvious narrative to structure this dissertation’s investigation of the uneven institutional valuation of different writing. However, these obvious critiques also require rejecting many generations of sincere work by my fellow academics, without necessarily offering new discoveries of value to replace them. One experiment of this project, not yet complete, is to articulate an assessment of the limitations of contemporary digital resources which nonetheless allows those resources to be recuperated. My touchstones are two descriptions from Sedgwick’s original chapter:  

The desire of a reparative impulse... is additive and accretive. Its fear, a realistic one, is that the culture surrounding it is inadequate or inimical to its nurture; it wants to assemble and confer plenitude to an object that will then have resources to offer to an inchoate self. (149)

What we can best learn from such practices are, perhaps, the many ways selves and communities succeed in extracting sustenance from the objects of a culture - even of a culture whose avowed desire has often been not to sustain them. (150-151)

What Sedgwick describes, here, is a “desire,” not a methodology. I therefore understand “reparative reading” to refer, not to a precise set of practices, but to a position one might occupy in relation to a text. What I posit is also a desire: that my methods here can provide useful practices for others. The reparative position is a generous one, both in terms of giving of oneself to a text, and in terms of seeking a text’s strengths over its weaknesses. What I learn from Sedgwick, therefore, that *attention* is the first step toward *caring*, and that non-judgment can be more informative than rejection.  

I have mentioned moving away from critique as well as from paranoia: in rethinking the role of critique, I draw upon the work of Rita Felski, and the theories of “surface reading” described by Sharon Marcus, Stephen Best, and Heather Love. Felski, in her article “After Suspicion” and then further in her monograph *The Limits of Critique*, seeks to attend seriously to literary attachments, including our own attachments as critics. Felski’s approach to these attachments is essentially sociological, drawing heavily on Bruno Latour’s actor-network-theory, and thus involves almost no close reading. “Surface reading” positions itself as an alternative to “symptomatic reading”; rather than seeking to expose hidden truths concealed within texts, it attempts accurate descriptions that “make visible what is invisible only because it's too much on the surface of things” (Best 13). The analogues to reparative and paranoid reading are obvious, but not perfect: all paranoid reading is symptomatic, but not all symptomatic reading is paranoid[ unpack]. Reparative reading, as described by Sedgwick, is often still interested in ‘deep’ meanings of texts, in which striking textual features can be interpreted to locate additional meanings. Felski’s readings are often symptomatic in this way. In contrast, “surface reading,” as Heather Love describes, pursues “a turn away from the singularity and richness of individual texts” (374), seeking descriptions that are “complex and variegated, but not rich, warm, or deep” (378). Love’s disavowal of “richness” here is part of her attempt to move away from “the ethical charisma of the literary translator or messenger” (374) who characterizes the paranoid, critical figure that both Sedgwick and Felski also seek to escape.  

Love’s later article, “Close Reading and Thin Description,” provides a more precise articulation of the kind of close reading that she calls for, in which an “exhaustive, fine-grained attention to phenomena” (404) enables “taking up the position of the device; by turning oneself into a camera, one could---at least ideally---pay equal attention to every aspect of a scene that is available to the senses and record it faithfully” (407). Although Love is uninterested in “distant reading” as synonymous with Moretti (Love 411), this invocation of the mechanical implies, I argue, an obvious potential for computation. The actual *practice* of computational research requires a great deal of laborious, intimate encoding. The researcher must occupy a “mechanical” position of receiving inputs and responding to them consistently over time, whether entering details in a spreadsheet with a consistent taxonomy or running the same program over multiple datasets.[^cf9] Love says:

Good descriptions are in a sense rich, but not because they truck with imponderables like human experience or human nature. They are close, but they are not deep; rather than adding anything ‘extra’ to the description, they account for the real variety that is already there. (377)

A computational model is unlikely to “truck with imponderables,” but it *absolutely* *must* “account for the real variety that is already there” or else the code will simply fail to run. If you are forced to manually encode your assumptions into a system, you are forced to confront what they are. Even deleting or ignoring information is still a way of “accounting for” it in the coding process: some part of the program will have to say, in effect, ‘if I get an input that doesn’t match what I expect, discard it.’ Choosing to ignore contradictory or difficult information carries the assumption that this information does not ‘count,’ or does not matter to the question at hand. The choice faced by scholars is how to address our encoded assumptions. The encounter with variety does not in itself produce nuanced results: it is possible to selectively ignore any uncomfortable details. But it is also possible to do computation reflectively, asking not “how can I make this work the way I want?” but “where do my assumptions encounter resistance?” and turning one’s attention to the nature of the resistance. Integrating this reflection into the research process can allow a scholar to avoid both the pitfalls of “conquering” their material and of claiming an algorithmic grasp of “objective” truth.   

To bring these principles into the field of Digital Humanities by way of an example, I want to offer an alternative geneaology for the practice of distant reading itself. Rachel Buurma and Laura Heffernen provide a valuable history[ Ted Underwood’s “Genealogy of Distant Reading” presents a history of distant reading which is not for the most part centrally concerned with computers, and is therefore fundamentally distinct from concepts of “digital humanities” \\cite\{Underwood:2017uc\}. In Underwood’s history, distant reading is “a tradition continuous with earlier forms of macroscopic literary history, distinguished only by an increasingly experimental method, organized by samples and hypotheses that get defined before conclusions are drawn” (Underwood:2017uca p.29\}. Underwood “tease\[s\] out the elided social-scientific genealogy behind distant reading” \\cite\{Underwood:2017uca p.39\} to argue that the term “\[d\]istant reading was not coined to describe a radically new method. The first occurrence of the phrase, in \[Franco Moretti’s\] ‘Conjectures on World Literature,’ seems in fact to describe the familiar scholarly activity of aggregating and summarizing previous research” \\cite\{Underwood:2017uca p.9\}\[ what is your position on this? You go on to Buurma who does connect distant reading to computation - and you suggest how you will draw on similar ideas about collaboeation etc - but this underwood bit dangles\].

]of Josephine Miles as the first ‘distant reader’. Miles’ history, briefly, is as follows:

In the 1930s, as a graduate student at Berkeley, she completed her first distant reading project: an analysis of the adjectives favored by Romantic poets. In the 1940s, with the aid of a Guggenheim, she expanded this work into a large-scale study of the phrasal forms of the poetry of the 1640s, 1740s, and 1840s. In all of this distant reading work, Miles created her tabulations by hand, with pen and graph paper. She also directed possibly the first literary concordance to use machine methods. In the early 1950s, Miles became project director of an abandoned index-card-based Concordance to the Poetical Works of John Dryden. Partnering with the Electrical Engineering department at Berkeley, and contracting with their computer lab and its IBM tabulation machine, Miles used machine methods to complete the concordance. It was published in 1957, six years after she and several woman graduate students and woman punch-card operators began the work. It was thus begun around the time that Busa circulated early proof-of-concept drafts of his concordance to the complete works of St. Thomas Aquinas, and published 17 years before the first volumes of the 56-volume Index Thomasticus began to appear. (Buurma and Heffernan)

Buurma and Heffernan bring Miles’ history to our attention not simply because Miles predates  Roberto Busa, whose *Index Thomisticus* is often credited as the first large scale computational literary study.[^cf10] Rather, they emphasize, Miles’ origin story for computational literary study “can stand as an example of how we might write a history of literary scholarship that does not center originality and individual accomplishment” (n. pag.). Unlike Busa, Miles not only gave authorship to the (female) graduate students who carried out much of the labour of creating the concordances, she also thanked and credited the (female) punch card operators who encoded the resulting data.[^cf11] Moreover, when talking of Penny Gee, one of the female staff members of the computer lab, Miles praises her as “‘very smart and good’ and---most importantly---a true collaborator, as opposed to those ‘IBM people from San Jose’ ... ‘I’ve never been able to connect with them,’ Miles explains, ‘though I did with Penny Gee. She really taught me’” (n. pag.). Of the positive qualities highlighted here, only one, “smart,” is traditionally valorized among literary critics: to be “good,” a “collaborator,” who can “connect” and “teach” --- these qualities are often seen as irrelevant to the singular authority of the figure of the critic, but they are core to a reparative practice. Miles’ work, too, struggled to find appreciation “among literary critics who viewed her datasets as merely preparatory to the true work of evaluation” (n. pag.).  

What’s crucial, to use computational reading reparatively, is to use it *reflectively*. The desirable kinds of computation which I describe above will not happen inevitably. Here I draw upon the rich body of work emerging in critical algorithm studies, which examines (and attempts to reform) the human elements of computational algorithms. Any methodology is, to a certain extent, an “algorithm,” in the loose definition of ‘a series of pre-defined steps to be carried out’. But computational algorithms differ from “algorithms” implemented by humans. Computational algorithms have two key vulnerabilities: first, their operations are less easily scrutinized; second, their results are more easily trusted. The second vulnerability --- the cultural aura of empirical trustworthiness which accrues to anything ‘computational’ --- is another flavour of the same vulnerability that Drucker describes with ‘data’ generally. Because the human agents who designed and trained any given algorithm appear to be absent from its operation, the algorithm appears able to discover truth directly. This is how Daily Wire reporter Ryan Saavedra was able to tweet with disdain that “Rep. Alexandria Ocasio-Cortez (D-NY) claims that algorithms, which are driven by math, are racist” (@RealSaavedra): anything “driven by math,” he assumes, must be incapable of human fallibilities like racism. But as Safiya Noble shows extensively in *Algorithms of Oppression*, algorithms by default reproduce, and can easily exaggerate, the assumptions and biases of the culture in which they are made (CITE). In other words, in a racist world, algorithms *are* racist --- and sexist, and duplicative of all other systemic inequities.





### 1.2.2.  game studies ###



Game studies: infrastructure embeds a procedural argument.  

Critical algorithm studies is therefore a crucial background for my work --- but “critical” is literally in the name of of the field, and I still seek to be post-critical and reparative. As I encounter the limitations of the various information and tools through which I attempt to understand the 1790s, my goal is to do something other than facilely observe that they are limited. Instead, I want to identify the best ways to continue building on their foundations. In a digital humanities context, a focus on building connections can be mundanely practical: typing indexes from print works into spreadsheets, correcting errors within datasets, writing programs to process metadata: all of these maintain the functional usability of existing resources in new contexts. When this kind of extended, detail-oriented labour is combined with serious reflection on the histories and possible futures of these resources, I contend, they bring us to new knowledge. In this, maintaining and using digital resources is also a way to repair them --- and to produce reparative readings of their contents.



## 1.3.  methods ##  

This dissertation undertakes computational distant reading. At every possible point, however, the underlying methodology will be made visible, and its assumptions scrutinized. The bibliographic histories of my multiple corpora are explicit objects of inquiry. Much of the code underlying this project I have written myself. Some has been written at my request. In every case where the code is available to me, the program itself appears in Appendix A (“Codebase”), accompanied by a plain language explanation of how it operates. Where I have used closed-source software, Appendix A contains an explanation of my best guess at its underlying process. My exact use of these tools --- sufficient for another to replicate my work --- is provided in Appendix B (“Methodology”). These details are explicated in full in the appendices in order not to over-burden the body of the dissertation, but they are by no means *confined* to the appendices. Computation is not a “black box” to be consulted for simple answers, but is inextricable from my reasoning and argument.  

My attention to the *sources* of digital knowledge creation comes, in part, from Johanna Drucker, and her distinction between “data” and “capta.” Drucker, in “Humanities Approaches to Graphical Display,” specifically addresses the digital humanities practice of creating, and then close reading, data visualizations. She argues that the tools for visual representation which may be effective in the sciences cannot be simply and uncritically transposed to humanistic subject matter. When an experiment is presented as a ‘data visualization,’ she says, “the rendering of statistical information into graphical form gives it a simplicity and legibility that hides every aspect of the original interpretative framework” (8). In fields where the readers of such charts are also frequent creators of charts, and where norms exist to explicitly describe one’s interpretive frameworks in a methodology section, the simplicity and legibility of an individual chart may be a benefit which does not impede complex scrutiny of the information it presents.[^cf12] In a field like literature, however, the “graphical force” of something like a network graph or even a simple pie chart “conceals what the statistician knows very well --- that no ‘data’ preexist their parameterization” (8). Drucker problematizes the term “data,” the etymology of which presents it as a “given” which is stable and independent of observation. She proposes that humanities visualizations embrace, instead, the framework of “capta,” that which is “‘taken’ actively” (3), “fundamentally codependent, constituted relationally, between observer and observed phenomena” (50). Drucker’s assessment shapes my own prioritization of qualitative and reflective computational research. The term “capta” itself has not seen uptake in subsequent digital humanities scholarship, even in cases where scholars explicitly take Drucker’s warnings to heart. Accordingly, for clarity, this dissertation will continue to use the more usual term “data” to refer to the information gathered for analysis here. However, as I integrate and compare a wide variety of data from many disparate sources, a preliminary task of my analysis is always to determine, as precisely as possible, how the information was captured and quantified.   

Additionally, all of the figures presented in this dissertation are of my own design. My design praxis is informed by the work of Edward Tufte and Alberto Cairo, both of whom provide practical design advice in service of demystifying the visual rhetoric by which graphs present their arguments.[^cf13] Neither Tufte nor Cairo is a scholar of media studies; rather, they are professional practitioners of ‘data visualization’ who reflect critically on the assumptions of their work. Tufte’s work primarily strives to correct badly designed data visualizations, and the dangerous decisions that bad design can lead people to. His most famous example is an analysis of the engineers’ report at NASA which led to the ill-fated launch of the Challenger space shuttle in 1986: as his extensive visual analysis argues, the engineers (untrained in graphic design) unintentionally obfuscated crucial information about the day’s launch conditions. The poorly designed graphics these engineers produced made the launch appear low risk to their superiors; despite the engineers’ strong warnings, their verbal argument was disregarded in favor of their accidental graphical argument. As Tufte demonstrates, a few simple alterations of their graphic design would have made it obvious that the day’s unprecedentedly low weather was extremely dangerous, and potentially averted disaster \\cite\{Tufte:2001vw\}.[^cf14] Tufte’s six principles of design[^cf15] primarily seek to guide undertrained designers away from misleading themselves. Cairo, following on Tufte’s work from the perspective of an active journalist, more often turns his attention to successful designs which mislead their audiences intentionally. His forthcoming book, *How Charts Lie*, addresses the readers of infographics with insights into visual literacy \\cite\{Cairo:ikIksuMr\}. His preceding book, *The Truthful Art*, addresses the creators of good faith infographics with insights into visual manipulation \\cite\{Cairo:2016uv\}. Cairo draws a distinction between “data visualization” and “infographics”: “an infographic tells the stories that its designer wants to explain, but a data visualization lets people build their own insights based on the evidence provided,” summarized more succinctly as “infographics to explain, data visualizations to explore” \\cite\{Cairo:2014tl\}. Using this terminology, my argument will proceed with infographics in the body of the dissertation as curated figures to support my argument, with fuller data visualizations available in Appendix C (“Data”) to allow further exploration. Following in both Tufte and Cairo’s footsteps, I conceive of the figures throughout this dissertation as rhetorical devices. In service of arguing honestly, therefore, my designs --- in the body of the dissertation and in Appendix C --- are accompanied by footnoted explanations of my design rationale.   

This dissertation understands archives, bibliographies, anthologies, and corpora to all be, variously, *models* of an imagined object of study. In the language of social science, these models might be described as ‘samples,’ which are intended to permit discoveries about an underlying ‘population’ by being ‘representative’ of that population’s features. Only the language and not the methods of social science need to be imported here, since it has long been ordinary practice in literary studies to select and examine representative texts for insights about larger movements[^cf16]. A work like Ann Tracy’s bibliography *The Gothic Novel 1790-1830*, for example, clearly names the population of works which are of interest to her: all Gothic novels published between 1790 and 1830. She tentatively defines her principles of selection as \_\_\_\_\_. But in providing detailed information on 208 texts --- mostly Gothic, mostly novels, mostly between 1790 and 1830 --- Tracy obviously does not claim to have presented all that might belong within this population. Instead, her book operates as a model of the underlying population, which can be queried for further insight into ‘the Gothic novel, 1790-1830’ only so long as one keeps the limits of the model in mind. Indeed, by presenting plot summaries and bibliographic data, rather than reproducing the novels in full, Tracy provides a model of a model. One challenge to studying these models is that they present a “moving target”: even a bibliography or anthology is subject to change through successive editions (not to mention their now-common digital supplements), and a digital database has the potential to change daily. I follow Kath Bode’s approach in *A World of Fiction*, in artificially “freezing” each resource for study, and presenting my analysis as a description of a snapshot in time.[^cf17] Importantly, a model is a tool for thinking, and not necessarily a truth claim in itself: creating a model is a way of saying, ‘it might be helpful to think of X as Y,’ not an assertion that X is equivalent to Y. Willard McCarty[ need a citation for McCarty here] articulates this important feature of models by stressing that a model’s value is determined not by its exact correspondence with the object it models --- if it were possible to fully examine the underlying object, then no model would be necessary --- but by the *fruitfulness* of its simplifications. Even a deeply incorrect model can be fruitful if its divergence from observed phenomena rules out an incorrect theory. As I examine the many existing models of ‘English literature, 1789-1799,’ and create several more of my own, I articulate the underlying assumptions of each model, and assess the fruitfulness of the results.



## 1.4.  scope ##  

All of the computational work in this dissertation aims to identify, in as minute detail as possible, all works printed in England between January 1 1789 and December 31 1799. This eleven-year “decade” was a turbulent one across the Channel, encompassing the whole of the French Revolution, from the Estates General in 1789 to Napoleon’s coup in 1799.[^cf18] In England, these events caused strong and variously nationalist reactions in a country which had so recently lost its colonies in America and feared that a French invasion could come at any moment. This is the decade of *Common Sense*, it is the decade of *Lyrical Ballads*; it is the decade of Hannah More, it is the decade of Ann Radcliffe; it was the age of wisdom, it was the age of foolishness; it was the epoch of belief, it was the epoch of incredulity. Charles Dickens’ now famous superlatives capture the tension often seen by scholars between ‘Enlightenment’ modes of writing and ‘Romantic’ or ‘Gothic’ modes, which are no longer neatly periodized as mutually exclusive.

Scholarship on 18thC works often takes the form of evaluating or assigning the cultural capital of individual works, or, perhaps, analyzing the strategies by which they accrue or fail to accrue that capital. The winners of the cultural capital game are the Romantics in poetry and Walter Scott in prose. For example, Simon Bainbridge examines the decade and its poetry through the lens of war to identify “the attempts made by several writers to fill the role of national bard prior to Scott” (3). Both poetry and the poet, in his conception, are pursuing a particular kind of cultural capital that allows them to rise above their own popularity. Richard Cronin’s *The Politics of Romantic Poetry* and Robert Miles’ \[WHICHEVER ONE IT IS\], too, seem to treat Scott’s \[intensely serious popular romances\] as the teleological end of the late eighteenth century birth development within the novel. These works follow a pattern established from the beginning with \[Kiely and Tompkins\], of treating the novel as synonymous with the realist novel, and treating Romantic and especially Gothic novels as aberrations in the history of the novel, a problem which needs to be explained away. E.J. Clery’s *The Rise of Supernatural Fiction* has examined at length the historical conditions by which supernatural plot elements began to make limited claims to literary seriousness throughout the eighteenth century. The “rise” she describes is not an increase in volume and prominence of supernatural stories, since her starting point in 1762 (the Cock Lane ghost) is a major national phenomenon with many imitators. Rather, supernatural fiction ‘rises’ when it acquires cultural legitimacy. Michael Gamer has more recently expanded on how this ‘rise’ fuelled Romanticism’s own rise. Gamer, like Bainbridge and Cronin, primarily examines Wordsworth and the ‘winners’ of the struggle for cultural capital: I, like Clery, am more interested in the ‘losers.’ Accordingly, I attend to much that is *not* literature, in order to better understand why it is not.  

To navigate the 1790s, I turn to an author whose careers and works usefully focalize my core questions of genre, publics, and the status of literature: Charlotte Smith. Smith was highly productive in multiple genres throughout the 1790s, and had a complex and contested literary legacy after the 1790s. As literary scholars re-assess ideas about literary seriousness, popularity, and women’s writing, our assessment of Smith has shifted as well. By examining their bibliographies with computational methods, I again ask how she might continue to look different if we look at her a different way. I particularly examine the extent to which digital resources have kept up with the re-evaluation of Smith as a central figure in British Romanticism.  

Charlotte Smith is selected as a writer who was productive in multiple genres, only some of which may end up represented in corpora. Charlotte Smith’s literary career began with the publication of her volume of poetry *Elegiac Sonnets*, in 1784.[ maybe these whole bibliography sections should just be tables?] This work is the one upon which much of Smith’s fame and prestige rested in the eighteenth century. A second edition of *Elegiac Sonnets* rapidly followed the first in the same year, with only slight amendments. The third and fourth editions of *Elegiac Sonnets* appeared in 1786, adding new poems. 1786 also saw the publication of Smith’s *The Romance of Real Life*, a translation of *Les Causes Célèbres,* her first foray into prose, which would occupy the major part of the next phase of her career. In 1788 she published her first original novel, *Emmeline, or the Orphan of the Castle*. 1789 begins this dissertation’s decade of interest, a period of intense productivity for Smith: she had at least one new publication almost every year from 1789-1799. In 1789, she published her second original novel, *Ethelinde, or the Recluse of the Lake*, and a fifth edition of *Elegiac Sonnets*. In 1791 she published *Celestina,* her third novel; in 1792, her fourth novel, *Desmond*[ add info about the “phases” of her novelistic career], and a sixth edition of *Elegiac Sonnets*. Although *Elegiac Sonnets* continued to be reprinted, reaching its tenth edition in 1812, after this edition no further poems were added. Instead, her new poetry appeared in their own independent publications, and no longer took the form of sonnets. In 1793 she published *The Emigrants*, a poem in two volumes, as well as *The Old Manor House*, her fifth novel. In 1794, her sixth and seventh novels, *The Wanderings of Warwick* and *The Banished Man*. In 1795 she published her eighth novel, *Montalbert*, and began writing in a new genre with *Rural Walks*. With *Rural Walks*, Smith’s dominant genre again changed: having gone from a poet to a novelist, she now primarily published in a form which does not have a contemporary name: morally instructive natural history for “young persons.” 1796 saw the sequel to *Rural Walks*, *Rambles Farther*, as well as the novel *Marchmont*, and the poem *A Narrative of the loss...* of several ships. 1797 saw the eighth edition[ when was the seventh???] of *Elegiac Sonnets*, unchanged since the sixth. 1798 saw the novel *The Young Philosopher*, and more natural history for children in *Minor Morals*. In 1799, Smith tried her hand at theatre with *What Is She?*, a comedy --- not a form she will revisit. After this dissertation’s decade of interest, Smith continued to write at a slightly less frenetic pace. In 1800 she published the first three volumes of *Letters of a Solitary Wanderer*, an epistolary anthology of narratives. In 1802 she published two additional volumes of *Letters of a Solitary Wanderer.* In 1804, she published *Conversations, Introducing Poetry*, for children. In 1806, Smith published *History of England*, another work for young persons, and Smith herself died, age 55. The next year saw the posthumous publication of the poem *Beachy Head* and the work for young persons, *The Natural History of Birds.*  

Smith’s personal life sometimes overshadows this career. As her works often make clear to her readers, after a briefly comfortable youth as the daughter of a well-off country gentleman who lived beyond his means, she was married at age sixteen to Benjamin Smith, “son of a prosperous London merchant and owner of Barbados sugar cane plantations. The marriage was contracted hastily to remove her from her paternal home, now dominated by her new wealthy stepmother. Looking back in bitterness nearly forty years later, Charlotte Smith described the event as her father's decision to sell her like a ‘legal prostitute, in my early youth, or what the law calls infancy’ (Smith to Sarah Rose, 15 June 1804)” (Roberts). Benjamin Smith was cruel and violently abusive. He was also so financially irresponsible that his wealthy father, Richard Smith, wanted to prevent Benjamin from inheriting. Charlotte Smith assisted Richard with business correspondence and impressed him as responsible and competent. In recognition of her husband’s unreliability, “she persuaded \[Richard\] to relieve his son of all his ties to the business and establish him as a gentleman farmer in Hampshire” in 1774 (Zimmerman). Richard Smith died in 1776. “In an attempt to provide for his daughter-in-law, Richard bequeathed the bulk of his property to her children. But he had drawn up his will without professional advice; legal wranglings over the inheritance worth nearly £36,000 soon arose and were not settled until almost forty years later. By 1783 Benjamin had already unlawfully squandered more than a third of this trust and, as a consequence, found himself first in deep debt and then in King's Bench Prison.” (Roberts). After the success of the *Elegiac Sonnets* allowed Smith to pay for her husband’s release from prison, Benjamin Smith fled to France to escape further creditors. Charlotte Smith moved between England and France over the next year and a half to negotiate his debts, and in 1785, the family was able to return to England. In 1787, after 22 years of marriage, Charlotte Smith legally separated from her husband, “an unusual step for a woman of her time” (Fry 7), and moved to a town near Chichester with her nine surviving children (of the twelve she had given birth to). However, despite this separation, Benjamin Smith retained a legal right to Charlotte Smith’s profits from her writing. Smith moved frequently after her separation, due to financial instability and declining health. “On 23 February 1806 Benjamin died in a debtors' prison and some money reverted to Charlotte Smith. By then she was far too ill to execute her favourite scheme, to settle on the shores of Lake Leman. On 28 October 1806 she died, only eight months after her husband, and seven years before Richard Smith's estate was finally settled.” (Blank)  

Smith’s posthumous critical reception has undergone multiple shifts in appreciation and obscurity. Duckling’s study of her presence in anthologies indicates that shortly after her death in 1806, Smith was widely eulogized and anthologized, remembered and emulated as an important British poet. As the nineteenth century went on, poetesses began to be anthologized separately from poets, in collections with ambitions that were commercial rather than intellectual; Smith, too, “lost intellectual ground” even as she continued to be sold (Duckling 2016). By the end of the nineteenth century, even these volumes marginalized Smith’s poetry, with prefatory material which dismissed them as trite and depressing, unenjoyable reading. In the early twentieth century, Smith began to be considered as a novelist, rather than a poet; this new field did not lead at first to a much better reputation for her. Florence Hilbish produced the first extensive study of Smith, considering her as both poet and novelist, in 1941, to unappreciative reviews: Ernest Bernbaum’s faint praise said that “‘much time and care have been devoted to it; whether deservedly, is perhaps questionable,” since “the subtle or intricate is absent from Charlotte Smith's writings” (138). Hilbish presents Smith’s emotional poetry as sincere rather than conventional, and her prose as more motivated by politics than commerce.

Duckling credits the feminist movement of the 1960s and 1970s with the beginning of Smith’s recovery (217): the renewed interest in women’s writing rediscovered her novels, and especially the radical political content which Hilbish had observed. At the same time, Bishop Hunt published a record of Smith’s influence on Wordsworth, as demonstrated by an almost overwhelming amount of physical evidence: Wordsworth owned copies of her works, which he annotated; he copied out some of her sonnets in his own hand; he paid her a personal visit; he edited some of her poetry for publication; he wrote explicitly of her influence in notes to his works. Hunt calls Smith “an important early influence on Wordsworth which has not been explored in any detail up to now” (85); his abstract somewhat snarkily asserts that “Wordsworth did not suddenly start writing sonnets in 1802 simply because he happened to read Milton’s.” However, Hunt has little praise for Smith herself: of one poem, he says, “Whatever the artistic value of such verses,” what matters is the underlying theme which Wordsworth would later express more masterfully (89). Smith continued to be treated separately as an interesting woman novelist, and a minor pre-Romantic poet, through the 1980s. Smith rose to greater prominence in both of these fields in the 1990s: with work by Stephen Curran, Roger Lonsdale, Jennifer Breen, Andrew Ashfield, and Jacqueline Labbe, “Smith became established not only as a prominent figure in the revised female canon, but also as a central figure in Romanticism” (Duckling 217).

Throughout this history, two aspects of Smith which have prompted frequent re-assessments are her personal life, and her work across genres. The first matter, the importance of a female author’s life as a woman to her importance as a figure worth remembering, is implicit in several phases of the rise and fall described above. Fry is not alone in concluding that “\[f\]ew writers have presented themselves in their works so fully as did Charlotte Smith” (3): Smith’s poetry lyricizes her personal experiences, her novels feature autobiographical stand-in characters, and “the often intensely personal pleading prefaces” (Behrendt 189) to her works explicitly ask for them to be read light of her ongoing struggles. Perhaps as a result, much scholarship on Smith takes the stance of *The Literary Encyclopedia* in defining her as a woman who wrote because of, and chiefly about, her personal distress. Antje Blank’s article there highlights Smith’s financial motive to write: “Smith turned to writing when a failing marriage and a costly lawsuit left her without resources to raise her large family” (Blank). “And so,” Blank says, Smith “churned out” her novels (and the many editions of *Elegiac Sonnets*, and her other poetry, and her educational writing) to support herself and her nine children (Blank). Even when Smith’s Elegiac Sonnets “won her the reputation as an author of serious verse,” this is important primarily because it “lent greater respectability to her ensuing productions in a less prestigious but more lucrative genre -- the novel” (Blank). At the same time, as Labbe argues in her article “Selling One's Sorrows: Charlotte Smith, Mary Robinson, and the Marketing of Poetry,” Smith cultivated a public persona as a paragon of victimhood and motherhood, suffering deeply but turning her suffering into marketable prose out of a duty to her children. In periods where this image of womanhood is valuable, Smith is more easily valued, as in the eighteenth and nineteenth century anthologies which saw Smith as a moral exemplar (Duckling 203-4). Or, in periods when women’s resistance to patriarchal oppression is of scholarly interest, the direct, personal nature of Smith’s writing is valuable in itself, as in early feminist scholarship.

A complicating factor to these evaluations of Smith is that, as Labbe’s edited volume *Charlotte Smith in British Romanticism* thoroughly demonstrates, Smith’s writing is neither as uniform nor as simplistically personal as autobiographical readings sometimes see it. Labbe contends that Smith-the-novelist and Smith-the-poet have been largely studied as separate entities, “and consequently we have been learning about two separate Smiths, each closely linked to the genre she writes in, neither closely linked to the other” (5). Labbe is not quite the first to attempt to unify Smith: Carol L. Fry’s 1996 monograph *Charlotte Smith* also addresses her poetry before moving on to the several phases of her novel-writing, including the children’s writing which made up much of Smith’s later career but does not appear in Labbe. Indeed, from the beginning, Hilbish’s 1941 monograph explicitly identifies Smith as “Poet and Novelist” in its title. However, Labbe is accurate regarding the somewhat different assessments of Smith current in the somewhat separate study of novels and of poetry in general: Labbe argues that as a novelist, Smith is now often praised for her innovative narrative techniques (implying a mode of writing that is intellectual and ‘distant’), whereas as a poet, she is praised for her innovative expressions of interiority (implying a mode of writing that is emotional and ‘close’). Labbe draws greater attention to important differences between Smith’s writing personae in different genres, and her edited collection “pulls together many Smiths” (2) to address these disjunctions. The volume not only addresses her novels and poetry, but also includes her plays, letters, and posthumous reception. Each of these Smiths, the volume contends, has something innovative and unexpected to reveal, important to the formation of British Romanticism. In Judith Phillips Stanton’s “Recovering Charlotte Smith's Letters,” for example, Smith’s letters, less studied, reveal a third kind of writer, different from both the novelist and the poet, who conceives of herself as a professional businesswoman of her craft. More Smiths are available in genres not included in this volume, such as Smith the naturalist and children’s author (touched on only lightly in Labbe’s volume), or Smith the political philosopher who drives Amy Garnai’s *Revolutionary Imaginings in the 1790s,* a highly political Smith who consciously participates in the “political public sphere” conceived by Habermas, despite Habermas’ insistence that women were excluded from this sphere (1)*.*[ \[Something about each Smith having her own peers...? Political Smith now becomes peers with Mary Robinson and Elizabeth Inchbald, whereas Poetic Smith is peers with Wordsworth & Coleridge, and Novelistic Smith with Radcliffe etc\]] From these distinctions, Labbe concludes that “Smith, significantly, composes herself anew according to genre” (2) --- and then asks, “Is it all to do with inherent qualities of genre, or is it more to do with the expectations we as readers bring to different genres?”[ Labbe’s immediate answer: “Genre, it seems, carries a greater force in constructing our preconceptions of identity than has been recognized, and Smith is a case in point, a case we can crack by studying closely Smith’s style and techniques across genres.” (Labbe 5)] (5). This question about genre is one of the initial questions to inspire this dissertation: to see it asked as a core question about Smith demonstrates Smith’s suitability as a figure whose career can shed light on important questions about the mediascape of the 1790s.  

A core object of study for this dissertation is the makeup and history of contemporary digital databases. Eighteenth-century materials of various kinds have been collected in many digital archives, of very different scopes. I will draw materials from the English Short-Title Catalogue (ESTC), Eighteenth Century Collections Online (ECCO), the ECCO Text Creation Partnership corpus (ECCO-TCP), Google Books, Project Gutenberg and HathiTrust. My examination of these six databases will, of necessity, examine a ‘time capsule’ of their holdings at a particular moment; the sources of my data, and my procedures for working with them, are described in more detail in Appendix B (“Methodology”). The databases vary from each other in terms of two main qualities: their size, and their reputation. The reputation of any given digital resource is shaped largely, I argue, by its ability to signal ‘rigour’ in its collection practices. Several databases of different sizes have established reputations of seriousness, and, correspondingly, cultural capital within scholarly communities. The databases that I will examine at length form two groupings of three each, to explore two sets of related concepts. The first set consists of ESTC, ECCO, and ECCO-TCP, all of which follow the same rigorous collection practices at different scales. The second set consists of Google Books, HathiTrust, and Project Gutenberg, which follow very different collection practices while sharing a dubious scholarly reputation.  

The first three databases I examine will be no surprise to eighteenth-century scholars: ESTC, ECCO, and ECCO-TCP. Gale’s Eighteenth Century Collections Online (ECCO), contains over 180,000 titles 1701-1800, of which 42,000 were printed in England between 1789 and 1799. ECCO is itself (mostly) a subset of the broader English Short Title Catalogue (ESTC), which contains more 460,000 texts 1473-1800, of which 51,965 were printed in England between 1789 and 1799 (indicating that nearly 10,000 titles in the decade appear in the ESTC but not ECCO). The ESTC does not provide access to texts themselves: instead, it is an authoritative bibliographic catalogue, available as a searchable database. It is ECCO which provides texts: ECCO’s 180,000 titles works are available as photographed facsimiles of the full text of each title. The facsimiles can be searched within ECCO’s online interface; these searches examine a plaintext version of the facsimile pages that was generated by Optical Character Recognition (OCR), but this OCR text is not made directly available. As a result, the facsimiles may be read individually by scholars, but cannot form the basis for computational corpus analysis. A subset of ECCO’s texts have been hand-prepared, as part of the Text Creation Partnership (TCP), to be easier to use in computational research. The resulting corpus of ECCO-TCP texts contains 2,231 titles, of which 466 were printed in England between 1789 and 1799. These titles are available as carefully edited texts encoded according to the Text Encoding Initiative (TEI) standard, which not only provides an accurate version of the text’s words, but encodes substantial details regarding its context on the page. Most large scale distant reading of eighteenth-century literature relies on the ECCO-TCP corpus as its ‘model’ or ‘sample’ to represent the period[ footnote some examples]. Accordingly, one of the tasks of this dissertation is to examine the makeup of this corpus, and how it differs both from other corpora and from print culture in the period itself. These three digital collections --- ECCO, ESTC, and ECCO-TCP --- are the primary digital resources for the period, which form the basis of most digital research. However, they represent only one approach toward the collection and presentation of digital texts, to which there are two broad kinds of alternatives. These large but meticulous collections occupy a middle space between, on the one hand, highly selective thematic collections, such as The Shelley-Godwin Archive, of which there are many, and the giants of indiscriminate textual accumulation, such as Google Books, of which there are few.  

Smaller collections allow for more scholarly curation, but have corresponding limitations. Whereas the ‘main players’ of the the mega-archives can be easily enumerated, these specialized collections are numerous. Some will focus on particular kinds of texts, such as the Early Novels Database (2,041 novels 1700-1799) or Broadside Ballads Online (more than 30,000 broadside ballads). Others exhaustively index particular publications, such as *The Hampshire Chronicle* (1,950 references to fiction in issues from 1772-1829), the Index to the *Lady’s Magazine* (14,729 articles from 1770 to 1818), or the Novels Reviewed Database (1,836 reviews from *The Critical Review* and *The Monthly Review*, 1790-1820). Feminist scholarship in particular has seen the creation of resources like the Orlando Project, the Chawton House library Novels Online, Northeastern University’s Women Writers Online and UC Davis’s British Women Romantic Poets. The virtue of these collections is that they achieve even greater accuracy and comprehensiveness within their defined scope. The Shelley-Godwin Archive, for example, can reasonably aspire to digitize *every* known manuscript of Percy Bysshe Shelley, Mary Wollstonecraft Shelley, William Godwin, and Mary Wollstonecraft, and to provide these manuscripts in hand encoded plaintext transcripts. However, as is inevitable, these specialized archives have the vices of their virtues: their specialized focus allows them to adapt precisely to their materials, and their idiosyncratic data structures can rarely be combined with other resources. The William Blake Archive, for example, benefits enormously from designing its archive around the unique images of each page of each copy of each of Blake’s works. But because this approach is so well suited to Blake, it cannot be applied beyond Blake. Even if the archive’s resources were available for download, they could not be directly compared to materials from another source which does not record its information at such a minute level of detail. As a result, although a great deal of excellent digital scholarship is contained in specialized micro archives, I do not examine them further in this dissertation.  

Instead, I look at a set of larger archives of more contested “scholarly” status: Google Books, Project Gutenberg, and HathiTrust. Google Books may be the most infamous database of books. In a scholarly context, one hesitates even to designate this as an “archive,” particularly in the same breath as resources like ECCO: books of all kinds are scanned indiscriminately with only the bare minimum of roughly accurate metadata collected about them. These rapidly scanned books are prone to unpredictable errors, including inaccurate dates, misspellings, duplicate copies, and inaccurate subject classifications[^cf19] --- infamously, many books have “1899” assigned as their publication date because this date was used as a placeholder for “no date”.[^cf20] Nonetheless, Google Books is frequently used to study the prevalence of various “n-grams” (words or short phrases) over time, thanks to Google’s built in tool. The tool is able to search books which are, for copyright restrictions, not available directly to readers, making it highly tempting for questions about contemporary language use.  

Also in the category of smaller and specialized archives is Project Gutenberg. Project Gutenberg makes no claims to scholarly reliability but nonetheless underlies a not-significant amount of scholarly work[^cf21] --- its cultural capital as a resource lags far behind its use and utility. Project Gutenberg is easily conceived of as a haphazard, ‘unscholarly’ source for materials, but unlike Google Books, Project Gutenberg actually does have selection criteria. Project Gutenberg will only collect public domain works which contemporary audiences might be interested in reading for pleasure. It narrows the field substantially to exclude works which have either ceased to be broadly interesting (as in the case of most forgotten fiction), or which were never particularly interesting (as in the case of almanacs and tax codes). Project Gutenberg includes 57,796 texts: far more than specialized scholarly archives like the Early Novels Database or the Shelley-Godwin Archive, but nonetheless an order of magnitude fewer than its more voracious potential competitors. And, like smaller specialized scholarly archives, Project Gutenberg has tailored its holdings to make it easy for readers to read, and quite difficult for its collection to be applied to any other use. By tailoring the structure of the archive itself to its specific materials, these collections are able to thoughtfully achieve their aims --- but they also make it correspondingly difficult for users[ does this not depend on the user? unpack] to achieve their own, different aims.  

What makes Google Books of interest in the context of this dissertation is its relationship to HathiTrust, an increasingly popular resource for scholars. HathiTrust’s collection contains digitized content from “a variety of sources, including Google, the Internet Archive, Microsoft, and in-house member institution initiatives.” The “in-house member institutions” include one hundred and fifty-five universities, colleges, and consortia of universities. The aggregate scholarly authority of these institutions carries the weight of elevating HathiTrust above the Google Books scans which form the backbone of much of its contents: “The members ensure the reliability and efficiency of the digital library,” the website assures us, “by relying on community standards and best practices.” The texts themselves are stored in the database as facsimile page images and full-text OCR transcripts. In order to comply with copyright law, however, HathiTrust only provides large scale downloads and OCR transcripts for texts which are in the public domain. Most scholars use HathiTrust to run experiments on OCR transcripts of copyrighted texts, which they can only access through computational workarounds that intentionally make it impossible for the scholar to see the full transcript itself.[^cf22] These tools provide a unique solution to real barriers for digital scholars of contemporary literature: although copyright law would make it prohibitively expensive or even impossible to build corpora of post-1920s literature, HathiTrust’s mediated access to these texts enables corpus analysis. Through its collection, HathiTrust provides a hodgepodge of texts, of often unverifiable provenance and accuracy, selected largely by happenstance and convenience in a quest to contain all printed books. Through its tools, however, and through its institutional affiliations, HathiTrust has acquired a cultural capital among scholars which Google Books still lacks.  

HathiTrust’s success in acquiring scholarly capital stands in interesting contrast with Project Gutenberg’s continued lack of cachet. Project Gutenberg is used in research with similar frequency to Google Books’ n-gram tool,[^cf23] but scholars often mention Project Gutenberg with a note of apology for not having found a better source. Its cultural capital as a resource lags far behind its actual use and utility, likely, I argue, because its organizing principles are the ‘unserious’ ones of popularity and pleasure. Project Gutenberg is easily conceived of as a haphazard source for materials, but unlike Google Books, Project Gutenberg actually does have selection criteria. Project Gutenberg will only collect public domain works which contemporary audiences might be interested in reading for pleasure. This criteria might not render Project Gutenberg more useful for scholarly work but, it nonetheless narrows its selection substantially. Project Gutenberg includes 57,796 texts: far more than specialized scholarly archives like the Early Novels Database or the Shelley-Godwin Archive, but an order of magnitude fewer than its more voracious potential competitors. In taking Project Gutenberg seriously as a collection of texts, I seek to explore the extent to which its reputation as “unreliable” may or may not be deserved.  

As this brief survey of eighteenth-century digital archives shows, there is no ‘perfect’ corpus for large scale study of eighteenth-century texts.  Moreover, I argue, the imperfect samples which each archive provides are shaped not only by historical factors of eighteenth-century print culture, but also by contemporary digital culture. Each archive represents a unique set of choices in response to the same sets of questions: what to include, why, how; what to make accessible, why, how, to whom; what, in the end, makes a text matter, and what we are meant to *do* with texts. As this dissertation will argue, these questions of digital history have important resonance with literary questions about literary canon formation.



## 1.5.  dissertation map ##  

Chapter two examines Charlotte Smith and the ways that her writing is made accessible today. The specific experimentation undertaken in chapter two tests the basic assumptions and methods of my project. I identify what subset of Smith’s works each corpus contains, as a concrete example to compare their holdings overall. Smith’s *Elegiac Sonnets*, for example, are not included in the ECCO-TCP corpus (which is the one most often used for text mining research) --- only *Celestina* and *The Emigrants* are included. Why these two texts? And what text mining research based on ECCO-TCP might have found slightly different answers if Smith’s sonnets had been included? As a related test of comparison between databases, for each database which provides access to the actual text of Smith’s works, I compare the textual similarity of *Celestina* and *The Emigrants.* What editorial choices are being made? How *much* worse is the OCR text than the transcribed text? Another key concept I will explore through Smith is the role of reprints. HathiTrust, for example, includes multiple editions of *Elegiac Sonnets*. How reliable and effective are its distinctions between editions? How do the databases I examine handle multiple editions of a single work? I am particularly interested in how reprints can be incorporated into our understanding of what literature is “of” a particular decade: what does it mean to think of *Elegiac Sonnets*, initially printed in the 1780s, as “1790s literature”? Finally, having surveyed my six databases with the help of Smith, I discuss the multiple “Smiths” which emerge, and what it means to attempt to unify her disparate works.



### 1.5.1.  chapter 3: databases ###



In chapter three, I re-examine my core databases, but no longer with Smith as a focalizing lens. Instead, I undertake computational assessment and comparison of the databases’ contents. My research examines the authorship and subject matter (broadly construed) of all titles printed in England between 1789 and 1799 which are included in the English Short Title Catalogue (ESTC) database, the Eighteenth Century Collections Online (ECCO) database, the Eighteenth Century Collections Online Text Creation Partnership (ECCO-TCP) corpus, or the HathiTrust database. I calculate the proportion of the titles in each resource that are attributed to men, to women, or are left unsigned. Using the titles of these works and a topic modelling tool which has performed accurately when compared to a manual assessment of random samples, I also roughly identify the subject matter of each title, categorizing works into broad genres such as drama, poetry, Romance, History, or sermons. I am then able to compare these four resources to each other, and to existing scholarly work on the print production of the 1790s. For example, I am able to compare each resource’s holdings to the statistics on the English novel included in Garside, Raven and Schöwerling’s *Bibliographical Survey of Prose Fiction*. These comparisons illuminate the implicit principles of selection which have shaped the ECCO-TCP and HathiTrust digital collections, but also suggest conclusions about the period itself. I present multiple ways of visualizing the same underlying “data,” (that is, the print production of the 1790s itself) in order to analyze the implications of each way of seeing the decade’s literature. From what viewpoint, for example, can we see the hordes of female Radcliffean imitators which were so decried for dominating the print marketplace? And what other visions are possible?  

In my fourth chapter, I playfully attempt what might be considered a devil’s advocate method of textual selection: pure random sampling. Using a random number generator, I select ten arbitrary texts to close read.  

A final brief conclusion to this dissertation offers an assessment of the role of digital textual collections in contemporary literary study.  

“But however essential it is for data-rich literary history, modeling cannot be the sole foundation for the field. **Models of literary systems are not simply arguments** about the existence of and connections between literary works in the past; **they are arguments made with reference to the disciplinary infrastructure**---the bibliographies and collections, analog and digital---that transmit evidence of past works and relationships to the present. Modeling, even when integrated with descriptive bibliography as I have described, does not reflect on this transmission. Models embody a scholar’s arguments, whereas disciplinary infrastructure is an effect of multiple arguments: a sequence of assumptions, decisions, representations, and remediations. Such histories of transmission shape how the researcher can explore, and what she can know of, the historical context that disciplinary infrastructure appears to represent. To adequately perform literary history, data-rich projects must investigate these histories of transmission and how they constitute the documentary record.” (Bode 42)



“The distinctions or gaps between the context signified by collections and the exemplars used in signification **might partly arise from, but are not simply the consequence of, successive exclusions of documents**, as the Stanford Literary Lab Pamphlet 11 suggests. In chapter 1 I noted that, in defining “the published,” “the archive,” and “the corpus” as progressively smaller selections, those authors admit the constructed nature Page 44 →of literary data. Yet they also argue that mass-digitization largely avoids those exclusions, such that “the corpus of a project can now easily be (almost) as large as the archive, while the archive is itself becoming---at least for modern times (almost) as large as all of published literature” (Algee-Hewitt et al., 2).

Even with their account of the considerable practical challenges involved in accessing versions of specific literary works, this description of mass digitization drastically diminishes the degree of exclusion involved in constructing such collections. ” (Bode 43-4)



“Digitized collections are partial in another way, in that combining the holdings of multiple analog collections tends to obscure the individual histories of the contributing collections and their implications for the form, scope, and critical capacity of the resulting digital one.” (Bode 44)



“Collections have always been constituted in this way. In analog collections, documents are represented and remediated through the cataloging systems that organize holdings and the interfaces that interpret them: the card catalogs, special collection indexes, or online library catalogs that provide a method of searching and a type, form, and detail of metadata. ... A key difference between analog collections and digital ones is that literary historians rarely, if ever, treat the former (a given library, for instance) as proxies for literature as it circulated and was understood in the past, whereas digital collections such as Google Books or HathiTrust are sometimes assumed to be representative in this way.” (Bode 44)



“Due to their multiplicity and complex interaction, the components involved in producing digital collections expand access to the historical record in certain ways, even as they increase the likelihood of unrealized and significant disjunctions between the access we intend and the access we achieve. **Digital humanities scholars recognize that digital infrastructure shapes knowledge production, and digital literary historians have responded with explicitly curatorial approaches** to constructing and exploring digital documents and collections.” (Bode 45)



“These digital humanities projects highlight **four features** that I believe should also **underpin the modeling of literary systems in data-rich literary history**. First is a critical assessment of the **relationship between the historical context analyzed and the digital collection(s) used** for analysis; second is detailed attention to the **relationship between the documents included in the digital collection(s) and the terms in which they are represented**; third is explicit discussion of the **means by which data are extracted and modeled**; and fourth is a **published record of data** arising from that extensive history of transmission. Existing projects in data-rich literary history often (though by no means universally) demonstrate the second and third of these features. The need for data publication, and for platforms and modes of review to support it, is also increasingly recognized and enacted.7 But data-rich literary history projects rarely consider how the disciplinary infrastructure analyzed relates to the historical context investigated. The lack of shared standards for data publication---and, more specifically, of a framework for combining these four features in investigating and representing the transmission and transformation of historical evidence to and in the present---problematizes the field’s capacity to advance historical knowledge.” (Bode 46)



“Proprietary mass-digitized collections such as Google Books, Early English Books Online, and The British Newspaper Archive (owned by Google, ProQuest, and findmypast, respectively) are increasingly used in humanities research. But their scope and scale---let alone the histories of transmission that produce them---can be very difficult to discern; indeed, **the commercial imperatives of these enterprises arguably depend on them presenting these collections as comprehensive**.” (Bode 47)



# ch 2 - Charlotte Smith #



## intro ##  

Ian Gadd argues that critiques of digital databases often not based on the right grounds: “the real risk of scholarly misuse of \[Early English Books Online\] is less to do with the physical features of early printed books that it fails, one way or another, to represent (problematic though these are) and more to do with a lack of an informed knowledge of what exactly EEBO *is*.” (Gadd 682) The observation applies far beyond EEBO: although it is easiest to critique digital resources for their failures to perfectly replicate the full tactile experience of the books they simulate, or for failing to contain every possible book, those critiques are fundamentally futile. \[What other critiques are possible?\] This chapter examines the history, textual selection, and implicit model of six databases: the ESTC, ECCO, ECCO-TCP, Project Gutenberg, Google Books, and HathiTrust. I explore how each database encodes its assumptions about what literature is, who it is for, and how it should be used. I historicize these models in the context of the institutional infrastructure behind the creation of each resource, particularly the economic factors driving development. I argue that the differences between each database’s literary model are best understood as the result of different strategies to navigate conflict between commercial and anti-commercial values. In parallel, I follow Charlotte Smith through each of these databases, to explore the impact of their different literary models. Very different works by Smith are available in each resource, and in very different formats. Smith’s most major works are not readily available, suggesting that these literary archives lag behind scholarly consensus about her importance. The chapter concludes with a discussion of new developments in Optical Character Recognition (OCR) technology, which can be used to transcribe texts from page images. \[OCR BLUF.\]







## database histories ##  

In the next section I will close read the implicit models underlying each database, to examine how each enforces a particular concept of “literature” and “a text.” However, before these models can make sense, we must understand the history of how they were built. I contend that each database is best understood as a negotiation between the noncommercial values of textual reproduction and \[capitalism\]. Each database has the goal of making valuable information available. After the 1990s, they are particularly influenced by the utopian ideal that digital reproduction at last made textual reproduction free. Each had to contend, however, with the fact that before a text can be reproduced digitally it must be *created* digitally, and that even if the material costs are entirely eliminated (which, of course, they are not) textual creation continues to have costs in labour.



### 2.1.1.  ESTC timeline history ###



The history of the English Short-Title Catalog is long, as befits its enormous scope. “The English Short-Title Catalog (ESTC) is a vast database designed to include a bibliographic record, with holdings, of every surviving copy of letterpress produced in Great Britain or any of its dependencies, in any language, worldwide, from 1473-1800” (CBSR)[ not sure how best to cite this - <http://estc.ucr.edu/index.html#>]. Today, “The English Short-Title Catalogue is the most comprehensive record of what has appeared in print in Britain and the English-speaking world for all branches of human experience from the last quarter of the fifteenth century to the start of the nineteenth. More specialized studies exist for fields and eras within that span, but no other resource matches ESTC’s dependability over such a broad range” (Vander Meulen 265).

It began as the Eighteenth Century Short Title Catalogue in the 1970s, operating in a similar line as the original Pollard and Redgrave Short-Title Catalogue for 1476--1640, which first appeared in 1926, and Donald Wing’s catalogue for 1641--1700, which appeared in 1951. These catalogues established the ambitious simplicity of the ESTC: to accurately describe every edition of every printed work in English or from the United Kingdom. After the completion of Wing’s STC, “\[e\]xploratory studies, poorly funded and inadequate though they were” (Korshin 209) throughout the 1950s and 60s pursued the feasibility of systematically accounting for the much larger body of printed work produced in the eighteenth century. The Eighteenth Century Short Title Catalogue began properly in 1976, at a conference jointly sponsored by the British Library and the American Society for Eighteenth Century Studies (Crump 106). Here, “bibliographers and librarians attempted both to arrive at a consensus of the size of the task and the methodology that would have to be adopted to achieve a union catalogue. However, until the works were catalogued, it would not be possible to answer basic questions (such as the potential number of extant items) which would predetermine working methods. The very fact that they found it difficult to agree for want of sound and accepted figures indicated the need for ESTC.” (Crump 105). A pilot project began at the British Library in 1977, under the direction of Robin Alston (Crump 105).

Unlike earlier Short-Title Catalogues, which appeared as lengthly print publications, the Eighteenth-Century Short Title Catalogue was conceived as digital from the beginning --- a decision which, as Karian notes, “exhibited considerable foresight” (283) in the 1970s. As a result, “ESTC records existed in digital form long before many humanists saw computer technology as central to their work” (Karian 283).  Robin Alston and Mervyn Jannetta developed their own cataloguing rules, distinct from the Library of Congress MARC and UK MARC standards (Korshin 211). Once these standards were established, the British Library began to re-catalogue its own holdings, and in 1979 libraries in the United States, Germany, and Australia undertook to supplement them. In these international collaborations, “Where ESTC records already existed, these were adopted as the \[new\] record and only those works not held in the ESTC base file were catalogued again” (Crump 105). “One implication of the publication history of short-title catalogues is that they have been deemed functional and valuable even before they were complete. (That estimation is crucial, for their full completion is for all practical purposes impossible.) Judging that even a preliminary form of the records was useful to scholars, the planners of ESTC determined to conduct its development ‘in full public view’ and to make the incomplete file available ‘warts and all’[ I can probably say a LOT more about all of this --- this is my answer to the “moving target” problem; move it to the introdction] (in the words of Henry Snyder and Michael Crump, responding to criticism by Peter Blayney)” (Vander Meulen 270). Accordingly, the in-progress database “was soon available online, from 1980 via the British Library BLAISE \[British LibraryAutomated Information SErvice\] system and from 1981 in the US Research Libraries Group RLIN \[Research Libraries Information Network\] system” (Norman). Each of these databases was worked on locally by researchers, and then updated and reconciled with each other weekly.

To supplement these databases, accessible almost exclusively to librarians with specialized training in operating them and primarily used by the scholars compiling the file, the ESTC intended to publish editions at particular milestones of completeness, intended for the use of non-librarian scholars. Their “first step, a fiche catalogue of \[the British Library’s\] holdings, together with indexes, generated by the computer” (Crump 105) was published in a microform “snapshot” in 1983, but other milestones did not occur according to schedule. The “joint Anglo-American interim publication of the ESTC file ” (Korshin 212) which was expected to follow on microform in 1984 (Korshin 212) did not appear. Alston attributed the delays partly to the immensity of the task, and partly to the impact of short-term cost-cutting decisions, like the reduction of early-stage proofreading or of in-person examination of books, which dramatically increased the labour of verifying the resulting database record. Although he consistently warned “how easily strategic decisions based exclusively on cost usually lead to greater, not less, eventual costs” (Alston), the ESTC each year seemed to be facing a new budget struggle, and important maintenance labour was several times deferred. This created something like a paradox for the ESTC: funding bodies wanted to commit less money to a project which was behind schedule, but the project would remain behind schedule unless it was funded to complete the work required. 

Nonetheless, work continued, and in 1985, the online databases in RLIN and BLAISE were upgraded to allow dynamic updates to a single shared file (Crump 106), which for the first time allowed continuous access to a shared record, rather than the constant exchange and messy merging of individual partially-overlapping records. “Until the file was dynamically available online on RLIN in 1985 batch processing was a weekly nightmare” (Alston). At this time, it was hoped that the new RLIN file would “result in a more complete and coherent ‘first edition’ of ESTC” to be published in 1989 (Crump 106), though this deadline, too, was not met. In the mean time “the ESTC file \[was\] available to scholars on both BLAISE-LINE and on RLIN.” (Crump 106). To facilitate its use, the ESTC distributed “\[a\] simplified manual for searching the file on-line” (Crump 106). Crump took the opportunity of the update to rhapsodize on the database’s potential usefulness for other scholars: “No longer is the scholar limited in access to the data by the fixity of the printed page” (106). This valuable resource was not without cost. Although the manual on how to formulate search queries was free, use of the ESTC itself was notably not. Institutions or individuals paid to subscribe to the ESTC itself, paid per query for searches to be run, paid per minute for being connected to the database, and often paid for access to the computers they must use in their own libraries. Tabor says “the ongoing expense of consulting ESTC was the cyber-equivalent of the hefty up-front payment needed to acquire its printed predecessors, STC and Wing” (367).

“In 1987, with the agreement of the Bibliographical Society and the Modern Language Association of America, the International Committee approved the extension of the database to cover the period from the beginning of printing in the British Isles (ca. 1472) to 1700. The file changed its name to the 'English Short Title Catalogue', thereby keeping its well-known acronym. The USA team began cataloguing pre-1701 material in 1989, joined in the mid-1990s by the British Library team, and the resulting records were made available in the RLIN file from 1994.” (Norman). “In 1992, IESTC approved a further extension of the file to include serial publications. The USA team began work in 1994 on the cataloguing of serials within the scope of ESTC” (Norman). 

Concurrently with the development of the ESTC, Wing’s seventeenth-century STC was undergoing redevelopment into a second edition, overseen by Katharine Pantzer. The second edition of Wing’s STC was published in two volumes in 1976 and 1986, followed by a set of exhaustive indexes in 1991. This second edition “represented a vast development of the original” (Vander Meulen 268), incorporating thousands of new entries, expanding the titles, and adding explanatory notes and headnotes. Its completion in 1991 also marked the end of the ability of its publisher and sponsor, the Bibliographical Society, to support it (Vander Meulen 269). “Accordingly, in 1999 the Society made an agreement with ESTC whereby the latter... would assume official responsibility for receiving new STC data” (Vander Meulen 270). The ESTC continued to research new entries and improve existing ones, releasing a second edition of the file on CD-ROM in 1998 and a third edition in 2003 (Norman).

In 2006, almost thirty years after the commencement of the project, the ESTC underwent another major shift: the database was made publicly available to be searched for free online. This inspired more rhapsodizing, this time from Tabor: “The freeing of ESTC ... now places in one location, for the consultation of anyone with internet access, the fullest and most up-to-date bibliographical account of ‘English’ printing” (367). At the same time, the ESTC began a project “to provide full title and imprint transcriptions for the eighteenth-century records” (Tabor 370). Vander Meulen says that “The history of ESTC is in fact the record of steady developments. Some have been conspicuous---for instance, the physical progression from a printed prototype to microfiche, CD, online access via the vendors Blaise Line and RLIN, and universal online availability through the British Library.” (Vander Meulen 270) Many more have been less visible, in constant improvements to the accuracy and detail of the records. In 2011, the Center for Bibliographical Studies and Research at the University of California Riverside was awarded a planning grant from the Andrew W. Mellon Foundation to “redesign the ESTC as a 21st century research tool” (“Planning Grant”)[ add to bibliography], which was followed in 2013 by a larger two-year grant to execute software improvements to the ESTC.



### 2.1.2.  ECCO timeline history ###  

To understand the history of the ESTC and ECCO, we actually need to begin with another resource: Early English Books Online, or EEBO.



“EEBO’s relationship with the original STC and Wing is straightforward and clear; EEBO’s relationship with electronic ESTC, on the other hand, is less well-known.20 A series of agreements made between ESTC and University Microfilms/ProQuest between 1989 and 1997 allowed EEBO to draw directly on ESTC’s existing bibliographical data. Consequently, / every search run on EEBO (with some exceptions) relies, in a fundamental sense, on bibliographical information originally supplied by ESTC -- but not in the form that one might expect. First, EEBO heavily edited ESTC’s data for its own purposes: certain categories of data were removed (e.g. collations, Stationers’ Register entrances), some information was amended (e.g. subject headings), and some was added (e.g. microfilm- specific details). Second, there is no formal mechanism for synchronising the data between the two resources. Occasionally, snapshots of data are sent by EEBO to ESTC but there is no guarantee that a correction or revision made to an ESTC entry will be replicated in the corresponding EEBO entry or vice versa: neither ESTC nor EEBO will necessarily know when the other has made a correction. As both resources continue to amend and expand their bibliographical data for their own purposes, there is an increasing likelihood of significant discrepancy between the two resources. Finally, although EEBO continues to microfilm and digitise, there is no absolute one-to-one correspondence between the pre-1701 entries in ESTC and the materials on EEBO; there are -- and will always be -- items on ESTC not available on EEBO.” (Gadd 685-6)  

Began as Research Publications, Inc in 1981, which began the Eighteenth Century Collection in 1983. Their rival was University Microfilms. RP became Thomson Gale, now Gale Cengage. UM became ProQuest.



By 1997, Research Publications, Inc had become Primary Source Media. (LOC http://id.loc.gov/authorities/names/n98069963.html) In September 1998, “the Thomson Corporation \[merged\] three of its electronic and reference publishing subsidiaries---Gale Research, Information Access Company (IAC), and Primary Source Media---into a new company called The Gale Group.” (http://newsbreaks.infotoday.com/NewsBreaks/Thomson-Merges-Gale-IAC-Primary-Source-Media-into-The-Gale-Group-17999.asp)  

*1983 Eighteenth Century Collection* microfilm begins to be produced by Research Publications, Inc



EEBO: “unlike scholarly facsimile editions, the selection process for microfilming was often arbitrary. Copies were selected primarily by reference to the copies listed in STC and Wing, with particular preference for certain major collections; they were not selected because they were considered representative of a particular edition. By bringing together the bibliographical record for an edition and (usually but not always) only a single witness of that edition,22 EEBO is obviously aiming to provide a useful scholarly mechanism in terms of searching but by doing so are implying -- albeit not deliberately -- that the record and the copy *are* *one and the same thing*. It would be better, perhaps, if EEBO represented itself as a library of copies, rather than a catalogue of ‘titles’.” (Gadd 687)



EEBO: “in digitising the microfilms in their original forms, EEBO decided (presumably for commercial reasons rather than purely scholarly ones) against sanitising the images. Openings are retained rather than broken into single pages; images are not cropped; rulers, place-holders, and descriptive notes are left in place; blank leaves are not removed. While Kichuk’s concern about ‘remediation’ is a real one, it is difficult to use EEBO for any length of time without being reminded that these are reproductions of actual objects.” (Gadd 688)  

ECCO, as a distinct resource, is a historic latecomer compared to EEBO. The initial microfilms were created in \[???\] by \[???\]. In 2003, Thomson Gale began making digital copies of the Eighteenth Century Collection microfilms available to subscribers online. The digital images were made from the microfilm masters, which were 400 dpi, and thus higher resolution than the microfilm copies. 



“the move from microfilms to the Internet has meant easier searching, easier physical access, easier manipulation of the images, clearer images and (for the UK at least) cheaper institutional access” (Gadd 685).



“However, the path to digitization was not ideal: these documents were imaged in the late 1970s, transformed into microfilm during the 1980s, and the microfilms digitized in the 1990s. Because of the state of reproductive technologies during the late 20th century, as well as the circuitous path to digitization (through microfilm), the image quality is very poor and bitonal, with no greyscale / images available. Furthermore, the original documents themselves, printed with premodern technologies, pose problems even for human readers of their pages, but much more so for optical character recognition (OCR) engines. For example, printed characters were not perfectly situated on a baseline, blackletter fonts were used, ink bled through the paper, and the typeface was broken and overworn.” (Christy et al. 1-2)

Reference “deep fried memes” --- xkcd comic: <https://xkcd.com/1683/>



The microfilms were scanned at 300 dpi (Spedding 440).  

“Part I includes 135,000 printed works, comprising more than 26 million scanned facsimile pages.” (Gale, “Part I”) “From books and directories to bibles and sheet music to sermons and pamphlets, Eighteenth Century Collections Online, Part II features a variety of materials to provide a critical tool for both faculty research and classroom use. With more than fifty thousand new titles of previously unavailable or inaccessible materials, Eighteenth Century Collections Online, Part II is an essential addition for current owners of Part I.” (Gale, Part II). Each part (sold separately) also describes itself in breathless language assuring users of the comprehensiveness and breadth of the resource --- even as its existence in two parts belies this.



“Although "Eighteenth Century Collections Online, Part 2 adds six million pages that include previously unavailable or inaccessible titles," the latest promotional leaflet claims a total of "26 million pages of text" for ECCO - instead of 32 million - which is the same total that appeared in the original promotional leaflet for what is now understood to be ECCO Part 1. See "About Gale Digital Collections," accessed 1 March 2009, http://www.gale.cengage.com/DigitalCollections/; Eighteenth Century Collections Online (2008), accessed 1 March 2009 http://www.gale.cengage.com/pdf/facts/ECCO.pdf; and "Eighteenth Century Collections Online: The eighteenth century's most important revolution has just begun" (2005”) (Spedding 450)   

In late 2019, Gale began allowing access to a new interface, the Gale Digital Scholar Lab, which dramatically changed the forms of access available for ECCO texts. It became possible not only to see the underlying OCR for texts, but to run pre-built text mining on it, and to download the OCR as text files. The only limit to downloading is that only 10,000 texts may be downloaded at a time, but as long as the desired corpus can be defined as “collections” in chunks of 10,000 or less, any number of files can be downloaded. In a particularly dramatic departure from ECCO’s past practice and current norms, I was told that there were also no restrictions on sharing the downloaded files, even though downloading them in the first place required a library subscription.



### 2.1.3.  TCP timeline history ###  

“The Text Creation Partnership started, in 1999, as a collaboration between the university libraries of Michigan and Oxford, the Council on Library and Information Resources, and the publisher of Early English Books Online, Proquest. The aim was to create high quality ‘standardized, digitally-encoded electronic text editions’ starting with 25,000 titles from Early English Books Online.” (Gregg n. pag.)

“The Text Creation Partnership was conceived in 1999 between the University of Michigan Library, Bodleian Libraries at the University of Oxford, ProQuest, and the Council on Library and Information Resources as an innovative way for libraries around the world to:

* pool their resources in order to create full-text resources few could afford individually

* create texts to a common standard suitable for search, display, navigation, and reuse

* collaborate with commercial providers, rather than constantly bargaining and competing with them” (TCP “Welcome”)

Those four named organizers --- Michigan, Oxford, ProQuest, and CLIR --- are the tip of the iceberg for institutional buy-in. At “Our scholarly partners,” TCP lists *two hundred* institutions which were involved in at least one of EEBO-TCP Phase I, EEBO-TCP Phase II, ECCO-TCP, Evans-TCP, many of which were involved in multiple of these projects. “This list does not include higher-education institutions based in the UK, all of which are counted as EEBO Phase 2 partners thanks to consortial funding” (TCP “Our scholarly partners”). The “FAQ” page still shows traces of the enormity of this undertaking. One question reveals that each partner library committed $60,000 to the venture, an amount which funds the keying and conversion of only around 250-300 books --- a large number, but a drop in the bucket compared to the approximately 73,000 books which the TCP as a whole has encoded.

A ten-person working group developed an encoding standard. The University of Michigan Library staff developed instructions to use this encoding standard. And then the text creation began. “Texts were selected each month at Michigan, page-images were supplied by ProQuest, marked-up transcriptions were submitted by the vendors, and quality control and editing undertaken at U-M Library and soon also at Bodleian Libraries in Oxford and subsidiary sites at the National Library of Wales, Aberystwyth, and at the University of Toronto.” (TCP “About”)

  

“EEBO-TCP met its goal of producing 25,000 books in 2009 (thereafter known as “EEBO-TCP Phase 1”), and then undertook work on a second phase to convert the first edition of each remaining unique monographic work in EEBO---another 40,000 or so books, for a total of around 70,000, if all hopes were realized.” (TCP “About”)

“In 2005, the TCP executive board and staff sought to expand the TCP model to other databases of historical books, namely, Gale Cengage’s Eighteenth-Century Collections Online (ECCO) and Newsbank Readex’s Evans Early American Imprints (Evans-TCP). These projects never received quite the support attracted by EEBO-TCP, and in the end produced only about 8,000 texts, compared to the 60,000 produced by the latter, with another few thousand on the way.” (TCP “About”)

As early as 2003, the TCP executive board meeting minutes reported that “Michigan has made agreements with Gale and Readex to support conversion of subsets of the Eighteenth Century and Evans Early American materials which will allow us to create a cross-searchable corpus of important historical texts ... The University of Michigan has reached agreements to create a subset of accurately keyed and encoded texts in conjunction with these projects, and aims to produce 6,000 early American and 10,000 18th century texts. In the near term, this will not affect production of EEBO texts because there is adequate capacity to expand beyond existing levels of production. In the long term, this will produce a large number of culturally significant texts, produced to a single standard, that are owned by the library community and complement the EEBO texts for these early historical periods.” (“Meeting Minutes 2003-10-22.”).

In 2004, “Jeff Moyer then updated the Board on its progress with the ECCO product which contains over 26 million pages and 155,000 volumes. To date, they have 60 customers including 6 Canadian and 11 international institutions. They have also done OCR for the ECCO product and are interested in how the TCP text will work and integrate with their OCR.” (“Meeting Minutes 2004-10-21.”).

“In 2005 the project expanded to include Gale-Cengage’s Eighteenth-Century Collections Online (as well as Evans Early American Imprints by Newsbank). However, while the EEBO-TCP project flourishes (with around 40,000 texts transcribed so far), the work on ECCO-TCP stagnated at around 2,000 texts. As well as the main partner institutions of Michigan and Oxford that oess to the eighteenth-century TCP texts, so I’ve listed them below, with a few comments.” (Gregg n. pag.)

By October 2005, “the ECCO-TCP project has commenced with the release of a demo, and TCP sponsored an ECCO selection task force in August. ... Rich Foley reported that ECCO now covers 120 subscribers and a recent purchase from the JISC. The ECCO product has also sold well in Canada this past year. Rich also reported (relating to a question on Metadata) the release of a My Library product which is opening up access to their metadata at Gale and that he was interested in further doing case studies on how ECCO is used in research and in the classroom.” (“Meeting Minutes 2005-10-20.”)

In 2006, ECCO-TCP was struggling compared to the other TCP products. “Rich Foley reported that ECCO is one of the biggest products at Gale with eighty to ninety ARLs subscribing as well as small institutions. He also said that a focus at Gale was to work on more tools to facilitate undergraduate teaching of their products. ... Mark Sandler reported that the TCP budget shows mostly positive balances through 2007. The exception to that is ECCO but because it is still so early in the project, it seems likely that TCP will overcome those problems within the next few months. Nonetheless, the TCP project in EEBO, Evans, and ECCO face potential budget deficits in fiscal year 2008.” At this time, the TCP began to think about the end of the project: “the TCP should set a date to close the partnership (likely around 2010 given current commitments,” partly to address financial solvency. (“Meeting Minutes 2006-09-16”).

In 2007, all three TCP project reported successful sales, though ECCO’s news was the most vague: “Brandon Nordin also reported good news from Gale and along with Mary Sauer-Games announced that the EEBO and ECCO databases will now be cross-searchable so that users can go to either collection and find records from the other” ( “Meeting Minutes 2007-10-30”). Nonetheless, “Evans-TCP and ECCO-TCP sales have historically (for a variety of reasons, chiefly the presence of OCR text in both projects) been weaker than anticipated” ( “Meeting Minutes 2007-10-30”). And the end loomed nigh: “Currently finances are good through fiscal year 2008. EEBO-TCP is on target to complete 25,000 texts by the end of fiscal year 2008. Evans-TCP is likely, given current finances to complete around 6,000 texts. ECCO-TCP will complete around 1,300 texts. Therefore, the TCP, particularly in EEBO-TCP has been a success meeting most of its goals. Nonetheless, Evans-TCP and ECCO-TCP are still short of their goals of 6,000 and 10,000 texts respectively, and in fiscal year 2009, the TCP overall is facing a deficit of around $400,000 if it does not either reduce its current staff or bring in a large influx of money within the next six months” ( “Meeting Minutes 2007-10-30”). These are the last meeting minutes available online.  

“Begun in 2009, Phase II both shrank and expanded the scope of EEBO TCP.  Selection became more discriminating and focused more on English-language (and Welsh- and Gaelic-language) texts to the exclusion of French and Latin titles, and also set aside the serials (periodicals) as a fit project for another time. But within the constraints of English-language monographic titles, it aspired to something approaching comprehensive treatment: EEBO Phase II planned to convert each and every unique work in Early English Books Online (usually the first edition), or an estimated total of around 45,000 books on top of the 25,000 completed in Phase I. This was an ambitious, and always risky, goal. As it happened, enough institutions joined Phase II to fund the completion of about 40,000 titles, of which about 35,000 have been released to date, the remainder slowly working their way through the production pipeline. (TCP “EEBO”)

“As of 2019, the total number of books available in Phase II came to 34,963, with a further release of several thousand additional titles tentatively scheduled for later in the year.  Short of an infusion of new funding, or the adoption of a new production model, this should bring the active work of the TCP to at least an interim conclusion.” (TCP “EEBO”)  

Project Gutenberg began in 1971 with one individual, Michael Hart, who did not begin with a specific project vision in mind. From the beginning, then, Project Gutenberg was not goal-oriented in the same way as the other resources under discussion. By this I mean that Project Gutenberg orients itself toward goals of a fundamentally different kind than the goals which structure other textual archives, not that it has no goal. Project Gutenberg is, in general, subject to being dismissed as unserious or lacking rigorous standards, but I argue that these dismissals come from a failure to recognize and respect the real goals, seriousness, and standards which drive the project. In the case of the project’s founding, that goal was not, as in the case of the other databases under discussion, to provide a particular kind of access to a particular kind of texts. Instead, the goal of Project Gutenberg was born from a moment of happenstance and nepotism by which Hart, a student at the time, was donated $100,000,000 of computer time on the Xerox Sigma V mainframe at the Materials Research Lab at the University of Illinois. This mainframe was one of the first fifteen nodes on the early ARPANet, the precursor to the modern internet. As Hart described it, he “decided there was nothing he could do, in the way of "normal computing," that would repay the huge value of the computer time he had been given ... so he had to create $100,000,000 worth of value in some other manner” (“History and Philosophy”). Rather presciently for 1971, Hart concluded that the greatest value computing would offer was the storage, searching, and retrieval of other materials. He therefore typed up and distributed the Declaration of Independence. This became the first text of what would eventually become Project Gutenberg. It might even be considered the first ebook (according to Lebert 2008). Project Gutenberg was certainly “the first information provider on the internet and is the oldest digital library” (Lebert).

“During the fist twenty years, Michael Hart himself keyed in the first hundred books, with the occasional help of others from time to time.” (Lebert)

“when we started, the files had to be very small as a normal 300 page book took one meg of space which no one in 1971 could be expected to have (in general). So doing the U.S. Declaration of Independence (only 5K) seemed the best place to start. This was followed by the Bill of Rights --- then the whole US Constitution, as space was getting large (at least by the standards of 1973). Then came the Bible, as individual books of the Bible were not that large, then Shakespeare (a play at a time), and then into general work in the areas of light and heavy literature and references.” (Hart “History and Philosophy”) “That edition of Shakespeare was never released, due to copyright changes. If Shakespeare's works belong to the public domain, the comments and notes may be copyrighted, depending on the publication date. But other editions belonging to the public domain were posted a few years later.” (Lebert)

“As of 1987 he had typed in a total of 313 books in this fashion. Then, through being involved in the University of Illinois PC User Group and with assistance from Mark Zinzow, a programmer at the school, Hart was able to recruit volunteers and set up an infrastructure of mirror sites and mailing lists for the project. With this the project was able to grow much more rapidly.” (History-Computer)

“When the internet became popular, in the mid-1990s, the project got a boost and an international dimension. Michael still typed and scanned in books, but now coordinated the work of dozens and then hundreds of volunteers in many countries.” (Lebert)

“In August 1989, Project Gutenberg completed its 10th book, The King James Bible, that was first published in 1611, with the standard text dated 1769. In 1990, there were 250,000 internet users, and the standard was 360 K disks. In January 1991, Michael typed in Alice's Adventures in Wonderland, by Lewis Carroll (published in 1865). In July 1991, he typed in Peter Pan, by James M. Barrie (published in 1904).” (Lebert)

1989 is also when Project Gutenberg began to make use of OCR to generate base texts which were then proofread, rather than having the text typed from scratch. (Paywalled WSJ article)

“By the time Project Gutenberg got famous, the standard was 360K disks, so we did books such as Alice in Wonderland or Peter Pan because they could fit on one disk. Now 1.44 is the standard disk and ZIP is the standard compression; the practical filesize is about three million characters, more than long enough for the average book.” (Hart “History and Philosophy”)

“Project Gutenberg gradually got into its stride, with the digitization of one book per month in 1991, two books per month in 1992, four books per month in 1993 and eight books per month in 1994. In January 1994, Project Gutenberg celebrated its 100th book by releasing The Complete Works of William Shakespeare.” (Lebert)

1994, Italian volunteer Pietro Di Miceli developed and administered the first Project Gutenberg website and started the development of the Project online Catalog. (“Credits”)

”The number of electronic books rose from 1,000 (in August 1997) to 5,000 (in April 2002), 10,000 (in October 2003), 15,000 (in January 2005), 20,000 (in December 2006) and 25,000 (in April 2008). ... The steady growth went on, with an average of 8 books per month in 1994, 16 books per month in 1995, and 32 books per month in 1996.” (Lebert) “If 32 years were necessary to digitize the first 10,000 books, between July 1971 and October 2003, 3 years and 2 months were necessary to digitize the following 10,000 books, between October 2003 and December 2006.” (Lebert) “In the first 11 weeks of 2004, Project Gutenberg added 313 new e-books. It took from 1971 to 1997 to produce the first 313 e-books---that's 11 weeks compared to about 26 years.” (Hane)

The books doubled again between 2006 and 2011, from 20,000 (2006) to 40,000 (2011). (Hosch)

The current collection is 60,000 ebooks. This falls short of Hart’s bombastic declaration in 2004 that “We want to grow the collection to 1 million free e-books and distribute them to 1 billion people for a total of 1 quadrillion e-books to be given away by the end of the year 2015.” (Hane)  

“A fast growth thanks to Distributed Proofreaders, a website launched in October 2000 by Charles Franks to share the proofreading of books between many volunteers. Volunteers choose one of the books listed on the site and proofread a given page. They don't have any quota to fulfill, but it is recommended they do a page per day if possible. It doesn't seem much, but with hundreds of volunteers it really adds up.” (Lebert)

“In 2002 Distributed Proofreaders became part of Project Gutenberg.” (Hosch) “By 2009 roughly half of all Project Gutenberg books had been handled by using Distributed Proofreaders.” (Hosch)

“As of 2018, the 36,000+ DP-contributed books comprised almost two-thirds of the nearly 60,000 books in Project Gutenberg.” (Wikipedia)  

Because the texts are available without restrictions, a number of \[spin-off\] websites exist, some of which extend PG’s mission and some of which are slightly exploitative. On the most purely beneficial side are website “mirrors,” which duplicate the contents of the website in order to distribute the costs of hosting and to make sure that the texts remain accessible even if the “primary” Project Gutenberg website goes down. Also in keeping with Project Gutenberg’s core mission are projects which take the very plain text of PG works and make them more appealing or accessible in different formats. LibriVox, for example, is a similarly volunteer-driven effort which produces audiobooks of public domain works. Many organizations produce polished ebooks of PG works, and offer them for sale or for free in individual ebook reader libraries such as the Apple iBooks store or the Kindle store.

“In 2004 Project Gutenberg Europe and Distributed Proofreaders Europe were formed to facilitate the process of adding more non-English works.” (Hosch)  

All directly quoted from “Partners, Affiliates and Resources”







2	Sister Projects

2.1	Project Gutenberg of Australia

2.2	Project Gutenberg of Canada

2.3	Projekt Gutenberg DE

2.4	Project Gutenberg Europe

2.5	Project Gutenberg Self Publishing Portal

2.6	Projekt Runeberg

2.7	ReadingRoo.ms



3	Affiliates

3.1	ClassicalArchives.com

3.2	The Internet Archive

3.3	Librivox.org

3.4	LiteralSystems

3.5	ManyBooks.net

3.6	The Online Books Page

3.7	Outernet

3.8	RocketReader.com

3.9	Wattpad



4	Links to locations that provide software, tools, or Project Gutenberg eBooks in different formats

4.1	Andrew Sly's List of Canadiana in Project Gutenberg

4.2	Breeno.org

4.3	The Early Canadiana Online Project

4.4	GutenMark

4.5	Mobipocket

4.6	MobileRead

4.7	Noveltrove

4.8	thefifthimperium.com

4.9	Wikibooks  

“PG has been giving away CDs and DVDs; a volunteer mails them out for free on request.” (Hane)



Wikipedia: ”In August 2003, Project Gutenberg created a CD containing approximately 600 of the "best" e-books from the collection. The CD is available for download as an ISO image. When users are unable to download the CD, they can request to have a copy sent to them, free of charge.

In December 2003, a DVD was created containing nearly 10,000 items. At the time, this represented almost the entire collection. In early 2004, the DVD also became available by mail.

In July 2007, a new edition of the DVD was released containing over 17,000 books, and in April 2010, a dual-layer DVD was released, containing nearly 30,000 items.

The majority of the DVDs, and all of the CDs mailed by the project, were recorded on recordable media by volunteers. However, the new dual layer DVDs were manufactured, as it proved more economical than having volunteers burn them. As of October 2010, the project has mailed approximately 40,000 discs. As of 2017, the delivery of free CDs has been discontinued, though the ISO image is still available for download.”

Full info at “The CD and DVD Project”  

In 2000, Distributed Proofreaders was founded. Also “In 2000, a non-profit corporation, the Project Gutenberg Literary Archive Foundation, Inc. was chartered in Mississippi, United States to handle the project's legal needs. Donations to it are tax-deductible. Long-time Project Gutenberg volunteer Gregory Newby became the foundation's first CEO.” (Wikipedia)



At some point Carnegie Mellon University agreed to administer the project’s finances.

It’s currently hosted by ibiblio at UNC Chapel Hill.



Current partners: “- Distributed Proofreaders. DP allows people to share in the tasks of proofreading, verifying and formatting eBooks for Project Gutenberg.

\- iBiblio, the public's library. IBiblio is our main eBook distribution site, holds our Web pages, and offers a variety of supporting services.

\- Project Gutenberg Consortia Center: Project Gutenberg Consortia Center (PGCC). Collections of collections, with numerous languages and formats. Sponsored by the World eBook Library. Host of the self.gutenberg.org self-publishing portal.” (“Partners, Affiliates and Resources”)  

In 2004, there was some discussion of spinning off a PGII with the World eBook Library. “The co-founders say that PG 2 came from the need to include existing e-book collections, such as those found in schools, universities, and professional and religious organizations. Many such e-books do not fall into the acceptable criteria of Project Gutenberg. "For those books, PG 2 was created to find a home. PG 2 is a consortium of collections. Our vision is to create an additional portal where a broader variety of intellectual objects may be accessed.”” (Hane)

Did this go anywhere?



It also brought to light the fact that Hart himself had the trademark for PG, rather than the nonprofit. When did that change?  

“...one can certainly argue that the project is as old as Google itself. In 1996, Google co-founders Sergey Brin and Larry Page were graduate computer science students working on a research project supported by the Stanford Digital Library Technologies Project. Their goal was to make digital libraries work, and their big idea was as follows: in a future world in which vast collections of books are digitized, people would use a “web crawler” to index the books’ content and analyze the connections between them, determining any given book’s relevance and usefulness by tracking the number and quality of citations from other books. The crawler they wound up building was called BackRub, and it was this modern twist on traditional citation analysis that inspired Google’s PageRank algorithms -- the core search technology that makes Google, well, Google.” (“Google Books History”)



2002: “A small group of Googlers officially launches the secret “books” project. They begin talking to experts about the challenges ahead, starting with a simple but crucial question: how long would it take to digitally scan every book in the world? It turns out, oddly enough, that no one knows. In typical Google fashion, Larry Page decides to experiment on his own. In the office one day, he and Marissa Mayer, one of our first product managers, use a metronome to keep rhythm as they methodically turn the pages of a 300-page volume. It takes a full 40 minutes to reach the end. Inspired by the extraordinary digitization projects underway all around the world -- the Library of Congress’s American Memory project, Project Gutenberg, the Million Book Project and the Universal Library, to name only a few -- the team embarks on a series of site visits to learn about how they work. As part of this fact-finding mission, Larry Page reaches out to the University of Michigan, his alma mater and a pioneer in library digitization efforts including JSTOR and Making of America. When he learns that the current estimate for scanning the university library’s seven million volumes is 1,000 years, he tells university president Mary Sue Coleman he believes Google can help make it happen in six.” (“Google Books History,” 2016)



2003: The team works to develop a high-speed scanning process --- “A team member travels to a charity book fair in Phoenix, Arizona, to acquire books for testing non-destructive scanning techniques. After countless rounds of experimentation, the team develops a scanning method that’s much gentler than current common high-speed processes. ... At the same time, the team’s software engineers make progress toward resolving the tricky technical issues they encounter processing information from books that contain odd type sizes, unusual fonts or other unexpected peculiarities -- in 430 different languages.” (“Google Books History,” 2016)



2004: formal partnership with Bodleian library “to digitize the library’s incomparable collection of more than one million 19th-century public domain books within three years.” (“Google Books History,” 2016)

“In October, Larry and Sergey announce “Google Print” at the Frankfurt Book Fair in Germany. The first publishers to join the program: Blackwell, Cambridge University Press, the University of Chicago Press, Houghton Mifflin, Hyperion, McGraw-Hill, Oxford University Press, Pearson, Penguin, Perseus, Princeton University Press, Springer, Taylor & Francis, Thomson Delmar and Warner Books. In December, we announce the beginning of the “Google Print” Library Project, made possible by partnerships with Harvard, the University of Michigan, the New York Public Library, Oxford and Stanford. The combined collections at these extraordinary libraries are estimated to exceed 15 million volumes.” (“Google Books History,” 2016)



“Every weekday, semi trucks full of books would pull up at designated Google scanning centers. ... The books were unloaded from the trucks onto the kind of carts you find in libraries and wheeled up to human operators sitting at one of a few dozen brightly lit scanning stations, arranged in rows about six to eight feet apart. ... Each one could digitize books at a rate of 1,000 pages per hour. The book would lie in a specially designed motorized cradle that would adjust to the spine, locking it in place. Above, there was an array of lights and at least $1,000 worth of optics, including four cameras, two pointed at each half of the book, and a range-finding LIDAR that overlaid a three-dimensional laser grid on the book’s surface to capture the curvature of the paper. ... What made the system so efficient is that it left so much of the work to software. Rather than make sure that each page was aligned perfectly, and flattened, before taking a photo, which was a major source of delays in traditional book-scanning systems, cruder images of curved pages were fed to de-warping algorithms, which used the LIDAR data along with some clever mathematics to artificially bend the text back into straight lines. At its peak, the project involved about 50 full-time software engineers.” (Somers)



2005: “In keeping with our mission to organize the world’s information and make it universally accessible and useful, we donate $3 million to the Library of Congress to help build the World Digital Library, which will provide online access to a collection of rare and unique items from all around the world. We also extend our pilot scanning program with the Library, which includes digitizing works of historical value from the Library of Congress Law Library. Google renames “Google Print” Google Books, which more accurately reflects how people use it. The team also responds to the controversy over the Library Project by engaging in public debate about its underlying principles.” (“Google Books History,” 2016)

Public debate: <https://web.archive.org/web/20160204071159/https://googleblog.blogspot.com/2005/10/point-of-google-print.html>



2006: “We launch a series of product enhancements to make Book Search more useful and easier to use. First, we expand access to the public domain works we’ve scanned by adding a download a PDF button to all out-of-copyright books. A few months later, we release a new browsing interface that makes it easier to browse and navigate Book Search. The new interface is also accompanied by new About this Book pages which use Google algorithms to populate pages with rich related content on a book -- initially, related books, selected pages and references from scholarly works. In the fall, four new libraries join the Library Project: the University of California, University Complutense of Madrid, the University of Wisconsin- Madison and the University of Virginia.” (“Google Books History,” 2016)



2007: “Using the new UI as a launching point, we experiment with new ways for people to interact with books.

Places in this Book: A mashup with Maps lets people browse books by locations mentioned in the text (later, we release an experimental KML layer for Google Earth that does the reverse -- the user picks a location, and we map books to it).

Popular Passages: We create a new way to navigate between books, tracking the use of a single passage through a collection of books.

My Library: We help people harness the power of Google search within their own personal book collections. Users begin to curate and share their personal libraries, reviews and ratings with others.

New homepage (initially US only): We give people more jumping off points for exploring the books in our index.

... we add a “View plain text” link to all out-of-copyright books. T.V. Raman explains how this opens the book to adaptive technologies such as screen readers and Braille display” (“Google Books History,” 2016)

“By December, the Book Search interface is available in over 35 languages, from Japanese to Czech to Finnish.  Over 10,000 publishers and authors from 100+ countries are participating in the Book Search Partner Program.” (“Google Books History,” 2016)

“In May, the Cantonal and University Library of Lausanne, and Ghent University Library join the Book Search program, adding a substantial amount of books in French, German, Flemish, Latin and other languages, and bringing the total number of European libraries partners to six. ...  Over 10,000 publishers and authors from 100+ countries are participating in the Book Search Partner Program.  The Library Project expands to 28 partners, including seven international library partners: Oxford University (UK), University of Complutense of Madrid (Spain), the National Library of Catalonia (Spain), University Library of Lausanne (Switzerland), Ghent University (Belgium) and Keio University (Japan).”(“Google Books History,” 2016)



2008, the year when Google 



The “Google Books History” page ends with 2007, with the final words of “As we look to the year ahead, we continue to develop our technology and expand our partnerships with publishers and libraries all around the world. Stay tuned...” but the page spent more than a decade with no further updates. Some time between August 2019 and March 2020, the page was edited so that it no longer had a year-by-year breakdown of milestones. Now, after briefly telling the anecdote about the 1996 origin of Google Books, the “history” says ”Fast forward to today:

After more than a decade of evolution, innovation and strong partnerships, Google Books has helped to make more than 40 million books discoverable, in more than 400 languages.

And we're not done -- not until all of the books in the world can be found by everyone, everywhere, at any time they need them.” (“Google Books History,” 2020)



2008, the year that Google stops being quite so proud of the rapid progress of Google Books, is also the year that they begin to face legal repercussions for their “scan first and ask questions later” approach to mass digitization. Somers has characterized this process as equivalent to the burning of the library of Alexandria: “When the most significant humanities project of our time was dismantled in court, the scholars, archivists, and librarians who’d had a hand in its undoing breathed a sigh of relief, for they believed, at the time, that they had narrowly averted disaster” (Somers).



Wikipedia: “May 2008: Microsoft tapered off and planned to end its scanning project, which had reached 750,000 books and 80 million journal articles.\[92\]

Wikipedia: “October 2008: A settlement was reached between the publishing industry and Google after two years of negotiation. Google agreed to compensate authors and publishers in exchange for the right to make millions of books available to the public.\[9\]\[93\]

Wikipedia: “November 2008: Google reached the 7 million book mark for items scanned by Google and by their publishing partners. 1 million were in full preview mode and 1 million were fully viewable and downloadable public domain works. About five million were out of print.\[18\]\[94\]\[95\]

Wikipedia: “December 2008: Google announced the inclusion of magazines in Google Books. Titles include New York Magazine, Ebony, and Popular Mechanics\[96\]\[97\]

Wikipedia: “February 2009: Google launched a mobile version of Google Book Search, allowing iPhone and Android phone users to read over 1.5 million public domain works in the US (and over 500,000 outside the US) using a mobile browser. Instead of page images, the plain text of the book is displayed.\[98\]

Wikipedia: “May 2009: At the annual BookExpo convention in New York, Google signaled its intent to introduce a program that would enable publishers to sell digital versions of their newest books direct to consumers through Google.\[99\]

Wikipedia: “December 2009: A French court shut down the scanning of copyrighted books published in France, saying this violated copyright laws. It was the first major legal loss for the scanning project.\[100\]

Wikipedia: “April 2010: Visual artists were not included in the previous lawsuit and settlement, are the plaintiff groups in another lawsuit, and say they intend to bring more than just Google Books under scrutiny. "The new class action," read the statement, "goes beyond Google's Library Project, and includes Google's other systematic and pervasive infringements of the rights of photographers, illustrators and other visual artists."\[101\]

Wikipedia: “May 2010: It was reported that Google would launch a digital book store called Google Editions.\[102\] It would compete with Amazon, Barnes & Noble, Apple and other electronic book retailers with its own e-book store. Unlike others, Google Editions would be completely online and would not require a specific device (such as kindle, Nook, or iPad).

Wikipedia: “June 2010: Google passed 12 million books scanned.\[12\]

Wikipedia: “August 2010: It was announced that Google intends to scan all known existing 129,864,880 books within a decade, amounting to over 4 billion digital pages and 2 trillion words in total.\[12\]

Wikipedia: “December 2010: Google eBooks (Google Editions) was launched in the US.\[103\]

Wikipedia: “December 2010: Google launched the Ngram Viewer, which collects and graphs data on word usage across its book collection.\[31\]

Wikipedia: “March 2011: A federal judge rejected the settlement reached between the publishing industry and Google.\[104\]



“At a time when the rest of Google was obsessed with making apps more “social”---Google Plus was released in 2011---Books was seen by those who worked on it as one of those projects from the old era, like Search itself, that made good on the company’s mission “to organize the world’s information and make it universally accessible and useful.” It was the first project that Google ever called a “moonshot.”” (Somers)



Wikipedia: “March 2012: Google passed 20 million books scanned.\[105\]\[106\]

Wikipedia: “March 2012: Google reached a settlement with publishers.\[107\]

Wikipedia: “November 2013: Ruling in Authors Guild v. Google, US District Judge Denny Chin sides with Google, citing fair use.\[109\] The authors said they would appeal.\[110\]

Wikipedia: “October 2015: The appeals court sided with Google, declaring that Google did not violate copyright law.\[111\] According to the New York Times, Google has scanned more than 25 million books.\[10\]

Wikipedia: “April 2016: The US Supreme Court declined to hear the Authors Guild's appeal, which means the lower court's decision stood, and Google would be allowed to scan library books and display snippets in search results without violating the law.\[112\]” (Wikipedia)



Wikipedia: “As of October 2015, the number of scanned book titles was over 25 million, but the scanning process has slowed down in American academic libraries.\[10\]\[11\] As of October 2019, Google celebrated 15 years of Google Books and provided the number of scanned books as more than 40 million titles.\[14\]”



Wikipedia: “Google has been quite secretive regarding its plans on the future of the Google Books project. Scanning operations had been slowing down since at least 2012, as confirmed by the librarians at several of Google's partner institutions. At University of Wisconsin, the speed had reduced to less than half of what it was in 2006. However, the librarians have said that the dwindling pace could be a natural result of maturation of the project -- initially stacks of books were entirely taken up for scanning whereas now Google only needed to consider the ones that have not been scanned already.\[49\] The company's own Google Books history page ends in 2007, and the Google Books blog was merged into the Google Search blog in 2012.\[113\] Despite winning the decade-long litigation in 2017, The Atlantic has said that Google has "all but shut down its scanning operation."\[20\] In April 2017, Wired reported that there were only a few Google employees working on the project, and new books were still being scanned, but at a significantly lower rate. It commented that the decade-long legal battle had caused Google to lose its ambition.\[113\]”



### 2.1.4.  Hathi timeline history ###



“The vast majority of those digitized books-around 95 percent, as of mid-2017- had originally been scanned as part of the Google Books project; the agreements that Google Books entered into with the libraries typically stipulated that Google had to provide the library with a digital copy of each book scanned from that library.” (Bauder)

“When Google partnered with university libraries to scan their collections, it had agreed to give them each a copy of the scanning data, and in 2008 the HathiTrust began organizing and sharing those files. (It had to fend off the Authors Guild in court, too.) HathiTrust has 125 member organizations and institutions who “believe that we can better stewardresearch and cultural heritage by working together than alone or by leaving it to an organization like Google,” says Mike Furlough, the trust’s director.” (Van Helden)

”HathiTrust was launched in 2008 by the 11 University of California libraries and the 12-university consortium known as the Committee on Institutional Cooperation (CIC), with key support provided by the University of Michigan and Indiana University.” (Karels) “As of today \[October 13, 2008\], HathiTrust contains more than 2 million volumes and approximately ¾ of a billion pages, about 16 percent of which are in the public domain. Public domain materials will be available for reading online. Materials protected by copyright, although not available for reading online, are given the full range of digital archiving services, thereby offering member libraries a reliable means to preserve their collections.” (HathiTrust, “Major Library Partners Launch HathiTrust Shared Digital Repository”)

In 2010, just two years later, ”HathiTrust is now jointly owned and operated by 52 institutions from the U.S. and Europe, all focused on a common goal -- to build an extraordinary digital library that preserves and provides access to the cultural record. The new members to HathiTrust include the Library of Congress, Stanford University, Arizona State University, Massachusetts Institute of Technology, and University of Madrid, HathiTrust’s first international partner.” (Karels)

”In October 2015, HathiTrust comprised over 13.7 million volumes, including 5.3 million of which were in the public domain in the United States.” (Wikipedia)



“HathiTrust, as a co-managed and co-funded collaborative of academic and research libraries, relies on its members to govern its programmatic, financial, and strategic directions. Membership established the governance model in 2012 and comprises the following entities: 

The Board of Governors

The Program Steering Committee

The governing bodies operate under the HathiTrust Bylaws. In addition to the formal governing bodies, multiple working, advisory, and task-based groups further enable member libraries to participate in and guide the direction of HathiTrust. As necessary, members vote on various policies or changes using the weighted voting system established in 2011.” (HathiTrust, “Governance”)



“The budget of HathiTrust is a separately maintained budget held within the University of Michigan budget system and managed by the Executive Director with oversight from the Executive Committee. Some additional financial components of the operation are represented by commitments in kind by participating members. The University Library’s financial procedures are subject to audits by the University of Michigan Office of University Audits.” (HathiTrust, “Governance”)



Wikipedia: “The partnership includes over 60 research libraries\[7\] across the United States, Canada, and Europe, and is based on a shared governance structure. Costs are shared by the participating libraries and library consortia.\[8\] The repository is administered by the University of Michigan.\[9\] The Executive Director of HathiTrust is Mike Furlough.\[10\] The HathiTrust Shared Print Program is a distributed collective collection whose participating libraries have committed to retaining almost 18 million monograph volumes for 25 years, representing three-quarters of HathiTrust digital book holdings.\[11\]”  

To review all of these events, \<$n#table:databases-timeline\> shows the milestones of all of these databases in chronological order.  

1918	Pollard first proposes a “short-title handlist”

1926	Pollard and Redgrave Short-Title Catalogue for 1476--1640

1938	Eugene B. Power founds University Microfilms

1945	Wing starts collecting his STC, 1641--1700

1951	Donald Wing’s catalogue for 1641--1700, first edition

1971	First text in what would be Project Gutenberg. Over the next twenty years, Michael Hart personally keyed the first hundred books.

1972	Beginning of second ed of Wing STC, 1641--1700

1976	Proposal for Eighteenth Century Short Title Catalogue, British Library and the American Society for Eighteenth Century Studies

1976	Second edition, vol 1, of Wing’s STC

1976	Beginning of second ed of Pollard & Redgrave STC, 1475-1640

1977	ESTC pilot begun at British Library, directed by Robin Alston

1979	ESTC: Libraries from USA, Germany, and Australia began contributing to ESTC

1980	ESTC database available via British Library BLAISE \[British LibraryAutomated Information SErvice\]

1981	Research Publications, Inc begins microfilming books

1981	ESTC database available via US Research Libraries Group RLIN \[Research Libraries Information Network\] system

1983	ESTC catalogue of BL holdings and indexes published in microform

1983	*Eighteenth Century Collection* microfilm produced by Research Publications, Inc

1985	ESTC online databases in RLIN and BLAISE upgraded to allow dynamic updates to a single shared file

1986	Second edition, vol 2, of Wing’s STC

1987	ESTC expanded scope to add all print prior to 1700, changing its name to the English Short Title Catalogue. Information from Wing and STC is added to ESTC.

1987	Michael Hart recruits first Project Gutenberg volunteers

1989	Project Gutenberg completes its tenth book, the King James Bible

1991	End of second edition of Pollard & Redgrave STC, 1475-1640

1991?	Exhaustive index to Wing’s STC --- after which Bibliographical Society no longer supported Wing

1992	ESTC expanded scope to add serials

1994	ESTC made pre-1700 records available

1994

1994	Project Gutenberg completes its 100^th^ book, the Complete Works of William Shakespeare ?? (Contradicts 1987 count)

1994	Project Gutenberg’s first website is developed by volunteer Pietro Di Miceli

“By the late 1990s, several thousand reels had been published in two series: ‘Early English Books, 1475--1640’ and ‘Early English Books, 1641--1700’.” (Gadd)

1997	Project Gutenberg publishes its 1000^th^ book, La Divina Commedia di Dante, in Italian

1998	ESTC second edition released on CD-ROM

1998	Conclusion of second ed of Wing STC

1998	Beginnings of EEBO: University Microfilms (now ProQuest) began to make available digitised copies of its microfilms across the Internet to subscribing institutions

199	9	ESTC assumed official responsibility for receiving new Wing STC data

1999	TCP began

2000	Project Gutenberg: Charles Franks launches Distributed Proofreaders

2003	ESTC third edition released on CD-ROM

2003	Project Gutenberg 600 “best” ebooks released on CD-ROM, followed by 10,000 item DVD

2003	Beginning of ECCO: Thomson Gale (now Gale Cengage Learning) made digital copies of Eighteenth Century Collection microfilms available to subscribers online

2004	Google Print is announced

2005	TCP begins encoding ECCO texts

2006	ESTC made available to search free online; ESTC begins transcribing full title and imprints

“As of 26 April 2007 the number of microfilm reels of The Eighteenth Century that had been released was 16,625; the total number of titles on these reels is 189,569 (information provided by Katri Russick, Thomson Gale, Australia and New Zealand, in a private email). This number increased to at least 17,828 microfilm reels - the number received and catalogued by Monash University - by 1 November 2010.” (Spedding 450)

2007	Project Gutenberg DVD released with 17,000 items

2008	Project Gutenberg publishes its 25,000^th^ book, English Book Collectors, by William Younger Fletcher

2008	HathiTrust founded, by 12-university Committee on Institutional Cooperation and 11-library University of California Libraries

2009	EEBO-TCP Phase I complete: produced 25,000 books; beginning of Phase II

2010	Project Gutenberg DVD released with 30,000 items

2011	40,000 books in Project Gutenberg

2015	EEBO-TCP Phase I books released to the general public

2017	Project Gutenberg discontinues free mailing of CDs and DVDs, though the files remain available for people to burn their own copies at home

2021	EEBO-TCP Phase II books released to the general public



Table 1: A chronological history of major events in the development of \[LIST THE DATABASES\].



  

“ProQuest is to be commended for its attitude to the wider scholarly community.27 EEBO is a commercial product but nonetheless there is an encouraging and genuine wish to engage with its users. This ranges from the active monitoring and rapid responding to queries submitted via its ‘Webmaster’ form to informal and formal consultations with students, scholars, and other users. EEBO representatives appear at -- and often sponsor -- academic events. Content is frequently corrected, updated, expanded and enhanced (such as the new ‘EEBO Introduction Series’); the searching mechanism continues to be improved; the project to produce full-text transcriptions (the Text Creation Partnership) is an academic venture, not a commercial one. Unlike Jackson’s microfilm photographer, lurking in his lair with his livid lights and chemical smells, the present providers of EEBO seem to be rather more interested in -- and responsive to -- contemporary scholarship.” (Gadd 688)  

“It is perhaps the inconsistency of the OCR readings, obvious from this rough assessment, that makes Gale bashful about the restricted files. The editors of the ESTC confess to their public resource being a construction site in a way that the proprietors of private, commercial websites like ECCO prefer not to. But few really mind these days, especially if improvements are seen to be ongoing. Gale should be more relaxed about the incompleteness of their work, though perhaps not quite so relaxed as they are with the Burney Collection.” (Bullard 756)  

“Viewing the field of eighteenth-century digital humanities as a single prospect, it is the contrast between publicly funded, open-access sites, and privately owned, subscription- access resources that is most striking. Each side of the divide has much to learn from the other. Publicly funded academic projects must acquire the pragmatism and ambitiousness of scale that commercial developers have always shown. Commercial developers must adapt themselves more generously to the principles of scholarly openness and accuracy. They might also imitate the inventiveness of the open sector, its adaptability to the demands raised by different kinds of primary media. Both sides recognize the desirability of making their resources interoperable across the divide, and the business of interconnectivity will preoccupy all kinds of digital humanist in the coming decade. Another set of players likely to step further forward in future years is the university presses.” (Bullard 756)  

Two questions in the FAQ, “Why would I buy something that is achievable only if others do the same?” and “Why would I buy something that is going to become freely available?” taken together imply the speculative and ambitious nature of the original project. In the official answers provided to these evidently frequently asked questions, there is a sense that the project posed a prisoner’s dilemma: every individual institution’s “best” move, from a game theory perspective, was to contribute nothing to the project and then reap the benefits of everyone else’s work, but if every institution did so, then no one would benefit. A close reading of both responses illuminates an interesting tension in values:

Why would I buy something that is achievable only if others do the same?

Mere calculation may have disinclined some libraries from joining. TCP partnership was always less a purchase than an (admittedly risky) investment, since all of Michigan’s projections for the TCP corpus depended on a certain optimistic assumptions about how many other institutions would join.  Some libraries may have joined out of faith in Michigan’s track record, or because of a long-standing connection with the University or its staff. Some out of an idealistic belief in the collaborative model that TCP represented or in the public value of the product it promised. Some perhaps out of a cost-benefit risk estimate. For all the partner libraries, however, TCP membership was in effect a commitment to fellow libraries to share the burden and reward of this work. Partner libraries contributed to the cost of producing tens of thousands of painstakingly produced electronic editions of early English works. Each new library that joined made it possible for the project to key books that we otherwise would not, improving the corpus for everyone.

Why would I buy something that is going to become freely available?

This question too has no obvious answer that will please everyone, and indeed this question may have influenced some potential partners to refrain from joining. The structure of the TCP, with its provisions for exclusive access for a time, followed by public release, was something of a balancing act, designed to encourage membership by both those who were unwilling to wait ten years or so for access to the texts on behalf of their students and faculty, and those who believed in the creation of an unrestricted public resource and were willing on altruistic grounds to contribute to it.  Regardless of their motives for joining, the success of the EEBO-TCP depended on the support of partner institutions. The partnership fee directly funded the conversion of new books,  and greatly affected the rate at which the work was carried out. By joining up,  a library not only gained immediate access to the texts,  and not only contributed to making a larger, more comprehensive corpus for everyone, but also measurably affected the pace, and advanced the completion date, of the project--and thereby advanced the date at which  the texts would be released to the public.

The answers weakly attempt to provide the game-theory argument, but also carry the sense that the true answer, which they would like to give, has nothing to do with game theory and rationality, and everything to do with transcendent values of scholarship itself. In the minutes to the 2001 meeting of their executive board, they open with: “the project should determine more clearly if it is ‘a partnership or a product.’ Comment was offered that it is a ‘partnership to create a product’” (TCP Executive Board, 2001)



The TCP is thus an intervention into \[stuff about money.\] They describe the project as “a public-private partnership, led by libraries” (TCP, “About”) and emphasize the “librarian’s attitude toward content” which prioritizes the widest possible access and use. This “librarian’s attitude” is most evident in the (eventual) availability of all of the transcriptions in the public domain, despite the fact that the images they are based on remain privately restricted by the companies which own them. Their description of the “partnership,” however, continues to show signs of the strain in value systems when commercial and noncommercial goals are intertwined: “Through our partnership with private vendors, we had access to a huge trove of images from which to transcribe. In return, these companies were supplied with a full-text index to their images ---work which would have otherwise been difficult or expensive to produce.” In other words, through purchasing a service (access to images), the academic institution received that service. These institutions carried out an enormous feat of labour at their own expense, using the service they purchased. Then, “in return,” they provided the results of their labour for free to the company, for the company to then further profit from the improvements to their service. Most telling, here, is the word “otherwise.” The suggestion here is that, without the TCP, the companies themselves would not have been willing to undertake the encoding (so desired by the users of their service) because it would be difficult and expensive. However, the TCP certainly did not make the task any less difficult or expensive. Instead, academic institutions absorbed the difficulty and expense on those companies’ behalf. I do not say that they were wrong to do so: on the contrary, the “librarian’s attitude” mirrors my own attitude, and it is surely to everyone’s benefit for a wonderful thing to exist even if that wonderful thing is not profitable. Rather, I highlight this rhetorical moment in the TCP’s self description to suggest that \[it takes two to collaborate.\] One of the three key aims of the TCP identified on the homepage is to “collaborate with commercial providers, rather than constantly bargaining and competing with them” (TCP “Welcome”). However, the TCP seems instead to have simply come up with a *better* bargain, one which creatively offers scholarly labour as a bargaining chip.



The 2003 meeting minutes concluded with the observation that “considering where it was even two years ago, it is stunning to think that EEBO-TCP now produces texts comparable to any commercial product in a very cost-effective way. It was agreed that the TCP concept is on the cusp of revolutionary changes in research and teaching and that it will continue to grow and extend the foundation which the project has built.” (Meeting Minutes 2003-10-22.) (These minutes also talk a lot about spending, and about how to sell EEBO and TCP.)



In the 2004 minutes: “Since the TCP has evolved from one project in cooperation with ProQuest to now three projects in cooperation with three different commercial publishers, it is useful to consider how the Board might adapt to accommodate the changing situation. ... The representatives of the three companies began the discussion by highlighting the things they would feel uncomfortable revealing in front of their competitors. These included pricing, their contributions to the TCP, and general marketing strategy. It was agreed that the Board should attempt to structure its meetings so that all members could be present and, if at some point there was a need to divulge sensitive information, the Board could hold an executive session in which that could be discussed.”  

“Members of the Board also felt that TCP should complete an equity review of all of its employees to see what salaries are being offered from other universities and whether TCP’s salaries are in-line with what is being offered elsewhere. It was suggested that TCP might want to do a 10% raise across the board for all reviewing staff that remain with the project for five years.” (TCP Executive Board, Meeting Minutes 2005-10-20) In the next year, “Mark also reported on one item from the previous Board meeting, salary reviews. Maria and Shawn investigated the possibilities of raising salaries for TCP reviewers within the University of Michigan, and compared salaries at Michigan and Oxford. It was found that the salaries of the reviewers were comparable with expectations. Paul also reported that the reviewers who left had reasons other than salary (family issues, better opportunities within their field of study, etc.). Therefore, it was decided not to raise all reviewers salaries at once, but to continue doing merit increases that the University of Michigan does every year.” (TCP Executive Board, “Meeting Minutes 2006-09-16.”)  

Wikipedia: “In September 2011, the Authors Guild sued HathiTrust (Authors Guild, Inc. v. HathiTrust), alleging massive copyright violation.\[12\] A federal court ruled against the Authors Guild in October 2012, finding that HathiTrust's use of books scanned by Google was fair use under US law.\[13\] The court's opinion relied on the transformativeness doctrine of federal copyright law, holding that the Trust had transformed the copyrighted works without infringing on the copyright holders' rights. That decision was largely affirmed by the Second Circuit on June 10, 2014, which found that providing search and accessibility for the visually impaired were grounds to consider the service transformative and fair use, and remanded to the lower court to reconsider whether the plaintiffs had standing to sue regarding HathiTrust's library preservation copies.\[14\]”

See: <https://www.publishersweekly.com/pw/by-topic/digital/copyright/article/54321-in-hathitrust-ruling-judge-says-google-scanning-is-fair-use.html>

See: <https://www.hathitrust.org/authors_guild_google>



The current copyright policy states that "Many works in our collection are protected by copyright law, so we cannot ordinarily publicly display large portions of those protected works unless we have permission from the copyright holder. Where we have the right to show page images of works, we will make every effort to do so. We are currently displaying works that are in the public domain (such as US works published before 1925), uncopyrightable works (such as works of the US government), or works where we have permission from the copyright holder. If we cannot determine the copyright or permission status of a work, we restrict access to that work until we can establish its status.” (HathiTrust, “Help - Copyright”)



  

In 2005 the TCP was thinking about themselves as competing with Google: “John Price-Wilkin, Associate University Librarian for Library Information Technology & Technical and Access Services, was a guest of the Board to talk about issues relating to the Google initiative and TCP’s role in promoting “enhanced” product to the library and academic community. TCP does have an important role in noting that OCR text, though good for many things, cannot serve all purposes scholars need, and TCP should continue to argue for structured electronic text for at least a portion of the collection.” (TCP Executive Board, “Meeting Minutes 2005-10-20.”)



”When the library at Alexandria burned it was said to be an “international catastrophe.” When the most significant humanities project of our time was dismantled in court, the scholars, archivists, and librarians who’d had a hand in its undoing breathed a sigh of relief, for they believed, at the time, that they had narrowly averted disaster.” (Somers)



“He offered the library a deal: You let us borrow all your books, he said, and we’ll scan them for you. You’ll end up with a digital copy of every volume in your collection, and Google will end up with access to one of the great untapped troves of data left in the world. Brin put Google’s lust for library books this way: “You have thousands of years of human knowledge, and probably the highest-quality knowledge is captured in books.” What if you could feed all the knowledge that’s locked up on paper to a search engine?

By 2004, Google had started scanning. In just over a decade, after making deals with Michigan, Harvard, Stanford, Oxford, the New York Public Library, and dozens of other library systems, the company, outpacing Page’s prediction, had scanned about 25 million books. It cost them an estimated $400 million. It was a feat not just of technology but of logistics.” (Somers)



“The human operator would turn pages by hand---no machine could be as quick and gentle” (Somers)



“It was the first project that Google ever called a “moonshot.” Before the self-driving car and Project Loon---their effort to deliver Internet to Africa via high-altitude balloons---it was the idea of digitizing books that struck the outside world as a wide-eyed dream. Even some Googlers themselves thought of the project as a boondoggle. “There were certainly lots of folks at Google that while we were doing Google Book Search were like, Why are we spending all this money on this project?,” Clancy said to me. “Once Google started being a little more conscious about how it was spending money, it was like, wait, you have $40 million a year, $50 million a year on the cost of scanning? It’s gonna cost us $300 to $400 million before we’re done? What are you thinking? But Larry and Sergey were big supporters.”” (Somers)



“Out-of-print books, almost by definition, were commercial dead weight. If Google, through mass digitization, could make a new market for them, that would be a real victory for authors and publishers. “We realized there was an opportunity to do something extraordinary for readers and academics in this country,” Richard Sarnoff, who was then Chairman of the American Association of Publishers, said at the time. “We realized that we could light up the out-of-print backlist of this industry for two things: discovery and consumption.” But once you had that goal in mind, the lawsuit itself---which was about whether Google could keep scanning and displaying snippets---began to seem small time. Suppose the Authors Guild won: they were unlikely to recoup anything more than the statutory minimum in damages; and what good would it do to stop Google from providing snippets of old books? If anything those snippets might drive demand. And suppose Google won: Authors and publishers would get nothing, and all readers would get for out-of-print books would be snippets---not access to full texts. The plaintiffs, in other words, had gotten themselves into a pretty unusual situation. They didn’t want to lose their own lawsuit---but they didn’t want to win it either.” (Somers)

But the ambitious class-action settlement was nixed by the Department of Justice, so Google wouldn’t have a monopoly. “No one is quite sure why the DOJ decided to take a stand instead of remaining neutral. Dan Clancy, the Google engineering lead on the project who helped design the settlement, thinks that it was a particular brand of objector---not Google’s competitors but “sympathetic entities” you’d think would be in favor of it, like library enthusiasts, academic authors, and so on---that ultimately flipped the DOJ. “I don’t know how the settlement would have transpired if those naysayers hadn’t been so vocal,” he told me. “It’s not clear to me that if the libraries and the Bob Darntons and the Pam Samuelsons of the world hadn’t been so active that the Justice Department ever would have become involved, because it just would have been Amazon and Microsoft bitching about Google. Which is like yeah, tell me something new.”” (Somers)

““This is not important enough for the Congress to somehow adjust copyright law,” Clancy said. “It’s not going to get anyone elected. It’s not going to create a whole bunch of jobs.” It’s no coincidence that a class action against Google turned out to be perhaps the only plausible venue for this kind of reform: Google was the only one with the initiative, and the money, to make it happen. “If you want to look at this in a raw way,” Allan Adler, in-house counsel for the publishers, said to me, “a deep pocketed, private corporate actor was going to foot the bill for something that everyone wanted to see.” Google poured resources into the project, not just to scan the books but to dig up and digitize old copyright records, to negotiate with authors and publishers, to foot the bill for a Books Rights Registry. Years later, the Copyright Office has gotten nowhere with a proposal that re-treads much the same ground, but whose every component would have to be funded with Congressional appropriations.”(Somers)

  

The Project Gutenberg mission has always been focused on freely giving away things which were acknowledges to have financial value. After all, Hart’s goal was to “pay back” the $100,000,000 he had been given in computing time --- giving away ebooks would generate and give away this financial value. But even in seeking to “pay back” there is an impulse behind Project Gutenberg which exceeds \[capitalism\]. The explanatory documents of the project, when it became a major volunteer undertaking in the 1990s, read like a manifesto.

“Encourage the Creation and Distribution of eBooks

Help Break Down the Bars of Ignorance and Illiteracy

Give As Many eBooks to As Many People As Possible” (History-Computer)



“As Barbara Quint, editor of Searcher, said to me after reading some Gutenberg texts: "It struck me how noble, how wonderful, how great-spirited the people who made this all possible were. The time, the tedium, the labor it would take to render a book digital from a print copy." Further, she noted, "It's all a reminder of how much of what is the best on the Web and the Net comes from the kindness of strangers.”” (Hane)  

are they really making money or do they just think they are  

“Proprietary mass-digitized collections such as Google Books, Early English Books Online, and The British Newspaper Archive (owned by Google, ProQuest, and findmypast, respectively) are increasingly used in humanities research. But their scope and scale---let alone the histories of transmission that produce them---can be very difficult to discern; indeed, **the commercial imperatives of these enterprises arguably depend on them presenting these collections as comprehensive**.” (Bode 47)



## Smith in databases ##  

For the purposes of this chapter, I examine Smith’s works which fall outside this dissertation’s decade of interest. As Table 1 shows, Smith’s publishing career began in 1784 and continued until her death in 1806; when I refer to Smith’s “full” output, I consider all 47 editions of her works published in her lifetime or in the year immediately following her death. Her 1790s output (that is, the editions published 1789-99) consists of 30 of those editions.  I have slightly expanded my chronological focus in part because some of the most interesting exclusions occur earlier and later in Smith’s publishing career, such as the first edition of her immensely influential Elegiac Sonnets (1784), which is listed in the ESTC but not available in facsimile anywhere, or the publications in the last years of her life, which are excluded from the chronological focus of most resources but can still appear in HathiTrust. Of particular interest is the fact that *Beachy Head*, which is now one of Smith’s most frequently anthologized and taught poems, does not appear in a single digital database. None of these inclusions or exclusions represent an agenda against (or for) Smith, or indeed an interpretive choice at all, but they nonetheless shape the disciplinary infrastructure.  

![][CSmith-in-ESTC-ECCO-TCP-Hathi-table]

Table 1: All editions of Charlotte Smith’s works published in England during her lifetime or in the year immediately following her death, and their inclusion in the ESTC, ECCO, ECCO-TCP, and HathiTrust databases.  

Figure 1 shows how Smith’s presence in four major databases has the effect of winnowing down her full output arbitrarily. Even the largest collection, the 42 editions included in the ESTC, is not comprehensive: since the ESTC does not include any works published after 1800, it excludes volumes 4 and 5 of *Letters of a Solitary Wanderer* (1802), three works for children (*Conversations, Introducing Poetry*, 1804; *History of England*, 1806; and *Natural History of Birds*, 1807), and the posthumous publication that now forms a major part of Smith’s reputation as a poet, *Beachy Head* (1807). ECCO lacks these five editions for the same reason, and is also missing five others: the first and ninth editions of *Elegiac Sonnets* (1784 and 1800), the second edition of *The Banished Man* (1795), the first edition of *Minor Morals* (1798), and the second edition of *Rambles Farther* (1800[ Why these five?]).

HathiTrust contains 18 of Smith’s 47 editions, though these are not a simple subset of the ESTC and ECCO. Unlike the ESTC and ECCO, HathiTrust contains volumes 4 and 5 of *Letters of a Solitary Wanderer* (1802)[^cf24]. This is the only post-1800 work which appears in HathiTrust, however--- the others are also missing, including the important volume *Beachy Head* (1807). There is one work included in HathiTrust but not in ECCO, the second edition of *The Banished Man* (1795). Whereas ECCO does not include works unless there is a complete copy available, HathiTrust provides scans of volumes 2, 3, and 4, and simply implies through their numbering that there is a missing first volume --- perhaps in the optimism that a volume 1 will appear from another library’s holdings, to complete the set later.[^cf25] The remaining HathiTrust included titles appear in both the ESTC and ECCO, and a further 21 titles appear as facsimiles in ECCO but not in HathiTrust. At first blush it is somewhat surprising that HathiTrust has failed to include works which are, demonstrably, in known locations at institutional libraries, and in physically sound condition to be scanned--- but the scans making up HathiTrust bear no relation to the scans in ECCO. *The Young Philosopher* (1798), for example, appears in ECCO sourced from a British Library copy, but the HathiTrust images are “Google-digitized” from the New York Public Library. Google’s rapacious book-scanning, evidently, was not as thorough as ECCO’s sustained scholarly project.

The smallest subset of all of these texts is the ECCO-TCP holding of just two titles: the second edition of *Celestina* (1791), and the first edition of *The Emigrants* (1793). Both titles appear in all larger databases, including HathiTrust (though, as I will discuss, they arrive in HathiTrust from a different source). *The Emigrants* is included in ECCO-TCP as one file, based on the ECCO facsimile of an original from the Huntington Library. *Celestina* is included as four files, one for each of four volumes, based on the ECCO facsimile of an original from the British Library. Both works were first reproduced in the microfilm version produced 1982-2002 in by Research Publications,[^cf26] then digitized in 2003 (released on ECCO in June 2004), and finally published as TEI XML files in January 2007. The current files have been kept up to date with changes in TEI standards, and were created by converting TCP files to TEI P5 using tcp2tei.xsl. The bibliographic metadata for these works is the same between ESTC, ECCO, and ECCO-TCP records. In HathiTrust, however, the source text for *The Emigrants* is a University of California Library copy, rather than the British Library, scanned by Google Books, and presented with substantially less detailed bibliographic information. The ESTC, ECCO, and ECCO-TCP records for The Emigrants all provide the same physical description “ix,\[3\],68\[i.e. 60\]p. ; 4⁰” with the same note“\[n\]umbers 9-16 omitted in pagination; text is continuous.” HathiTrust, in contrast, gives the physical description “ix, 68 p. ; 26 cm,” which is both more and less information: a quarto volume could be a range of sizes, so HathiTrust provides new detail by giving a measurement in centimetres, but the data on page numbers is now misleading. Consulting the HathiTrust facsimile shows that it, too, omits the page numbers 9-16, going directly from page 8 to page 17 without a break in the poem. HathiTrust also omits information on the three unnumbered pages between the preface and the poem. Evidently, a human did consult the book, to identify a nine-page preface in roman numerals, and the page number on the last page, but they did not carry out a full collation.  

![][CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3]

Figure 1: An alluvial chart, showing the winnowing down of Smith’s works from database to database. Of the 47 editions printed in England between 1784 and 1807, 42 are included in the ESTC, and 5 do not appear in the ESTC because they were printed after 1800 and thus fall outside its purview. ECCO contains 37 of Smith’s 47 editions, all of which also appear in the ESTC. ECCO is missing the 5 editions not listed in the ESTC (since it, too, does not contain works past 1800), as well as another 5 works. HathiTrust contains 18 of Smith’s 47 editions, but unlike ECCO, these are not a simple subset of the ESTC. HathiTrust contains one of the 5 editions excluded from the ESTC, and one of the 5 editions included in ESTC but excluded from ECCO. The remaining 16 HathiTrust editions appear in both the ESTC and ECCO. ECCO-TCP includes only 2 of Smith’s 47 editions, both of which appear in every previous database. Graph generated using RAW Graphs (Mauri et al.).  

Only one of Charlotte Smith’s works is available in Project Gutenberg: *Emmeline, the Orphan of the Castle* (first published 1788).  

Searching the ESTC for records which both have “Toronto” in the library name and “Charlotte Turner” in the author name turns up two records: volume one of *Rural Walks* (1795) and *Minor Morals* (1798), both held at the Toronto public library. The Toronto Public Library catalogue has two distinct author identities for “Smith, Charlotte Turner, 1749-1806, author.” and for “Smith, Charlotte, 1749-1806,” and the special collections holdings only appear under the latter name (making them initially difficult to find). Under the “Smith, Charlotte” name, however, six titles printed during Smith’s appear: the two listed in ESTC, plus a complete two-volume copy of *Rural Walks* (1795), the first and second editions of *Rambles Farther* (1796 and 1800), and *Conversations Introducing Poetry* (1804). Of these, *Rural Walks* and both editions of *Rambles Farther* are listed in the ESTC but without records of the Toronto copies. All six titles are part of the Osborne Collection of Early Children's Books. \[This is interesting because it shows how scholarly disciplinary interpretations perpetuate themselves *infrastructurally*: as a Toronto-based scholar, the path is easier for me to study Smith-the-children’s-writer than other Smiths.\]  

“while ESTC may be based on two thousand public and private libraries worldwide, the Eighteenth Century microfilm series is based on books from only a tiny fraction of that number - almost certainly less than twenty libraries, and rarely anywhere other than the British Library, the Bodleian, Harvard, and the Hunt” (Spedding 440)



The “microfilm series is not a random - and therefore randomly representatve--- selection of items from ESTC. Texts have been selected for filming on the basis of criteria that are rarely mentioned, but which include ease of access for filming (initially, items at the British Library) and the desire to avoid duplication of texts. That is, by the desire to get the biggest bang for Gale’s buck.” (Spedding 441)



“There may also be commercial considerations at work. Alt not conducted a systematic search for items from the British L Case" (its collection of erotic material), it seems that little of tha the Eighteenth Century microfilm series, and the material that h has only been quite recently added.37 Consequently, much of this ing from ECCO. The reason for this may be that much Private Ca as late as 1989, not represented on ESTC,38 but it may also be be Private Case was microfilmed by Adam Matthew Publications in under the title Sex and Sexuality 1 640-1 940 . That is, the eighteent terial in the Private Case may have been withheld from the Eigh microfilm series (and consequently ECCO) to ensure the profitab Sexuality. Similar, and similarly hidden, criteria seem to affect o such as EEBO and Goo” (Spedding 441)



Gale proudly declares that “this collection contains every significant English-language and foreign-language title printed in the United Kingdom between the years 1701 and 1800.” (“Eighteenth Century Collections Online”) This claim is easily overturned with a single counterexample, such as the first edition of Charlotte Smith’s *Elegiac Sonnets*, a significant title which is absent.



“Full-text searching across all 26 million pages enables users to explore a vast range of books and directories, bibles, sheet music, sermons, advertisements, and works by both celebrated and lesser-known authors. Researchers will also find rare works from women writers of the eighteenth century, collections on the French Revolution, and numerous editions of the works of Shakespeare.” (Gale, “Part I”)  

What is *in* the TCP? Well, when active transcription was taking place, “users (especially those affiliated with partner libraries) were welcome to request works from EEBO that had not yet been keyed, and that their requests would go to the top of the queue” (TCP “FAQ”). So --- the TCP contains whatever individual works happened to interest particular scholars.



The TCP, unlike the ESTC and ECCO, intentionally avoids including multiple editions of a given work. This decision was a pragmatic one motivated by “limited funding” and a sense of scarcity: “Simply put, for every book that we chose to convert, a different book does not get converted: duplication, even partial duplication, has its costs” (TCP “FAQ”). Since the TCP never envisioned itself as a fully complete collection, the priority in textual selection “was always to capture as many different works and as great a variety of text as we could, usually focusing on the first edition of each work”(TCP “FAQ”). To a certain extent, this lack of duplication can be useful for text-mining: it places all texts on an equal playing field, rather than double-dipping on some works. However, they “have keyed additional editions where there is sufficient justification for doing so, and a user has made a case for it,” so the corpus cannot be assumed to contain *no* duplicated works (TCP “FAQ”).



“Selection of works to transcribe for EEBO Phase 1 was initially based on named authors mentioned in the New Cambridge Bibliography of English Literature.  Though this tended to bias selection a bit toward canonical, or at least attributed, works, anonymous works may also have been selected at this stage if their titles appeared in the bibliography. The New Cambridge Bibliography of English Literature was chosen as a guideline because it included foundational works as well as less canonical titles related to a wide variety of fields, not just literary studies. In any case, this initial reliance on the New Cambridge soon gave way to a series of deliberate attempts to cast a wider net, for example by selecting works exemplifying a particular theme (food, drugs, piracy, witchcraft), or fitting a particular format (broadsides, pamphlets, etc.)  The intention was to supplement methodical selection with more or less random selection based on arbitrary criteria in order to expand the generic diversity of the corpus. Requests for particular works by faculty at partner institutions were also taken into consideration and, if feasible, placed at the head of the queue. A user willing and able to make a case for a given work almost always prevailed over other considerations.” (TCP “EEBO”)



“Discussion then began on how to further develop the TCP project and insure that we can create 25,000 texts. In order to do this, the Board felt that it needed more information about the complete number of texts possible to include in the corpus, a dollar amount per title required to complete the project, a report on the total number of institutions, and the gap between EEBO subscribers and EEBO-TCP partners. Shawn Martin will work with Mary Sauer-Games to gather this information and report back to the Board. Some members thought that EEBO-TCP should go back to already existing partners and ask for a second round of funds to complete the project. In order to do this, Board members felt EEBO-TCP should look very carefully at what titles it is selecting and come up with summaries of the number of desirable titles to convert and initiate a strategy that could persuade libraries that it would be worth another years commitment to complete, for example, all of the Thomason tracts, or all of a particular genre or canonical category.” (TCP Executive Board, “Meeting Minutes 2005-10-20.”)



“Shawn Martin then discussed the selection process for all of the TCP projects. Though there are commonalities between them, selection runs differently for all 3 projects and each project runs fairly independently. Therefore, it becomes a question of how much should TCP coordinate collection between the three projects and how should TCP manage duplication. Scholars prefer that TCP duplicate titles between EEBO, Evans, and ECCO; librarians prefer to avoid duplication. The Board felt that it should receive a report of all duplicated materials, that TCP create an oversight group of librarians to coordinate the three projects, and where feasible TCP should try to minimize duplication.” (TCP Executive Board, “Meeting Minutes 2005-10-20.”)  





“There are three portions of the Project Gutenberg Library, basically be described as:

Light Literature; such as Alice in Wonderland, Through the Looking-Glass, Peter Pan, Aesop's Fables, etc.

Heavy Literature; such as the Bible or other religious documents, Shakespeare, Moby Dick, Paradise Lost, etc.

References; such as Roget's Thesaurus, almanacs, and a set of encyclopedia, dictionaries, etc.

The Light Literature Collection is designed to get persons to the computer in the first place, whether the person may be a pre-schooler or a great-grandparent. We love it when we hear about kids or grandparents taking each other to an etexts to Peter Pan when they come back from watching HOOK at the movies, or when they read Alice in Wonderland after seeing it on TV. We have also been told that nearly every Star Trek movie has quoted current Project Gutenberg etext releases (from Moby Dick in The Wrath of Khan; a Peter Pan quote finishing up the most recent, etc.) not to mention a reference to Through the Looking-Glass in JFK. This was a primary concern when we chose the books for our libraries.

We want people to be able to look up quotations they heard in conversation, movies, music, other books, easily with a library containing all these quotations in an easy to find etext format.” (Hart “History and Philosophy”)  

The founding logic of Project Gutenberg resonates strikingly with Bordieu’s call to “*universalize in reality the conditions of access*” (qtd in Guillory 340, emphasis original) to literature.

The first Project Gutenberg texts are almost a parody of important texts: The Declaration of Independence, The King James Bible. These are the texts assumed to be urgently desired by “99% of the general public” (Hart “History and Philosophy”). They are then followed, however, by a work which has rarely been central to the institutional hierarchies of cultural capital: Alice in Wonderland. As Hart describes his choices of what texts to transcribe next, he seems to be describing a version of what Guillory hoped for, “another kind of game” in which texts can compete for cultural capital, a game “with less dire consequences for the losers, an *aesthetic* game” (Guillory 340, emphasis original).



“Project Gutenberg selects etexts targeted a bit on the "bang for the buck" philosophy ... we choose etexts we hope extremely large portions of the audience will want and use frequently. We are constantly asked to prepare etext from out of print editions of esoteric materials, but this does not provide for usage by the audience we have targeted, 99% of the general public.” (Hart “History and Philosophy”)



## database models ##



#### 2.2.1.1.  edition & ideal copy ####



One reason that it can be informative to close-read the data structures of a resource like the ESTC is that a resource’s categories of knowledge are driven by the *uses* to which it expects that knowledge to be put. Examining the implicit assumptions that will make a given organization of knowledge seem logical, we can work backwards to the purpose of mission of the initial knowledge creation. Thus Tabor describes the data structure and the mission of the ESTC in a single statement: “ESTC’s most basic bibliographical function is to provide, for each edition, a description of the ideal copy, meaning the most complete and correct manifestation of that edition as the printer and publisher intended it” (369). Korshin further elaborates the use envisioned for this information: “the ESTC's cataloguing rules have been devised in such a way that a scholar anywhere in the world can tell, from the ESTC entry, whether the copy of the book in his or her library is the same or different from the one listed in ESTC” (211). Both “edition” and “ideal copy” are terms defined around the interests of a specialist audience of bibliographers, which bear inexact but important relationships to the formulation of an ESTC record.

An “edition” is a group of copies of a work which are understood to be interchangeable with each other (Tabor 369),[^cf27] though in practice different levels of granularity are applied in distinguishing between editions. The ESTC sometimes has separate entries for groups within an edition “when certain separately planned marketing units can be identified within the edition, such as reissues, imprint variants, and large versus regular-paper copies” (Tabor 369). Karian describes that “\[s\]ometimes the ESTC contains additional records if there are multiple *states* of an edition (a different state results from cancels or minor changes to the setting of type)” (289). Or, in “the later eighteenth century, when reprints from standing type became more common, ESTC cataloguers have occasionally granularized down to the level of individual impressions” (Tabor 369). As a result, Karian argues persuasively that ESTC records should not be treated as synonymous with “editions,” “issues,” or “titles,” since the same definitions of those boundaries may not be applied consistently. The specific question he poses is “What is the unit that the ESTC uses?” (289), and important question, to which the answer cannot really be “editions,” despite the best attempts of the ESTC bibliographers. Instead, he says “one should refer instead only to the ESTC record, a unit created by the ESTC and having no meaning outside the ESTC” (Karian 289).[^cf28]

The “ideal copy,” too, represents an interpretation. Because the ESTC is essentially a movel based on limited samples of an imagined lost prior whole --- “the most complete and correct manifestation of that edition as the printer and publisher intended it,” as Tabor termed it (369) --- a new sample can change the model. As Tabor describes, “\[a\]s additional reports of copies arrive, it may be that the ideal description must change in response. For instance, the existence of a half-title may only emerge on the evidence of the seventh copy reported. A half-title would then be added to the description of the ideal copy, and the six previously matched physical copies will receive notes recording that they are imperfect in this respect” (370). The ideal copy, like the database itself, thus represents a moving target.

  

So, how do these ideas of the edition and the ideal copy shape the data structures employed in the building of the ESTC? Consulting an individual ESTC record in the online database, as we can see in Figure 2, reveals a lot of information all pointing ‘outside’ of the ESTC itself. It begins with six details which will be present for every title: the “System Number” and “Citation Number” uniquely identifying the record; the author; the title; the publication information; and a physical description. It then displays any uncategorized “notes,” which in the case of *The Emigrants* (1793) consist of two additions to the physical description. \[Add other examples of “general notes”?\] The entry then points ‘outward’ to two “Surrogates”: the microfilm, and the electronic reproduction of the microfilm which is collected in ECCO. A very brief description is made of the work’s content --- its subject is “English poetry --- 18^th^ century” and its genre/form is “Poems” --- which is the only information provided about the *work* rather than the *book*.[ There’s a lot more to be examined re: these subject headings, especially if I do topic modelling for contents.



“Because the ESTC was formed from three different projects over many decades, the existence of subject headings varies. For books that are from the periods 1475-1640 (from the original Short-Title Catalogue created by Pollard and Redgrave) and 1641-1700 (from Donald Wing’s short-title catalogue), subject headings exist. But subject headings were not initially created for the eighteenth-century records in the ESTC, and so subject headings rarely occur for the items in ECCO.” (Karian, “Guide” 3)



How many of my records have subject headings? What is the ESTC’s ontology?] The remainder of the record is an extensive list of libraries which hold copies of the edition, divided into the three geographic regions of “British Isles,” “North America,” and “Other,” followed by a direct link to the ECCO copy referenced above.

This, however, is only how the ESTC *displays* its contents. Clicking another tab makes visible the MARC tags in which the data itself is stored. The MARC tags encode information at a slightly more refined level of detail. For example, the publication location in the standard view is listed as “Publisher/year” and displayed as the string “London : printed for T. Cadell, in the Strand, 1793.” A human can parse that string, but as the MARC version of the same information reveals, it is made up of three points of information that have been combined. The MARC data is listed as “260,” which is the MARC standard code for “Publication, Distribution, etc.” The line itself is displayed as “\|a London : \|b printed for T. Cadell, in the Strand, \|c 1793” --- indicating three separate pieces of information in the subfields “a - Place of publication, distribution, etc.”, “b - Name of publisher, distributor, etc.”, and “c - Date of publication, distribution, etc.” The separation of these points of information in the underlying MARC data is what allows the online database to conduct searches based on publisher, publication location, and date of publication. Even this is a reformatting of the underlying MARC code, which would read “##$a London :$b printed for T. Cadell, in the Strand,$c 1793” --- with the two “#” symbols at the beginning encoding that this is the first edition.[^cf29] It is, of course, only sensible for the ESTC to reformat its MARC code for display: MARC stands for MAchine Readable Catalogue, and machines and humans have very different needs as readers. However, what this exploration reveals is that \[???[ Is it that the categories of information are made less explicit as they are translated for humans, relying on the expert reader’s interpretive skill?]\].  

Figure 2: A screencap of the ESTC record for Charlotte Smith’s *The Emigrants* (1793).  

There are several different ways to search ESTC records. The “Search” button takes a user to the “Basic Search” function, from which there are also links to “Advanced Search,” “Browse,” and “Browse Libraries List” (which takes the user to the identical page as “Browse” but with “Library name” pre-selected as the index to browse). Once you have found a work of interest, however, several new forms fo searching become available, implied in the hyperlink formatting: almost any field in the entry can be clicked to reach other matching ESTC entries.



### 2.2.2.  ECCO model ###



An edition being “included” in ECCO looks different from its inclusion in the ESTC --- whereas the ESTC lists just one record for each multivolume work, ECCO lists each volume separately, with links to the other volumes available in the “full citation” for the volume.



“Such consideration for users is sadly rather less visible with EEBO’s eighteenth-century equivalent ECCO. Unlike EEBO, ECCO presents users with a single, cropped page. In so doing, it has taken the opportunity to remove every blank page that ever appeared in an eighteenth-century book (pace its claim to provide ‘digital images of every page of 150,000 books published during the 18th Century’).” (Gadd 688)



“Unlike EEBO, ECCO includes an underlying text-transcription of its entire collection, which users can search but cannot access in any other way. According to ECCO’s online guide, the full-text transcriptions are generated using computerised ‘optical character recognition’ of the digitised images of \[start page 689\] each page, with ‘proprietary software created by the vendor to improve OCR accuracy, including the ‘correction’ of old English f/s ligatures and other spelling and character variants’; in addition, there are elaborate quality control systems:

For every digitized page of data, eight specific items are sampled for accuracy and correctness. Each page is visually scanned for glaring errors or omissions. Every 20th page is read in its entirety. (‘About Eighteenth Century Collections Online’)

With this in mind, it comes as somewhat of a surprise to discover that, according to ECCO, the word ‘fuck’ or versions thereof appear over 28,000 times in print in the eighteenth century. Finally, unlike EEBO, feedback from users does not seem to be much valued: the technical support e-mail address provided on its help pages no longer seems to be valid.” (Gadd 688-9)



“In various different ways, ECCO is less open with its users than is usual in academia. It likes to tidy away noisy information. The microfilms on which it is based reproduce images of the openings of books, but ECCO chops each opening into two single-page images and dispenses with all the detail (the rulers placed against type, the blank pages, and the indicators of physical structure) that is so precious to bibliographers. Since 2009, ECCO documents have been supported by MARC descriptions, drawn (presumably) from the ESTC and supplemented by Gale’s own ‘Subject’ entries. Users can search by subject, and yet the MARC files are not directly accessible for checking by readers using ordinary institutional subscriptions. Similarly, there is still no accurate way to identify by class- or shelf-mark the actual copy of a book that ECCO reproduces. An indication of the source collection is always given, but the larger libraries that the original microfilm photographers tended to favor often keep multiple copies of editions, and it is seldom possible to discover which copy has been used without a personal visit to the archive. This omission has several consequences: readers are further distanced from experience of the original material object; local features (such as MS annotations on the original) are inadequately documented; and the widespread phenomenon of stop-press variants within editions of hand-press era books is forgotten. Most frustratingly of all, ECCO does not allow users access to the scanned optical character recognition (OCR) documents that its full-text searches run on.

This secretiveness inevitably arouses suspicions about the accuracy of the scans.” (Bullard 755)



A glowing 2004 review of ECCO in the “Database & Disc Reviews” section of Library Journal says “The Advanced Search is so powerful it gave me sensory overload.” (LaGuardia 124)



### 2.2.3.  TCP model ###



Although the ECCO-TCP now seems obviously built for text-mining distant reading, in fact it is largely organized around searching and consulting individual works.



“ECCO natively supports OCR-based full-text searching of this corpus. This is significant because it meant that unlike EEBO-TCP (which produced searchable text where there was previously none at all), ECCO-TCP could only hope to produce *more accurate* text (and more reusable text) than what was already available. The larger size of ECCO (because of the great increase in printing and greatly enhanced chances of survival of printed works in the 18th century) also made it a different proposition: nothing so ambitious as EEBO-TCP coverage was feasible for ECCO-TCP. ... Because of these greater challenges facing ECCO-TCP, it is perhaps better described as a proof of concept than as a completed project. With the support of more than 35 libraries, the TCP keyed, encoded, edited, and released 2,473 ECCO-TCP texts. A further tranche of 628 texts was keyed and encoded but never fully proofed or edited. The texts in this group remain useful for many purposes, however, and bring the total of ECCO-TCP texts to over 3,000.” (TCP “ECCO”)



### 2.2.4.  Hathi model ###



Wikipedia: “PageTurner is the web application on the HathiTrust website for viewing publications.\[17\] From PageTurner readers can navigate through a publication, download a PDF version of it, and view pages in different ways, such as one page at a time, scrolling, flipping, or thumbnail views.\[17\]\[18\]”



See: <https://www.hathitrust.org/technology>

See: <https://search-proquest-com.myaccess.library.utoronto.ca/results/58AF728D91BD440DPQ/false?accountid=14771>  

The structuring principle of Project Gutenberg is its missions to make books available for pleasure reading. I argue that its core concept, analogous to the “edition” in the ESTC, or the “book” in ECCO and HathiTrust, is the “story.” Many of the priorities of Project Gutenberg which seem incompatible with scholarly approaches to textual history are explained by thinking of Project Gutenberg as being structured around “stories” rather than “books.”



“At the end of 1993, Project Gutenberg's eTexts were organized into three main sections: a) "Light Literature", such as Alice's Adventures in Wonderland, Peter Pan or Aesop's Fables; b) "Heavy Literature", such as the Bible, Shakespeare's works or Moby Dick; c) "Reference Literature", such as Roget's Thesaurus, and a set of encyclopaedias and dictionaries. This organization in three sections was abandoned later for a more detailed classification.” (Lebert)  

The article ”Quantitative patterns of stylistic influence in the evolution of literature” uses Project Gutenberg --- do mathematicians not know what a good source of literature is, or do they know better than us?



Cite Hammond’s book re: comparing modernists to bestsellers --- he can’t always find bestsellers, it depends on whether bestsellers were enjoyed enough for someone to bother to type them up  

Google Books prioritizes low-quality information over *no* information. The algorithmic extraction of publication dates from title pages, for example, can never be perfect. But algorithms give their predictions with certainty estimates: if accuracy was a higher priority, Google Books could calibrate the algorithm to simply provide no answer when none of the possibilities cross a given certainty threshold.



Per <http://languagelog.ldc.upenn.edu/nll/?p=1701> , they actually OVERWRITE metadata provided by partners with their algorithmic information!! They could very easily *not*.



“At its peak, the project involved about 50 full-time software engineers. They developed optical character-recognition software for turning raw images into text; they wrote de-warping and color-correction and contrast-adjustment routines to make the images easier to process; they developed algorithms to detect illustrations and diagrams in books, to extract page numbers, to turn footnotes into real citations, and, per Brin and Page’s early research, to rank books by relevance. “Books are not part of a network,” Dan Clancy, who was the engineering director on the project during its heyday, has said. “There is a huge research challenge, to understand the relationship between books.”” (Somers)  

How accurate does OCR need to be? This depends on how the OCR will then be used.  

The existence of a carefully hand-corrected transcription of *The Emigrants* in ECCO-TCP provides an opportunity to check the reliability of the OCR in both ECCO and HathiTrust. I will proceed from the assumption that the ECCO-TCP files are 100% accurate, and that any differences between the OCR and ECCO-TCP represents an OCR error.[^cf30] Before beginning the experiment, my hypothesis was that both ECCO and HathiTrust would differ from each other in where and how they are inaccurate, but would have similar accuracy overall. I suspected that they were likely around 50% accurate, plus or minus 10% --- I wouldn’t be surprised if they were worse, but would be quite surprised if their accuracy was 80% or higher.[ What level of accuracy do people usually want for OCR research?] Acquiring the plaintext files from all three sources required some hunting for some hidden options and some workarounds; rendering them suitable for comparison required some modifications of each file, described more fully in Appendix B. Although Gale Digital Scholar Labs prominently provided an “OCR Confidence” of 95%, the first glance at the document was not very promising. To my surprise, Juxa calculated a relatively low “change index” for each text compared to the TCP witness: ECCO had a .16 change from base (i.e., 84% accuracy), and my normalized HathiTrust document had only a .09 change from base (i.e., 91% accuracy).[^cf31] This surprised me, and suggests that skepticism of OCR in eighteenth-century text mining may no longer be appropriate.  

To make these comparisons concrete, consider the first page of Smith’s dedication, as it is captured by OCR in ECCO and HathiTrust, and in the ECCO-TCP transcript:



TO WILLIAM COWPER, Es DEAR SIR, THERE is,- I hope, some propriety in my addrefing a Com- potion to you, which would,never perhaps have existed, had I not, amid the heavy prefure of many sorrows, derived infinite consolation from your Poetry, and some degree of animation and of confidencefrom your efieen. . 'he.following performance isfarfrom aspiring to be con- fidered as an imitation of your inimitable Poem, " THE " TASK;" I am perfeetly sensible, that it belongs not to a feeble andfemninine hand to draw the Bow of Ulyfes.,Theforce, clearness, and sublimity ofyour admirable Poem; the felicity, almost peculiar to your genius, of givingto the moJ familiar objegls dignity and eset, I could never hope to,a reach (ECCO)



T O WILLIAM com/PER, Ess. DEAR SIR, THERE is, I hope, ſome propriety in my addreſſing a Com- poſition to you, which would never perhaps have exiſted, had I not, amid the beavy preſſure of many ſorrows, derived infinite conſolation from your Poetry, and ſome degree of animation and of confidence from your ºfteem. The following performance is far from aſpiring to be con- ſidered as an imitation of your inimitable Poem, “ The “TAsk;” I am perfºy fººl, that it belongs not to a feeble and feminine band to draw the Bow of Ulyſſes. The force, clearneſ, andſublimity of your admirable Poem; the felicity, almoſt peculiar to your genius, of giving to the moſt familiar obječís dignity and effečf, I could never hope to 3. - Reach (HathiTrust)



TO WILLIAM COWPER, ESQ.

DEAR SIR,



THERE is, I hope, some propriety in my addressing a Com\|position

to you, which would never perhaps have existed, had

I not, amid the heavy pressure of many sorrows, derived

infinite consolation from your Poetry, and some degree of

animation and of confidence from your esteem.



The following performance is far from aspiring to be con\|sidered

as an imitation of your inimitable Poem, "THE

TASK;" I am perfectly sensible, that it belongs not to a

feeble and feminine hand to draw the Bow of Ulysses.



The force, clearness, and sublimity of your admirable Poem;

the felicity, almost peculiar to your genius, of giving to the

most familiar objects dignity and effect, I could never hope to (ECCO-TCP)



Figure 3 shows how Juxta highlights the words which vary between these three copies.

Both of the OCR copies contain errors in individual letters which render the whole word interpretable by a human but not by text mining software, as in the case of “beavy” for “heavy.” The ECCO copy struggles with the fact that ſ is not an available character, sometimes substituting an f, as in “prefure” for “preſſure.” Both leave out spaces between words, creating new tokens like “isfarfrom” and “andſublimity,” though HathiTrust is less prone to this error.

Other features of the OCR copies are accurate to the page image but would nonetheless interfere with text mining. The hyphenation of “Com- poſition,” for example, would prevent it from rendering as a single word, though here even the careful TCP copy would introduce the same problem, since the line break is encoded as “Com\|position.” Before the TCP copy could be used for text mining, the \| characters would likely need to be removed --- not too different from removing the hyphenation from the ECCO and Hathi copies. Most difficult to resolve is the fact that OCR naturally attempts to capture *all* text on the page, including the signature mark and catch word. In ECCO these appear as “,a reach” and in Hathi they are “3. - Reach” whereas TCP more appropriately leaves these out. Unlike the problems with hyphenated words, there is no way to correct for the inclusion of catchwords in a document, since there is no predictable way to identify them --- but keeping them in the document will cause any text-mining software to count these words twice.

The usual “text cleaning” procedures would further prepare these OCR texts for text mining by transforming all words to lowercase, removing all punctuation, and, in most cases, deleting all words which don’t match a predefined dictionary of valid words. A scholar working with the HathiTrust OCR would almost certainly add to this a step converting the ſ character to an s, as discussed above, in order to make the dictionary comparison feasible. The result of this ‘cleaning’ would likely look something like the following:



to william dear sir there is i hope some propriety in my a potion to you which would never perhaps have existed had i not amid the heavy of many sorrows derived infinite consolation from your poetry and some degree of animation and of your he following performance aspiring to be con as an imitation of your inimitable poem the task i am sensible that it belongs not to a feeble hand to draw the bow of clearness and sublimity admirable poem the felicity almost peculiar to your genius of the familiar dignity and i could never hope to a reach (ECCO, as it would likely appear after text “cleaning”)



 

william dear sir there is i hope some propriety in my addressing a position to you which would never perhaps have existed had i not amid the pressure of many sorrows derived infinite consolation from your poetry and some degree of animation and of confidence from your the following performance is far from aspiring to be considered as an imitation of your inimitable poem the task i am that it belongs not to a feeble and feminine band to draw the bow of ulysses. the force of your admirable poem the felicity almost peculiar to your genius of giving to the most familiar dignity and i could never hope to 3 reach (HathiTrust, as it would likely appear after text “cleaning”)





Strikingly, these ‘clean’ texts are now further from legible to human eyes, as OCR errors which a reader could mentally correct (such as “beavy” for “heavy” are now entirely removed.  

![][juxta-emigrants-p1]

Figure 3: Juxta’s “Heat Map” visualization of the “base” witness of the first page of *The Emigrants* (i.e., the ECCO-TCP version carefully prepared by scholars), highlighting words which differ in the two witnesses of the ECCO OCR and the normalized HathiTrust OCR. A darker highlight indicates that the word varies in more than one witness.  

![][juxta-emigrants-histogram]

Figure 4: A histogram, produced by Juxta, showing where the two ECCO and normalized HathiTrust witnesses show the most difference from the base ECCO-TCP copy. “Longer lines indicate areas of considerable difference, while shorter lines indicate greater similarity between documents.” (“A User Guide to Juxta Commons”)  

![][fig-ecco-emigrants-p1]

Figure 5: The facsimile of the first page of *The Emigrants* found in ECCO, which forms the basis of the ECCO OCR text.  

![][hathi-emigrants-p1]

Figure 6: The facsimile of the first page of *The Emigrants* found in HathiTrust, which forms the basis of the HathiTrust OCR text.  

“16 In his discussion of JSTOR's "intolerably corrupt" OCR text, Nicholson Baker suggests that the reason why the user is prevented from scrolling through this naked OCR output is that scholars "might, after a few days, be dis- turbed by the frequency and strangeness of its mistakes . . . and they might no longer be willing to put their trust in the scholarly integrity of the database."17 Baker's criticism of JSTOR, however, is based on an error rate (with editorial intervention) of just one typo in every two thousand characters.” (Spedding 439)

“The two OCR-captured texts average over 150 typos per 2,000 characters,22 a high enough error rate to render parts of the text completely unintelligible. It is not clear how typical this error rate is, and how much it declines with editorial intervention,23 but again the scale of the problem is clear.24 Consequently, the claim that OCR errors "may occasionally result in incorrect character capture, which may affect some \[ECCO\] full-text search results," seems wildly, even heroically, optimistic.” (Spedding 440)  

Like literary canons, these corpora --- especially smaller ones, like the Eighteenth Century Collections Online Text Creation Partnership --- are vulnerable to a critique of their selection methods on the grounds of representation. However, unlike the various changing literary canons of the past, digital corpora tend to conceal which particular titles have been selected as representative. I argue that Charlotte Smith’s inclusion in these resources lags behind a scholarly consensus which sees her as increasingly important and canonical in the period. Her partial inclusion in ECCO-TCP seems particularly likely to lead to ill-supported conclusions by researchers who might easily assume that their text-mining research is taking her works into consideration. However, since none of her sonnets are included, nor any of the politically radical novels which made up a substantial portion of her latter career, nor any of her natural history, some of her most important contributions to the literature of the period are not able to impact studies in which they would be relevant. In particular, a study of women’s writing through the lens of the ECCO-TCP would emphasize the most conventional and expected women’s writing from Smith, with four volumes of one of her more straightforward marriage plot novels.

Exploring the technical affordances of the copies of Smith’s works available in each database also shows why the distorted impression of Smith’s works reflected in the ECCO-TCP’s corpus is likely to persist and continue to be reproduced: without the foundation of a reliable but transformable text (in the form of a human-corrected transcription, rather than a page image or machine OCR), there is a nearly insurmountable technical barrier before any individual project. Even to assess the accuracy of the OCR texts in ECCO and HathiTrust, I must rely on ECCO-TCP. Guillory has already argued persuasively that representation in literary canons is a matter of selection, not of exclusion, that the default state for a given text is not to be included. For Guillory, this serves as a proof that sexism and racism are rarely the direct cause of a particular text lacking canonical status; the role of social oppression in limiting textual representation occurs before scholars make their choices, when classes of people are systematically excluded from the means of textual production in the first place, limiting what we may select from. In the case of digital corpora, also, I see that the rhetoric of “exclusion” is not accurate, and directs attention away from the more complex systems at play. Although I critique the failure of ECCO-TCP to include important and relevant works by Charlotte Smith, it does not seem that she has been excluded out of a prejudice against women’s writing. Most likely, The Emigrants and Celestina were chosen because copies were conveniently accessible to a particular scholar involved in the creation of the ECCO-TCP, perhaps even directly related to a research question which would motivate them through the mind-numbing process of retyping long volumes of prose. Once these works had entered ECCO-TCP, they will naturally be re-used for text mining research which implicitly trusts the original selection. In this way, representation in digital corpora is a matter of infrastructure.  

We are on the cusp of eighteenth-century OCR meeting the standards of twenty-first century OCR. What texts should be OCR’d, by whom, and what should be done with those text files?  

“Of the data-rich literary history projects discussed in this chapter, Underwood and Sellers’s work on changing standards of literary prestigePage 51 → most consistently enacts the curatorial elements present elsewhere in digital humanities, not least in terms of data publication. As the authors explain in an online working paper on the project, its most time-consuming element was not training their supervised model but constructing their dataset: identifying the different subgenres---poetry, prose, fiction, and drama---present in HathiTrust (“How,” 6). As well as publishing the datasets and code used in their article (“Code”), in collaboration with HathiTrust Underwood takes the major, additional step of releasing the outcome of analysis of that mass-digitized collection for others to use. This takes the form of “word counts for 101,948 volumes of fiction, 58,724 volumes of poetry, and 17,709 volumes of drama” published from 1700 to 1922, as well as yearly summaries of word frequencies for each genre. Underwood refers to this dataset as a “collection” to differentiate it from a “corpus” because “I don’t necessarily recommend that you use the whole thing, as is. *The whole thing* may or may not represent the sample you need for your research question” (“Dataset” np).

Signifying growing recognition of the importance of data publication, the Underwood/HathiTrust collection is an important undertaking for data-rich literary history in at least two ways. In presenting a dataset designed for literary history, it offers a shared foundation for research. Working with it, researchers can ask a range of questions based on a reliable, standardized dataset and engage with each other’s arguments in terms not only of results produced but of data investigated. In characterizing that collection as the holdings of “American university and public libraries, insofar as they were digitized in the year 2012 (when the project began), Underwood also frames a major mass-digitized collection---HathiTrust---in terms of its history of transmission (“Dataset” np). However general this framing, Underwood thus explicitly associates the dataset he publishes with a sequence of production and reception that profoundly affects its capacity to support historical analysis. In their article, Underwood and Sellers acknowledge that this history of transmission shapes their findings, noting that their model “makes more accurate” predictions for American poetry collections because HathiTrust “mainly aggregates the collections of large American libraries” (“Longue” 338).” (Bode 50-51)



“Seeking to define the scope of their dataset, Underwood and Sellers note that HathiTrust “may represent more than half of the titles that were printed” because it contains “about 58% of titles recorded in standard bibliographies.” Yet their own “work on fiction” with this collection belies the apparently solid basis of this estimate, finding that HathiTrust contains “many titles left out of” existing bibliographies (“How” n11). Underwood and Sellers thus indicate a significant lack of overlap between established bibliographical records and the holdings of a major digital library, although they do not highlight the significance of this finding nor explore its implications for their own study or for literary history broadly (whether conducted by computational or noncomputational means). Emphasizing that we cannot know the documentary past except through the knowledge infrastructure we create to interpret it, this disjunction that Underwood and Sellers discover highlights the potentially major gaps in all existing forms of description and interpretation: **neither the analog nor the digital record offers an unmediated and comprehensive view of the documentary past; both are partial, and not necessarily in complementary ways.**” (Bode 53)



# Raw Writing #  

“One implication of the publication history of short-title catalogues is that they have been deemed functional and valuable even before they were complete. (That estimation is crucial, for their full completion is for all practical purposes impossible.) Judging that even a preliminary form of the records was useful to scholars, the planners of ESTC determined to conduct its development ‘in full public view’ and to make the incomplete file available ‘warts and all’ (in the words of Henry Snyder and Michael Crump, responding to criticism by Peter Blayney)” (Vander Meulen 270).  





  

“The Project staff found that many eighteenth-century books in hundreds of libraries around the world have never been / catalogued at all, or are described in a group heading,” especially for single-sheet items (Korshin 210-211). “Panizzis ‘Rules' lead to confusing entries and filing for anonymous entries or for items with corporate authorship. For these, and many other related reasons, Alston and Jannetta decided to write their own cataloguing rules, allowing their entries eventually to be converted into machine-readable form, but differing slightly from the standards for machine-readable cataloguing devised in this country (Library of Congress MARC) and in the United Kingdom (UK MARC). Modern machine-readable cataloguing has been devised to deal with cataloguing new books and serials; the ESTC's cataloguing rules have been devised in such a way that a scholar anywhere in the world can tell, from the ESTC entry, whether the copy of the book in his or her library is the same or different from the one listed in ESTC.” (Korshin 211)



“Because the ESTC was formed from three different projects over many decades, the existence of subject headings varies. For books that are from the periods 1475-1640 (from the original Short-Title Catalogue created by Pollard and Redgrave) and 1641-1700 (from Donald Wing’s short-title catalogue), subject headings exist. But subject headings were not initially created for the eighteenth-century records in the ESTC, and so subject headings rarely occur for the items in ECCO.” (Karian, “Guide” 3)





“In ESTC the matching process hinges on five points of identity: the title, as far as it is given by ESTC; the edition statement; the imprint, again as far as it is given by ESTC; the pagination; and the format” (Tabor 370).  

Using the database:

“the needs of the specialist audience that has formed ESTC’s main constituency in the past and may reasonably be expected to continue as such: the explorers in the field who need detailed maps. In my task of mediating between the file and its users, I find that these people approach early books with questions regarding one or both of two broad topics: intellectual content and physical characteristics, the latter including the location of copies,” with most interested in “the physicality of books.” (Tabor 369)

“When everything is working right, as it does in ESTC more often than not, the bibliographical description is completely accurate as far as it attempts to go, and variations in the matched copies can be reliably determined by the presence or absence of copy-specific notes. If anything happens to disturb the links between the description and its attached holdings, including errors committed in the course of creating the description, the record gets broken and starts telling lies about physical copies or, worse, about the ideal copy” (Tabor 370)



“the ESTC is one of the best resources to identify relevant printed materials, determine their publishing histories, and find out where they can be examined” (Karian 283)  

The ESTC is not actually only one database: “The STAR file[ is this still true?], maintained at the ESTC editorial office in Riverside, California, has been functioning for years as a repository for revisions that are transmitted in periodic updates to the publicly accessible file. ... In consequence, though invisible and inaccessible to most of ESTC’s users, STAR contains the most up-to-date version of the file.” (Tabor 373) There are two kinds of information, in particular, which the STAR file contains and which is concealed from ESTC users: the true ‘verified’ status of individual edition matches, and edition-specific notes on known errors.

A “match” in the ESTC is a known copy of a book held in a library which serves as a representative of a particular edition. A match can be encoded as “verified” when an ESTC staff member consults the book to confirm that it corresponds to the edition in question, or “unverified” when this has not happened. But the STAR copy contains two additional verification statuses: when a contributor such as a librarian or a scholar submits a match, it is coded as a “web match.” “Because most outside contributors have received no training in the matching process from ESTC, it would be fair to say that these matches occupy a level of certainty somewhere between the ‘verified’ and ‘unverified’ levels” (Tabor 374). Automated uploads of various outside databases can also generate new matches, encoded as “catalogue matches;” these “have a comparatively low level of reliability” (Tabor 374). However, in the transition from STAR to ESTC, “The standard holdings display smoothes over these nuances; here both catalogue and web matches are translated to ‘verified’. This forces the file to make categorical statements of certainty even in obvious cases of considerable doubt” (374).

\[STAR annotations and DFONOTEs\]  

Transcription errors



Broken records

“A broken record occurs when information added to a record has not been checked against the copies already listed under that record” (Karian 285)



Inaccurate dates

‘The ESTC sometimes records the date as questionable, and sometimes records the date within a range. But when one does a large-scale search for records---for example, everything from 1720---there is no easy way to screen out items not definitively dated to 1720” (Karian 291)



“only records bibliographic information about surviving books ... After a careful study of book advertisements, inventories and bibliographies, it seems to this investigator that, excluding jobbing and newspaper printing, as much as 10 per cent of the printed record from 1701--1800 has not been incorporated into the ESTC. In other words, for up to 10 per cent of the editions printed in the eighteenth century, not a single copy is known to survive.” (Suarez 40)  

As Suarez notes, the ESTC is a unique resource for pre-19thC works: “Regrettably, although this volume of The Cambridge history of the book in Britain ends in 1830, it is not possible to perform a similarly comprehensive analysis for the first decades of the nineteenth century because we have no equivalent bibliographical control for this period” (Suarez 40).  

“To use a metaphor, some people prefer to explore the world through books of photographs with occasional schematic maps. ESTC, on the other hand, provides the equivalent of a detailed topographic map, but no pictures. Such technical tools have limited appeal, even to some specialists; but if you want to thoroughly learn the lie of the land, you will need one, and the more complete and accurate the better.” (Tabor)



  

“An increasingly common trend, I am sorry to report, is that more and more people do not want ESTC at all --- they want ECCO or EEBO. The younger generation of scholars in particular, lured by full-text images and ransacking the Web for illustrations for their books and articles, are using these utilities as de facto bibliographic databases. They find that the stripped-down records and simplified indexes are good enough for their purposes. To a minority of them, the fact that other works, editions, and copies exist outside the Web is irrelevant, and perhaps even irritating.” (Tabor 368)  

**Comedies vs tragedies performed**: the ratio of comedies to tragedies performed was an astonishing 14 to 1 in Paris (Theatre, Opera, and Audience in Revolutionary Paris: Analysis and Repertory by Emmet Kennedy, Marie-Laurence Netter, James P. McGregor, and Mark V. Olsen)

**Suarez numbers**

Some Statistics on the Number of Surviving Printed Titles for Great Britain and Dependencies from the Beginnings of Print in England to the year 1800, by Alain Veylit.   

Other work which has used the methodology of sampling includes 



Suarez: “Lacking the resources to conduct a detailed analysis of the entire ESTC from 1701 to 1800, I have resorted to sampling. Electing to examine all eighteenth-century records that appear in years ending in three -- 1703, 1713, 1723 and so on -- I have sought to avoid a number of cohort effects, most especially the cumulation of indeterminate records into years ending in ‘0’ or ‘1’ and, to a lesser degree, ‘5’.” (41)  

Which of these archives are the most "reliable", and which the most "distorted"? (obvs interrogate this framework)

* What’s *in* all these, anyway?
    * What does ECCO-TCP leave out compared to ECCO? Compared to ESTC? (Can I come up with adjustment factors?)
    * How do digital vs physical holdings compare?  

What is a "normal" footprint in the print culture of this decade? (i.e., what are the boundaries a work has to surpass to be unusually popular or unusually unpopular?)  

The problem of textual selection--- the paired difficulty and importance of deciding which few books one will actually read--- is an urgently meaningful problem in the “real world,” outside the realms of academia. A common solution is one which will likely alarm and distress most scholars who have dedicated themselves to thinking through canonical selection, or even the construction of a syllabus: a solution which could perhaps be called ‘radical impatience.’ Consider, for example, the following advice in a blog post, “100 Ways to Live Better,” item number 6, the first entry in the category “Mind”:

There are more great podcasts than you’ll ever have the time to listen to. If it sucks after 10 minutes, skip half an hour ahead. Still boring? Delete and move on. Obviously, do the same for books.

There is much to find distressing here. The operant metric that another’s ideas “suck” if they are “boring” in the first ten or thirty minutes. The fact that “obviously” the same metrics apply for books as for podcasts. Indeed, even the fact that podcasts so strongly come *before* books. Each word in the first sentence is itself a link to a specific podcast recommended by the writer, but “books” are a monolith and a footnote.



Later we see another metric for textual selection:

Should you watch that movie / play that game / read that book? Use the ratio:

(\[# who rated it 5/5\] + \[# who rated it 1/5\]) / \[# who rated it 3/5\].

This doesn’t apply to everything, but it applies to many things, including media. There are too many options out there to waste time on mediocrity, and everything great will be divisive.

Paired with the first piece of advice, this would suggest a process of seeking out divisive and controversial works, determining within ten minutes whether one is strongly in sympathy or strongly opposed, and continuing only with works which evoke strong sympathy.

Of the seven mentions of the word “book,” two are in fact the word “Facebook.” Two are the pieces of advice quoted above. Two are suggestions to find “good audiobooks, and/or a dog” to form a habit of going on walks, or “a good app or guidebook” to practice meditation. And the last is the advice that “If you’ve been waiting for months for someone to create an event and invite you, whether it’s a book discussion or a BDSM orgy, just throw one yourself,” in which “a book discussion” functions rhetorically as an extreme example of an unusual and improbable kind of social event, paired against its assumed opposite extreme.  

I created a “content set” called ECCO-1798 in Gale Digital Scholar Labs, by searching for all works in ECCO published “between” 1798 and 1798. This located 4,158 records. I downloaded the metadata for these 4,158 records as a CSV. I then used random.org’s “Random Integer Generator” to generate ten integers from 1 to 4,158 (inclusive), resulting in the following numbers: 1792, 2365, 159, 3511, 919, 170, 2136, 2259, 190, and 2242. I looked up the ten works appearing in those rows of the spreadsheet, without altering the order of the records from Gale’s default. (It is unclear to me what sorting method was used to organize them in the document.) I used the Gale Content Numbers of these ten works to create a new “content set” of just these ten titles.  

159 - Poetry; original and selected	Monograph	Monograph	Literature and Language I.	\[1796-98\]	Gale	London, United Kingdom		British Library	null	GALE\|CW0115892706	

170 - Sir, You are desired to meet the committee for improving the navigation of the River Thames, and for preventing encroachments on the said river, on board the navigation barge, at Staines, on Saturday, the 7th day of July 1798, at eight o'clock in the morning, and proceed from thence down the river at nine precisely, ...	Monograph	Monograph	Social Sciences II.	\[1798\]	Gale, a Cengage Company	Oxford, United Kingdom	Great Britain. Commissioners Appointed for Improving and Completing the Navigation of the Rivers Thames and Isis	Bodleian Library, University of Oxford	null	GALE\|CB0130118850	

190 - Thoughts concerning the proper principles of finance, that ought to be adopted at present, and in future, in support of the British government. Addressed To The Freeholders And Mercantile Interest Of Leeds, Wakefield, Halifax, Huddersfield, Bradford, Doncaster, Hull, And The Other Towns In The County Of York. By a freeholder of Yorkshire	Monograph	Monograph	Social Sciences I.	1798	Gale	Lawrence, KS, United States	James Cochrane	Spencer Research Library, University of Kansas	null	GALE\|CW0107793549	

919 - National blessings considered and improved, in a sermon, preached on Thursday, November 29, 1798. By Alex. Black, Minister, Musselburgh	Monograph	Monograph	Religion and Philosophy I.	1798	Gale	London, United Kingdom	Alexander Black	British Library	null	GALE\|CW0123387895	

1792 - False impressions: A comedy in five acts. Performed at the Theatre Royal, Covent Garden. By Richard Cumberland, Esq.	Monograph	Monograph	Fine Arts II.	1798	Gale, a Cengage Company	London, United Kingdom	Richard Cumberland	British Library	Eighteenth Century Collections Online	GALE\|CB0129794221	

2136 - The surprizing adventures, of Jack Oakum, & Tom Splicewell, two sailors, who went a pirating on the Kings' highway. How that the first \[prize\] they took gave information of their course, and being pursued by a whole squadron, Tom Spicewell was taken and condemned to be hanged \[:\] but by means of his beloved friend Jack Oakum, who interested with his Majesty, he was pardoned.. Also a copy of Jack's polite letter to the King, on the above occasion. To which is added, The merry revenge; \[our,\] Joe's stomach in June	Monograph	Monograph	Literature and Language I.	1798	Gale	Oxford, United Kingdom		Bodleian Library, University of Oxford	null	GALE\|CW0116560788

2242 - The American preceptor; being a new selection of lessons for reading and speaking. Designed for the use of schools. By Caleb Bingham, A.M. author of The Columbian orator, Child's companion, &c. \[One line of quotation\] Published according to act of Congress	Monograph	Monograph	Literature and Language II.	1798	Gale, a Cengage Company	Boston, MA, United States	Caleb Bingham	Boston Public Library	null	GALE\|CB0130828924	

2259 - The letters of Junius	Monograph	Monograph	Literature and Language I.	1798	Gale	Cambridge, MA, United States	Junius	Houghton Library, Harvard University	null	GALE\|CW0110410930	

2365 - Gil Blas corrigé; ou histoire de Gil Blas de Santillane. Par M. Le Sage. Dont on a retranché les expressions & passages contraires à la décence, ... & à laquelle on a ajouté un recueil de traits brillans, des plus célèbres poëtes françois. Par J. N. Osmond. ...	Monograph	Monograph	Literature and Language I.	1798	Gale	Oxford, United Kingdom	Alain René Le Sage	Bodleian Library, University of Oxford	Eighteenth Century Collections Online	GALE\|CW0116687637

3511 - A Plain narrative of facts respecting the trial of James Coigley; Including his letter to an Irish gentleman, in London, and A. Young's letter to G. Lloyd	Monograph	Monograph	Law II.	1798	Gale, a Cengage Company	Cambridge, United Kingdom		University of Cambridge Library	null	GALE\|CB0132172434	  

1053 - Le juge à paix, et officier de paroisse, pour la province de Quebec. Extrait de Richard Burn, chancellier du diocèse de Charlisle, & un des juges à paix de Sa Majesté, pour les comtés de Westmorland & Cumberland. Traduit par Jos. F. Perrault	M.DCC.LXXXIX.\[1789\]	London, United Kingdom	Richard Burn	

2464 - An apology for professing the religion of nature, in the eighteenth century of the Christian aera; addressed to the Right Reverend Dr. Watson, Lord Bishop of Landaff	MDCCLXXXIX. \[1789\]	Oxford, United Kingdom	David Williams

1211 - Some account of the discovery, made by the late Mr. John Dollond, F. R. S. which led to the grand improvement of refracting telescopes, in Order to Correct some Misrepresentations, in Foreign Publications, of that Discovery: with an attempt to account for the mistake in an experiment made by Sir Isaac Newton; on which Experiment, the Improvement of the Refracting Telescope Entirely Depended. By Peter Dollond, Member of the American Philosophical Society at Philadelphia	 M.DCC.LXXXIX. \[1789\]	London, United Kingdom Peter Dollond	

1086 - Reflections on the contentions and disorder of the corporation of Cambridge	M.DCC.LXXXIX. \[1789\]	Oxford, United Kingdom

51 - Nécessité de supprimer et d'éteindre les ordres religieux en France, prouvée par l'histoire philosophique du monachisme, ...	1789 London, United Kingdom

938 - Emma Dorvill. By a lady	MDCCLXXXIX. \[1789\]	London, United Kingdom

629 - Virtue and peace forever inseparable. A discourse at the interment of Capt. John Howard, of Hampton, June 17th, 1789. By Joseph Huntington, D.D. \[Three lines from Young\] A few thoughts appear in the copy which, for the sake of brevity, were omitted in preaching M.DCC.LXXXIX. \[1789\]	Washington, DC, United States	Joseph Huntington

1524 - A practical treatise on the gonorrhoea, and on the superior efficacy of the cure by injection. By Peter Clare, surgeon \[1789\] Boston, MA, United States	Peter Clare	

702 - Analyse raisonnée de la sagesse de Charron. premiere partie	M.DCC.LXXXIX. \[1789\]	Manchester, United Kingdom	Jean-Pierre-Louis de Luchet

1732 - Two discourses. I. On wisdom attainable by meditation of the vanity of human life ... II. Men more influenced by example than precept; ... Preached ... March 8, 1789, by the Reverend Samuel Hopkinson, ...	\[1789\]	London, United Kingdom	Samuel Hopkinson  

I could end by describing what Bode-like scholarly edition I’d like to make. What would it be?

Datasets of men’s, women’s, and unsigned writing from the 1790s? Filtering to fiction, or poetry? Or sermons?? Non-literary writing? Things that people might want to teach?

Circulating libraries? Maybe one specific library, to be feasible?

Reprints??  

A collocation formula like “ix,\[3\],68\[i.e. 60\]p. ; 4⁰” (the physical description of Smith’s T*he Emigrants* provided in the ESTC, ECCO, and ECCO-TCP) is no more transparent and obvious in meaning than the following markup:

\<listPrefixDef\>

\<prefixDef ident="tcp"

matchPattern="(\[0-9\\-\]+):(\[0-9IVX\]+)"

replacementPattern="https://data.historicaltexts.jisc.ac.uk/view?pubId=ecco-$1&index=ecco&pageId=ecco-$1-$20"/\>

\<prefixDef ident="char"

matchPattern="(.+)"

replacementPattern="https://raw.githubusercontent.com/textcreationpartnership/Texts/master/tcpchars.xml#$1"/\>

\</ListPrefixDef\>

Indeed, the collocation formula is less transparent than simple markup like the following:

\<titleStmt\>

\<title\>The emigrants, a poem, in two books. By Charlotte Smith\</title\>

\<author\>Smith, Charlotte Turner, 1749-1806.\</author\>

\</titleStmt\>

It may even compare unfavorably to relatively well-commented code, like the following:

| \# iterate through the directory ||
|  | for filename in listdir\_nohidden("./" + directory): |
| -----: | :----- |
|  |  |
|  | \# define the path to this file |
|  | path = "./" + directory + "/" + filename |
|  |  |
|  | \# strip the file's namespace |
|  | try: |
|  | xmlstring = stripNamespace(path) |
|  | except: |
|  | print "error stripping namespace of file %s" % (filename) |


What these comparisons intend to illuminate is *not* that collocation formulae ought to be simpler or more accessible. Rather, my point is that specialized encoding serves a practical purpose, and that it is a matter of training which determines what encodings will seem natural and useful.  

My own sliver of the ESTC was generously provided to me by the British Library in January 2017. It contains all items matching the query I specified, “(Words= alldocuments and W-year= 1789-\>1799 and W-Country of publica= enk),” which requests all documents published between 1789 and 1799 (inclusive) with a place of publication encoded as “England.” Running this search on the ESTC website at the time returned 52,001 records. The tools used to create the file, according to the librarian with whom I corresponded, returned 51,965 records, 36 records having gone missing; however, the file itself contains only 51,860, another 105 mysteriously lost. These 141 missing records are currently an unsolved mystery. My records come from the British Library’s ESTC database, rather than the STAR file. The corpus itself consists of a csv file[^cf32] with fifteen columns of information. The columns are: “Type of resource” (“Monograph” or “Serial”); “ESTC citation number”; “Name” (e.g., of an author, editor or illustrator); “Dates associated with name” (generally, the years they lived); “Type of name” (“meeting/conference,” “organization,” or “person”); “Role” (e.g., “author,” “cartographer,” or “bookseller”), “All names”, “Title”, “Variant titles”, “Place of publication”, “Publisher”, “Date of publication” (a single year), “Date of publication (not standardised)” (e.g., a year in roman numerals, or a date which includes a month or day), and “Publication date range” (for serials). In other words, it includes the very basic information of author, title, publisher, and year, in a complex structure which belies the apparent simplicity of these “basics.” Some of the ESTC records included in this corpus do not necessarily match my selection criteria (England, 1789-99), which is inevitably true of every corpus collected, and which I discuss in more detail in SECTION, Data Cleaning.  

My first source of ECCO metadata consisted of MARC records, kindly provided by University of Toronto libraries (my thanks to Leslie Barnes!). I requested information for all works published 1789-99 in the UK (so, including Ireland and Scotland, but excluding America.)

My ECCO metadata presented particular challenges. I had access to MARC records, which stands for MAchine Readable Catalogue. At several points, I read this data with my feeble non-machine eyes in order to guide my data processing. Using MarcEdit, I converted these MARC records to csv files which could, in OpenRefine, be read, manipulated, and merged like my other corpora. Since I was not able to simply convert “all the MARC headings that exist” using MarcEdit, I used all numbers 1 to 999 and \[will\] delete empty columns.

ECCO encodes much of its data in “unassigned” columns, rather than the standardized LOC categories.

<https://www.itsmarc.com/crs/mergedprojects/helpauth/helpauth/tag_list.htm>

Library of Congress MARC info







I also have ECCO metadata records, now, from the Gale Digital Scholars Lab. Due to their restrictions on how much data may be downloaded at once, I have created 11 files, each one holding the metadata for all of ECCO’s works in each year. ECCO-1789.csv, for example, is based on the following search: “LIMITS: Archive (Eighteenth Century Collections Online) And Publication Date (1789 - 1789).” These files initially contain works from a wide range of publication locations, not just England, since the Digital Scholars Lab does not provide any way to filter items by publication location.  

For each author, I went to http://estc.bl.uk/, determined the authoritative format of that author’s name within the database (e.g., “Smith, Charlotte Turner, 1749-1806”) and got the list of all titles attributed to that author. Paging through each entry, I recorded the date, edition, city, and publisher for every title which met my conditions of being printed in England 1789-99, and recorded the “permalink” URL to the work’s full ESTC record.  

I started by testing just *The Emigrants*. To get the text without the XML markup, I viewed it at <https://quod.lib.umich.edu/e/ecco/004801766.0001.000?rgn=main;view=fulltext> and simply copy-pasted the page into a plaintext file, manually deleting the header and footer website text so that the file only contained the poem. I saved it as emigrants-TCO-OCR.txt.

I attempted to download the HathiTrust edition by going to the “text-only view of this item” at <https://babel.hathitrust.org/cgi/ssd?id=uc1.31175035214942;page=ssd;view=plaintext;seq=15;num=ix>, where I discovered that I only had “one page at a time access to this item,” even though it was correctly identified under “Rights” as a public domain work. Authenticating through the University of Toronto gave me access to the full work. This page came with the warning “Use of this online version is subject to all U.S. copyright laws. Please do not save or redistribute this file,” which I disregarded under Fair Use to save a personal research copy of the text. Again the easiest method was to copy-paste the page into a plaintext file and delete unnecessary headers. I deleted “Book Text - Front Cover” and everything above, and “End of Section 9” and below, and saved the file as emigrants-Hathi-OCR.txt.

To download the ECCO OCR file, I went to the new Gale Digital Scholar Labs portal (since ECCO itself does not make the OCR available). The “basic search” very annoyingly attempted to autocomplete my search for “the emigrants” to unrelated terms, like “henry the eighth” and “female emigrants”. The “advanced search” did the same, changing “the emigrants” to “therefore” and “mall of the emirates.” Eventually, I was able to find the desired text by searching “the emigrants” as document title and “charlotte smith” as author. Gale Digital Scholar Labs prominently assigns an “OCR Confidence” of 95% to the record. The viewer did not give me 95% confidence. This method of accessing the text was the first to offer a download of the OCR. I downloaded it with their tool, examine the file, and deleted the disclaimer about OCR which appeared at the top, then renamed the file emigrants-ECCO-OCR.txt to match the other files.

At this point I was ready to load the files into Juxta. I signed in to Juxta Commons (at <http://juxtacommons.org/home/index>) to use the web interface, and uploaded all three files as “sources.” I prepared all three as “witnesses,” then created a “comparison set” of “Emigrants OCRs” with them and collated. It took a long time to collate, and then I discovered that, unfortunately, whichever file appears first (in this case, the ECCO file) is taken as the “base” to compare the others to, and I couldn’t determine how the base could be changed. I deleted the ECCO and Hathi witnesses from the set, then dragged them back in to it, to place them below the TCP witness. This caused unexpected server errors and failed, so I deleted the whole set, then created a new one with just the TCP witness and tried again. This worked, but the ECCO witness was immediately listed about the TCP witness, presumably due to alphabetization. I created a new witness from my TCP source, naming it beginning with A, and added that to the comparison set, then collated. To my surprise, Juxa calculated a relatively low “change index” for each text compared to the TCP witness: ECCO had a .16 change from base (i.e., 84% accuracy), and HathiTrust a .29 change from base (i.e., 71% accuracy).

Since some of HathiTrust’s inaccuracy may come from the fact that it uses the ſ character whereas TCP modernizes this to an s, I then ran a second experiment. Using “find and replace” in TextWrangler, I changed all 1278 instances of ſ in the HathiTrust file to s, and saved this as a new file, emigrants-Hathi-OCR-regularized.txt. I uploaded this to Juxta as a new source and witness for the same set. In doing so, it became clear that Juxta takes whatever the newest witness is as the “base,” so I had to delete/recreate my TCP witness again to make it the base. Once I got this working, the new HathiTrust copy had only a .09 difference from the TCP copy --- for 91% accuracy!  

The “gender” package in R is able to draw on a range of historical sources for gender information, but only one is applicable for this project. Most are US-based or only contain information beginning in the nineteenth century (or both). \[Or, SHOULD I use ipums...? Will US names differ a lot?\]

If no value is specified, then for the "ssa" method it will use the period 1932 to 2012; acceptable years for the SSA method range from 1880 to 2012, but for years before 1930 the IPUMS method is probably more accurate. For the "ipums" method the default range is the period 1789 to 1930, which is also the range of acceptable years. For the "napp" method the default range is the period 1758 to 1910, which is also the range of acceptable years. 



"The "napp" method uses census microdata from Canada, Great Britain, Denmark, Iceland, Norway, and Sweden from 1801 to 1910 created by the North Atlantic Population Project.” (Mullen, Blevins, and Schmidt)  “For the "napp" method the default range is **the period 1758 to 1910**, which is also the range of acceptable years.”(Mullen, Blevins, and Schmidt)

“The North Atlantic Population Project (NAPP) is a machine-readable database of the complete censuses of Canada (1881), Denmark (1787, 1801), **Great Britain (1851, 1861, Scotland 1871, 1881, 1891, 1901, 1911)**, Norway (1801, 1865, 1900, 1910), Sweden (1880, 1890, 1900, 1910), the United States (1850, 1880) and **Iceland (1703, 1729, 1801**, 1901, 1910).” (NAPP)

So actually, NAPP data has nothing relevant to *both* my time and place. But this may be the same US data that’s in the IPUMS method, so it’s still worth comparing the two.



“The "ipums" method looks up names from the U.S. Census data in the Integrated Public Use Microdata Series.”(Mullen, Blevins, and Schmidt) “For the "ipums" method the default range is **the period 1789 to 1930**, which is also the range of acceptable years.”(Mullen, Blevins, and Schmidt)  

To get a handle on how the gender package in R actually worked, and to assess how well it would meet my needs for this project, I began my making a sample csv of 41 arbitrary titles from ECCO’s 1789 holdings. The choice was driven by simplicity for a proof of concept: my ESTC and full-ECCO files were too large to open on my computer in spreadsheet software, and writing a program to extract a random sample was more work than it was worth, so I chose a file that I knew I could open in Numbers, the sample of all 1789 ECCO titles which I had recently downloaded through Gale’s Digital Scholar Lab. I selected some rows from the top of the list, skimming the names as I went to make sure I had selected enough titles that they would include two by women, and then copied those rows to a new spreadsheet (ECCO-1789-sample.csv). After some false starts with RStudio, I was still struggling to read the file into the program. Since all I wanted to know was whether, once I got it running, the gender package would tell me results worth working with, I decided to do this first test in whatever way would get a result, even if it required a great deal of non-scalable manual labor.

I copied the authors column into a plaintext file, and manually created a comma-separated list of just the first name of each other, to match the data format which the gender package required. (I took the first word before a space in each name, to reflect how a future first-name-grabber algorithm would work, even when this meant choosing things which were clearly not first names --- one of my core questions is how the gender package will handle those kinds of exceptions.) The resulting list of names was saved as ECCO-1789-sample-firstnames.csv. Trying to paste in this list for the names finally helped me understand what is meant by a “character vector” in R. It’s basically an array of strings. You have to make the character vector before you can run gender() on it. Sample code:

names = c("john", "madison") # creates a character vector called “names” by ‘combining (c-ing) two strings

gender(names, method = "demo", years = 1985) # guesses gender for both of those names

Once I had gotten something to produce guesses for more than one name, I wanted to try using a method other than “demo.” This sent me down a spiral of trying to install genderdata, and trying to install devtools to install genderdata. Finally I got everything installed and able to run!

If a command is run requesting a name-year-method combination for which the package has no data, it simply returns an empty “tibble.” The following three sets of results illustrate the limits and possibilities of napp and ipums data:



\> **gender(ECCOnames, method = "ipums", years = 1789) - earliest year possible with ipums**

\# A tibble: 25 x 6

name      proportion\_male proportion\_female gender year\_min year\_max

\<chr\>               \<dbl\>             \<dbl\> \<chr\>     \<dbl\>    \<dbl\>

1 Andrew              1                 0     male       1789     1789

2 Ann                 0                 1     female     1789     1789

3 Benjamin            1                 0     male       1789     1789

4 Charles             1                 0     male       1789     1789

5 Charles             1                 0     male       1789     1789

6 Charles             1                 0     male       1789     1789

7 Charlotte           0                 1     female     1789     1789

8 Edward              1                 0     male       1789     1789

9 Francis             0.464             0.536 female     1789     1789

10 James               1                 0     male       1789     1789

\# ... with 15 more rows



\> **gender(ECCOnames, method = "napp", years = 1769) - earliest year that knows about “John”**

\# A tibble: 7 x 6

name  proportion\_male proportion\_female gender year\_min year\_max

\<chr\>           \<dbl\>             \<dbl\> \<chr\>     \<dbl\>    \<dbl\>

1 John                1                 0 male       1769     1769

2 John                1                 0 male       1769     1769

3 John                1                 0 male       1769     1769

4 John                1                 0 male       1769     1769

5 John                1                 0 male       1769     1769

6 John                1                 0 male       1769     1769

7 John                1                 0 male       1769     1769



\> **gender(ECCOnames, method = "napp", years = 1789) - more direct comparison to ipums**

\# A tibble: 9 x 6

name      proportion\_male proportion\_female gender year\_min year\_max

\<chr\>               \<dbl\>             \<dbl\> \<chr\>     \<dbl\>    \<dbl\>

1 Charlotte               0                 1 female     1789     1789

2 John                    1                 0 male       1789     1789

3 John                    1                 0 male       1789     1789

4 John                    1                 0 male       1789     1789

5 John                    1                 0 male       1789     1789

6 John                    1                 0 male       1789     1789

7 John                    1                 0 male       1789     1789

8 John                    1                 0 male       1789     1789

9 Samuel                  1                 0 male       1789     1789



Neither is very stunningly thorough, especially since the year is supposed to reflect the *birth* year of the person, and I can only barely get information about the year they published something. Somewhat to my surprise, the ipums method --- which I originally planned to reject in favor of napp, because ipums is US only --- has much more data available, and doesn’t seem to reflect any US-specific oddities. So, I might try to use both ipums and napp, but if that is difficult, just using ipums seems appropriate. I’m not entirely sure, yet, whether using ipums will be faster than just manually assigning genders: my next question, I think, is determining how many unique first names there are, and then trying to guess how many of them ipums would be able to sort out for me. It managed 25/41 = 61% of the random ECCO sample, which is a substantial number, especially since it will by definition include the most common names. Expanding the acceptable years slightly also seems to help it know a lot more about the names, while still according with my own estimates. (ipums can learn “Augustus” in 1799, for example.)  

Table 1 shows data that I compiled by hand in Numbers. The first three columns are based on my synthesis of scholarship on Charlotte Smith. As I consulted a range of work on Smith, I updated this information to reflect the most complete and accurate information possible. My editorial decisions included, for example, the exclusion of *D’Arcy* from consideration, since it was never published in England. I introduced standardized titles for the two volumes of *Elegiac Sonnets*, retroactively naming the initial publication “volume 1” to distinguish it from the second volume which would appear 13 years later, so that each title has its own edition count. The next four columns represent the results of my queries in the ESTC, ECCO, ECCO-TCP, and HathiTrust databases. I searched each database with several queries to locate Smith’s works, beginning (where possible) by finding all works categorized under her authorship, and then searching individual titles of works. This figure shows the simplified results from a more detailed spreadsheet, which also includes links to the records themselves where they exist, and notes on how the records are encoded (e.g., multiple volumes or all as one volume.) Simplifying inclusion down to a boolean yes/no involved some editorial decisions. If only *part* of a work was included (as in HathiTrust’s record for the first edition of *Celestina*, which only includes volumes 3 and 4), I recorded that as a “yes.” If a work was only included in its Dublin edition (as in HathiTrust’s record for *Desmond*), I recorded that as a “no.” These searches were conducted in February 2020.  

Figure 1 is based on the data recorded in Table 1, which was pasted into the RAW Graphs visualization tool (Mauri et al.) and processed using their default settings for an alluvial diagram. Using an alluvial diagram required imagining the databases as sequential “stages” through which all books flow. Accordingly, I added a “source” for this flow of books by adding a column labeling every edition as falling into the category “Works printed in England, 1784-1807.” I could have chosen to place the following “stages” in any order; to assist in visualizing Smith’s representation in these databases as a process of winnowing down, I chose to place them in order from largest selection to smallest. The scale of each “strand” of the diagram is scaled in width based on the number of editions it represents, as per RAW Graphs’ default settings. The colours, fonts, and width of the graph are also simple defaults.

[CSmith-in-ESTC-ECCO-TCP-Hathi-table]: CSmith-in-ESTC-ECCO-TCP-Hathi-table.png width=184px height=312px

[CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3]: CSmith-ESTC-ECCO-TCP-Hathi-alluvial-3.png width=244px height=198px

[juxta-emigrants-p1]: juxta-emigrants-p1.png width=145px height=99px

[juxta-emigrants-histogram]: juxta-emigrants-histogram.png width=72px height=155px

[fig-ecco-emigrants-p1]: fig-ecco-emigrants-p1.png width=100px height=161px

[hathi-emigrants-p1]: hathi-emigrants-p1.png width=148px height=192px

[^cf1]: More specifically, these “authors” are “Great Britain, Parliament,” “Great Britain,” “Great Britain, Parliament, House of Commons,” “Great Britain, Lords Commissioners of Appeals in Prize Causes,” and King George III. After King George comes Thomas Paine and Hannah More, and then it’s “Great Britain, Parliament, House of Lords” and “Church of England.”

[^cf2]: Google Books and Project Gutenberg are not, of course, traditionally “scholarly” resources, but that is why they form an informative contrast with the other resources examined.

[^cf3]: This calculation is carried out by manually examining the metadata of the six corpora I have acquired.

[^cf4]: This calculation is carried out by a small, simple program I am writing, described in Appendix A. Because the program just simplifies a straightforward process of counting, it is only lightly theorized in the dissertation itself.

[^cf5]: I already know that ‘titles per year’ are distributed fascinatingly differently between ECCO, ECCO-TCP, ESTC, and HathiTrust

[^cf6]: This calculation is carried out by a larger, more complex program I am writing, applying topic modelling to the titles of works. Because it makes several major interpretive choices, it is theorized and discussed in detail when it is applied.

[^cf7]: ‘Too much’ and ‘too little’ are here, of course, defined from the point of those with cultural capital which they wish to maintain.

[^cf8]: Part of Guillory’s argument is that, although the rhetoric of the canon debates generally sought to re-value authors of any number of oppressed categories, often using the phrase “gender, race, and class” as a single unit, the work undertaken was in fact unable to address class, since class operates differently from gender and race.

[^cf9]: Appendix B (“Methodology”) contains many examples of these algorithmic procedures executed by the human researcher and the computational programs in concert. The act of writing a program is an iterative process of delegation.

[^cf10]: Indeed, Buurma notes, “There are good reasons, of course, that scholars and journalists like to begin with Busa: he was the first concordance-maker to automate all five stages of the process, in 1951,” and he intentionally foregrounded and publicized the innovative nature of his work. \\cite\{Buurma:2018wt\}

[^cf11]: In the interest of preserving this history of citation, the students were Mary Jackman and Helen S. Agoa, credited on the cover of the published Dryden index. (Miles herself attached her name only to the preface.) From the computer lab staff, Miles particularly thanked Shirley Rice, Odette Carothers, and Penny Gee.

[^cf12]: It may also be the case, of course, that even fields with a long history of graphical display would benefit from greater scrutiny of the evidence they use; see: the Data Dinosaur. But this is beyond the remit of what an English PhD can address.

[^cf13]: I cite Tufte and Cairo as the thinkers whose design philosophies best accord with my own current understanding of the work and craft of persuasive data visualization, but my actual practical training as a graphic designer is indebted to Judith Galas, Sonia Davis Gutiérrez, and Tom Hapgood.

[^cf14]: Tufte is careful not to blame the engineers for being better at engineering and systems analysis than they were at design: rather, this example shows that design is a skill that involves expertise; when designs matter, people with that expertise need to be involved.

[^cf15]: 1. show comparisons, contrasts, differences 2. show causality, mechanism, explanation, systemic structure (intervention relies on manipulable causality -- can't do anything with the information without causality) 3. show multiple variables (3 or more) -- the world is multivariate 4. \*completely integrate\* words, numbers, maps, graphics, etc, etc. Provide information at exact point of need 5. documentation must thoroughly describe evidence and its sources, provide complete measurement scales 6. presentations succeed based on their content. for better presentations, get better content.

[^cf16]: Kath Bode and Leah Price have both described at length how textual editing and anthologizing, respectively, are literary methods of sampling. See: Bode, Katherine. *A World of Fiction: Digital Collections and the Future of Literary History*. University of Michigan Press, 2018. And Price, Leah. *The Anthology and the Rise of the Novel: From Richardson to George Eliot*. Cambridge UP, 2000.

[^cf17]: Because Bode is examining only one database, she is able to present a single date on which her data collection ceased. I have not been able to accomplish this, but for any given resource, will identify the date of that resource’s “snapshot.” This approach means that my observations may be out of date from the moment I make them, though one of my findings in chapter 2 is that many databases are currently changing more slowly than one might expect.

[^cf18]: Although these events, of course, did not occur on January 1 or December 31, respectively, the entirety of 1789 and 1799 are both included in my study, out of sheer technological necessity.

[^cf19]: (Harper 2016; Jacsó 2008; Weiss 2016) (CITE Mike Sutton and Mark D. Griffiths)

[^cf20]: CITE http://languagelog.ldc.upenn.edu/nll/?p=1701

[^cf21]: I have heard it quipped more than once in digital humanities gatherings that you always think you’re going to get your texts from somewhere else, but Project Gutenberg is where you’ll actually get them.

[^cf22]: For example, it might be able to acquire a text document with all of the words of a novel, but sorted into alphabetic order: such a text file can be used for some analyses based on word-frequency, but cannot be read. Or, it might be possible to find collocations of where a given word appears, but with only a limited number of words of context on either side of the term in question. Or, scholars can run pre-written code provided by HathiTrust to carry out things like topic modelling on the full, intact texts of their chosen works, but without being able to inspect those texts or run their own code on them. All of these modes of analysis make research much more difficult to carry out, and nearly impossible to verify. In the study of contemporary copyrighted literature, however, even these very limited tools for corpus analysis are valuable.

[^cf23]: I have heard it quipped more than once in conferences sessions that you always *think* that you’re going to get your texts from OCR, but you always *do* get them from Project Gutenberg.

[^cf24]: Volumes 4 and 5 of *Letters of a Solitary Wanderer* are in fact part of the same bibliographic record as the first three volumes. The publication date for the combined five-volume work is listed as “1800-1802.”

[^cf25]: Several of HathiTrust’s records provide “mixed copies” like this, with some volumes scanned from one library’s holdings and other volumes scanned at another. If there is overlap, multiple scans will be provided for the duplicated holdings. Nonetheless, all of these scans are tied to a single unified MARC record, taken from only one of the holding library (with no indication of which library provided it).

[^cf26]: Later known as Primary Source Microfilm, an imprint of the Gale Group.

[^cf27]: “Because ESTC is a bibliographical database rather than a catalogue, strictly speaking, its records describe groups of copies,” such as editions, “rather than specific copies,” such as the Exeter Book (Tabor 369).

[^cf28]: “The first problem relates to the unit of classification. A clearly defined unit is necessary to ensure that a study of change over time is reliable and based on consistent terms. What is the unit that the ESTC uses? Scholars sometimes answer by using the terms “edition,” “issue,” or “title” interchangeably. But since the ESTC does not rely in a consistent manner on any of these terms for its unit of classification, one should refer instead only to the ESTC record, a unit created by the ESTC and having no meaning outside the ESTC.” (Karian 289)

[^cf29]: Technically, in the “##” sequence, the first “#” encodes that the work is a first edition (as opposed to a “2” for an “intervening” edition or a “3” for the “current” most recent edition), and the second “#” doesn’t encode anything. That position in the MARC record is undefined, with no possible meanings, and simply always contains a ‘blank’ #.

[^cf30]: One exception to this assumption has to do with treatment of the character ſ, which the TCP file modernizes to an s, but which HathiTrust renders as ſ. To avoid penalizing HathiTrust for “inaccuracy” when it is actually a more accurate reproduction of the page than my reference point, I amended every instance of ſ in HathiTrust to an s.

[^cf31]: Leaving the ſ characters unchanged in the HathiTrust document resulted in a .29 change from base (71% accuracy), so my normalization of ſ to s had a major impact on the comparison. I consider the .09 result more appropriate than the .29 because the normalized copy better reflects how an OCR file would be used.

[^cf32]: explain what a csv is